<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>IDEA搭建Scala_Spark开发环境</title>
      <link href="/2019/11/17/IDEA%E6%90%AD%E5%BB%BAScala-Spark%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/"/>
      <url>/2019/11/17/IDEA%E6%90%AD%E5%BB%BAScala-Spark%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/</url>
      
        <content type="html"><![CDATA[<p>IDEA本地运行Scala,Spark程序</p><p>前提:</p><ol><li>maven源设置为阿里云的源(下载依赖的速度很快)</li><li>安装了IDEA(社区版和专业版都可以)</li><li>安装了JDK 1.8 (下面的安装都是在Scala 2.11.x版本，对应的JDK版本是1.8)</li></ol><h3 id="1-下载安装Scala"><a href="#1-下载安装Scala" class="headerlink" title="1. 下载安装Scala"></a>1. 下载安装Scala</h3><p>下载二进制zip安装包解压，设置环境变量，类似安装JDK</p><p>命令行输入scala测试,</p><p><img src="scala_test.png" alt="image-20191117164012185"></p><h3 id="2-IDEA安装Scala-插件"><a href="#2-IDEA安装Scala-插件" class="headerlink" title="2. IDEA安装Scala 插件"></a>2. IDEA安装Scala 插件</h3><p><img src="scala_plugin.png" alt="image-20191117162228882"></p><p>下载慢的，可以在其他下载快的地方下载拷过来，使用磁盘安装的方式安装</p><a id="more"></a><h3 id="3-IDEA创建Maven工程"><a href="#3-IDEA创建Maven工程" class="headerlink" title="3. IDEA创建Maven工程"></a>3. IDEA创建Maven工程</h3><p>idea里创建一个maven工程后</p><ol><li><p>在src/main下建立scala文件夹</p></li><li><p>在scala文件夹右键,点击Mark Directory as Source Root ,将scala 设置为源码目录,设置为源码目录后，文件夹变成蓝色</p><p><img src="scala_dir.png" alt="image-20191117162610150"></p></li></ol><h3 id="4-配置Scala框架支持"><a href="#4-配置Scala框架支持" class="headerlink" title="4. 配置Scala框架支持"></a>4. 配置Scala框架支持</h3><p>现在在scala 文件夹，右键新建文件，发现没有新建scala 的选项，需要添加scala框架支持</p><p>打开Project Structure </p><p><img src="add_scala_sdk.png" alt="image-20191117162727290"></p><p>有两种方法</p><ul><li>一种是每个工程都点Libraires，添加Scala SDK</li></ul><p>在Libraries 选项卡里点击添加，选择Scala SDK ,选择browser浏览本地，选择上面解压的scala路径</p><ul><li><p>另一个是在Global Libraries里添加一个Scala SDK </p><p>新建完工程后，在Global Libraries里选择刚刚添加的SDK,右键添加到Module里也行</p></li></ul><p>添加完后,在源码目录右键就可以看到新建Scala类的选项了,新建个Scala类测试</p><p><img src="idea_scala_test.png" alt="image-20191117163449058"></p><h3 id="5-添加Spark-依赖"><a href="#5-添加Spark-依赖" class="headerlink" title="5. 添加Spark 依赖"></a>5. 添加Spark 依赖</h3><p>Spark的依赖包括Spark-Core,Spark-SQL,而且两个的版本要对应</p><p>下面的2.3.3为Spark的版本, 2.11为scala 2.11.x版本</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line"> <span class="comment">&lt;!--Spark RDD这些的依赖--&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span>S</span><br><span class="line">     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-core_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"> <span class="comment">&lt;!--  Spark SQL的依赖--&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.spark<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-sql_2.11<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.3.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.scala-lang<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>scala-library<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.11.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="6-测试"><a href="#6-测试" class="headerlink" title="6. 测试"></a>6. 测试</h3><p><img src="idea_spark_test.png" alt="image-20191117164432217"></p><h3 id="7-打包Jar包"><a href="#7-打包Jar包" class="headerlink" title="7. 打包Jar包"></a>7. 打包Jar包</h3><p>在Project Structure 里选择 Artifacts,添加一个Artifacts</p><p><img src="output_layout.png" alt="image-20191117164509553"></p><p>左边的为导出的jar需要的东西，右边为可选的东西,这里将右边的compile output(即编译的输出)put into 左边，就可以了</p><p>回到主界面,依次选择Build -&gt; Build Artifacts ,jar包就生成在out/目录下</p><p><img src="output_jar.png" alt="image-20191117164707761"></p><h3 id="8-常见问题"><a href="#8-常见问题" class="headerlink" title="8. 常见问题"></a>8. 常见问题</h3>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Spark </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Spark </tag>
            
            <tag> IDEA </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL创建用户</title>
      <link href="/2019/11/02/MySQL%E5%88%9B%E5%BB%BA%E7%94%A8%E6%88%B7/"/>
      <url>/2019/11/02/MySQL%E5%88%9B%E5%BB%BA%E7%94%A8%E6%88%B7/</url>
      
        <content type="html"><![CDATA[<p>Create User</p><p>MySQL 5.7</p><p>语法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">CREATE USER [IF NOT EXISTS]</span><br><span class="line">    user [auth_option] [, user [auth_option]] ...</span><br><span class="line">    [REQUIRE &#123;NONE | tls_option [[AND] tls_option] ...&#125;]</span><br><span class="line">    [WITH resource_option [resource_option] ...]</span><br><span class="line">    [password_option | lock_option] ...</span><br><span class="line"></span><br><span class="line">user:</span><br><span class="line">    (see Section 6.2.4, “Specifying Account Names”)</span><br><span class="line"></span><br><span class="line">auth_option: &#123;</span><br><span class="line">    IDENTIFIED BY &apos;auth_string&apos;</span><br><span class="line">  | IDENTIFIED WITH auth_plugin</span><br><span class="line">  | IDENTIFIED WITH auth_plugin BY &apos;auth_string&apos;</span><br><span class="line">  | IDENTIFIED WITH auth_plugin AS &apos;auth_string&apos;</span><br><span class="line">  | IDENTIFIED BY PASSWORD &apos;auth_string&apos;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">tls_option: &#123;</span><br><span class="line">   SSL</span><br><span class="line"> | X509</span><br><span class="line"> | CIPHER &apos;cipher&apos;</span><br><span class="line"> | ISSUER &apos;issuer&apos;</span><br><span class="line"> | SUBJECT &apos;subject&apos;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">resource_option: &#123;</span><br><span class="line">    MAX_QUERIES_PER_HOUR count</span><br><span class="line">  | MAX_UPDATES_PER_HOUR count</span><br><span class="line">  | MAX_CONNECTIONS_PER_HOUR count</span><br><span class="line">  | MAX_USER_CONNECTIONS count</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">password_option: &#123;</span><br><span class="line">    PASSWORD EXPIRE</span><br><span class="line">  | PASSWORD EXPIRE DEFAULT</span><br><span class="line">  | PASSWORD EXPIRE NEVER</span><br><span class="line">  | PASSWORD EXPIRE INTERVAL N DAY</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">lock_option: &#123;</span><br><span class="line">    ACCOUNT LOCK</span><br><span class="line">  | ACCOUNT UNLOCK</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>创建用户要有<code>create user</code>权限或者<code>mysql</code>系统数据库的<code>insert</code>权限</li></ul><blockquote><p>创建用户的命令可能会记录在~/.mysql_history 文件里，别人查看这个文件，可能能直接看到明文的密码</p></blockquote><p>刚创建完的用户是没有权限的</p>]]></content>
      
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL命令行内查看函数帮助</title>
      <link href="/2019/10/30/MySQL%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%86%85%E6%9F%A5%E7%9C%8B%E5%87%BD%E6%95%B0%E5%B8%AE%E5%8A%A9/"/>
      <url>/2019/10/30/MySQL%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%86%85%E6%9F%A5%E7%9C%8B%E5%87%BD%E6%95%B0%E5%B8%AE%E5%8A%A9/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Redis学习笔记</title>
      <link href="/2019/10/28/Redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2019/10/28/Redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>redis 一共有16个数据库,下标0-15</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">切换数据库</span><br><span class="line">select 1</span><br></pre></td></tr></table></figure><p>redis 数据用key value的格式存储数据</p><p>通用操作</p><table><thead><tr><th>操作</th><th></th><th></th></tr></thead><tbody><tr><td>keys *</td><td>查询库所有值</td><td></td></tr><tr><td>exists key</td><td>判断键是否存在</td><td></td></tr><tr><td>type key</td><td>查看键类型</td><td></td></tr><tr><td>del</td><td>删除键</td><td></td></tr><tr><td>expire key seconds</td><td>设置键过期时间</td><td></td></tr><tr><td>ttl key</td><td>查看还有多少秒过期</td><td></td></tr><tr><td>dbsize</td><td>查看数据库key的数量</td><td></td></tr><tr><td>flushdb</td><td>清空当前库</td><td></td></tr><tr><td>flushall</td><td>清空所有库</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table><p>redis value 数据类型</p><ul><li>string</li></ul><p>最基本的数据类型,一个字符串最大是512m</p><table><thead><tr><th>操作</th><th></th><th></th></tr></thead><tbody><tr><td>get <key></key></td><td>查询键值</td><td></td></tr><tr><td>set <key> <value></value></key></td><td>设置键值</td><td></td></tr><tr><td>append key value</td><td>添加value到原值的末尾</td><td></td></tr><tr><td>strlen</td><td>获取值长度</td><td></td></tr><tr><td>setnx key value</td><td>key不存在时设置key的值</td><td></td></tr><tr><td>incr</td><td>值增1，值只能是数字</td><td></td></tr><tr><td>desc</td><td>值减1，同上</td><td></td></tr><tr><td>incrby/decr key 长度</td><td>增加减掉指定值</td><td></td></tr><tr><td>mset key1 value1 key2 value2</td><td>同时设置一个或者多个key-value对</td><td></td></tr><tr><td>mget key1 , key2</td><td>获取多个值</td><td></td></tr><tr><td>msetnx key1 value key2 value2</td><td>设置多个值,但key不存在时</td><td></td></tr><tr><td>getrange key start end</td><td>获取值的范围</td><td></td></tr><tr><td>setrange key start end</td><td>设置值</td><td></td></tr><tr><td>setex key 过期时间 value</td><td>设置过期时间，单位秒</td><td></td></tr><tr><td>getset key value</td><td>获取新值，同时设置旧值</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table><ul><li><p>list</p><p>底层是个双向列表</p><table><thead><tr><th>操作</th><th></th><th></th></tr></thead><tbody><tr><td>lpush/rpush key value1 value2</td><td>左边或者右边插入值</td><td></td></tr><tr><td>lpop/rpop</td><td>pop一个值</td><td></td></tr><tr><td>rpoplpush</td><td>右边取出一个值，插到左边</td><td></td></tr><tr><td>lrange key start end</td><td>通过索引下标获取元素</td><td></td></tr><tr><td>lindex key index</td><td>通过索引获取元素</td><td></td></tr><tr><td>llen</td><td>获取列表长度</td><td></td></tr><tr><td>linsert key before value newvalue</td><td>在value前面插入newvalue</td><td></td></tr><tr><td>lrem key n value</td><td>从左边删除n个value</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table></li></ul><ul><li><p>set</p><p>没有重复数据</p><table><thead><tr><th>操作</th><th></th><th></th></tr></thead><tbody><tr><td>sadd key value v2</td><td>添加值</td><td></td></tr><tr><td>smembers key</td><td>取出所有值</td><td></td></tr><tr><td>sismember key value</td><td>判断集合是不是有这个value</td><td></td></tr><tr><td>scard</td><td>集合元素个数</td><td></td></tr><tr><td>srem key v1 v2….</td><td>删除元素</td><td></td></tr><tr><td>spop</td><td>随机取出一个值</td><td></td></tr><tr><td>srandmember key n</td><td>随机取出n个值</td><td></td></tr><tr><td>sinter key1 key2</td><td>取出n个值交集</td><td></td></tr><tr><td>sunion key1 key2</td><td>返回两个交集的并集</td><td></td></tr><tr><td>sdiff</td><td>返回两个元素的差集</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table></li></ul><ul><li>hash</li></ul><p>hash是个键值对集合,&lt;key,value&gt;</p><table><thead><tr><th>操作</th><th></th><th></th></tr></thead><tbody><tr><td>hset key field value</td><td>设置key集合里的field的值为value</td><td></td></tr><tr><td>hget key field</td><td>获取field</td><td></td></tr><tr><td>hmset key field1 field2</td><td>批量设置hash的值</td><td></td></tr><tr><td>hexists key field</td><td>判断field是否存在</td><td></td></tr><tr><td>hkeys key</td><td>获取所有field</td><td></td></tr><tr><td>hvals key</td><td>获取所有value</td><td></td></tr><tr><td>hincrby key field increment</td><td>给指定的field加上增量increment</td><td></td></tr><tr><td>hsetnx key field value</td><td>当field不存在时，设置field的value为value</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table><ul><li><p>zset</p><p>类似set,但是每个元素都关联了一个评分(score),用这个评分来排序集合里的元素</p></li></ul><table><thead><tr><th></th><th></th></tr></thead><tbody><tr><td>zadd key score value score value…</td><td>将一个或者多个元素添加到key的集合里</td></tr><tr><td>zrange key start stop withscores</td><td>返回下标在start 到 stop之间的元素,</td></tr><tr><td>zrangebyscore key min max withscores limit offset count</td><td>返回score在min到max之间的成员</td></tr><tr><td>zrangebyscore key max min withscores limit offset count</td><td>同上，大到小</td></tr><tr><td>zincrby key increment  value</td><td>为元素score加上增量</td></tr><tr><td>zrem key value</td><td>删除指定值的元素</td></tr><tr><td>zcount key min max</td><td>统计分数区间内的个数</td></tr><tr><td>zrank key value</td><td>返回值在集合里的排名，0开始</td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr><tr><td></td><td></td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop集群搭建（六）Spark部署</title>
      <link href="/2019/10/23/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E5%85%AD%EF%BC%89Spark%E9%83%A8%E7%BD%B2/"/>
      <url>/2019/10/23/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E5%85%AD%EF%BC%89Spark%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<p>HA ，主从master</p><p>Spark版本:2.0.0</p><p>解压到指定目录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf spark-2.0.0-bin-hadoop2.6.tgz  -C /opt/</span><br><span class="line">mv /opt/spark-2.0.0-bin-hadoop2.6/ /opt/spark</span><br><span class="line">添加到环境变量</span><br></pre></td></tr></table></figure><p>修改配置文件</p><ul><li>slaves</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure><a id="more"></a><ul><li><p>spark-env.sh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">  </span><br><span class="line">  <span class="built_in">export</span> JAVA_HOME=/opt/jdk8</span><br><span class="line">  <span class="comment">#在yarn模式运行时读取</span></span><br><span class="line">  <span class="built_in">export</span> HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop</span><br><span class="line">  <span class="comment">#指定master到其他主机，这里指定时本地,因为可能有俩个机器master</span></span><br><span class="line">  <span class="built_in">export</span>  SPARK_MASTER_HOST=master</span><br><span class="line">  </span><br><span class="line">  <span class="comment">#修改master的端口，这里还是7077,默认也是7077</span></span><br><span class="line">  <span class="built_in">export</span> SPARK_MASTER_PORT=7077</span><br><span class="line">  <span class="comment">#这台机器上使用的核心数</span></span><br><span class="line">  <span class="built_in">export</span> SPARK_WORKER_CORES=1</span><br><span class="line">  <span class="comment">#</span></span><br><span class="line">  <span class="built_in">export</span> SPARK_WORKER_MEMORY=1G</span><br><span class="line">  <span class="comment">#master高可用,使用zookeeper</span></span><br><span class="line"><span class="built_in">export</span> SPARK_DAEMON_JAVA_OPTS=<span class="string">"-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=master:2181,slave1:2181,slave2:2181 -Dspark.deploy.zookeeper.dir=/spark/ha"</span></span><br></pre></td></tr></table></figure><p>发送环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></li></ul><p>  配置好的spark文件夹复制到其他节点</p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">  scp -r /opt/spark/ root@slave1:/opt/</span><br><span class="line">  scp -r /opt/spark/ root@slave2:/opt/</span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">  修改配置文件就直接发送配置文件</span><br><span class="line">scp -r /opt/spark/conf/* root@slave1:/opt/spark/conf</span><br></pre></td></tr></table></figure><p>启动</p><p>master启动</p><p><code>start-all.sh</code></p><p>在slave1启动备用的master</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-master.sh</span><br></pre></td></tr></table></figure><p>效果</p><p>主master</p><p><img src="/../Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E5%85%AD%EF%BC%89Spark%E9%83%A8%E7%BD%B2/1571837573533.png" alt="1571837573533"></p><p>备用master</p><p><img src="/../Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E5%85%AD%EF%BC%89Spark%E9%83%A8%E7%BD%B2/1571835801466.png" alt="1571835801466"></p><p>spark-shell</p><h3 id="错误"><a href="#错误" class="headerlink" title="错误"></a>错误</h3><ul><li>启动备用master失败</li></ul><p><img src="/../Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E5%85%AD%EF%BC%89Spark%E9%83%A8%E7%BD%B2/1571834581434.png" alt="1571834581434"></p><p>绑定不了端口，之前SPARK_MASTER_HOST是设置为master,可能是根据这个值来绑定的，所以slave1启动master绑定不了</p><p>在spark-env.sh配置</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">单独配置slave1机器的spark-env.sh的这个变量，为这个机器的ip或者主机名</span><br><span class="line"><span class="built_in">export</span>  SPARK_MASTER_HOST=slave1</span><br></pre></td></tr></table></figure><p>和hive连接</p><p>默认的是spark默认的，改成自己的hive</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">复制hive-site.xml到spark的配置目录</span><br><span class="line">cp /opt/hive/conf/hive-site.xml  /opt/spark/conf/</span><br><span class="line"></span><br><span class="line"> scp /opt/hive/conf/hive-site.xml  root@slave1:/opt/spark/conf/</span><br><span class="line">  scp /opt/hive/conf/hive-site.xml  root@slave2:/opt/spark/conf/</span><br><span class="line">复制hmysql驱动到spark的jars目录</span><br><span class="line"> scp /root/mysql-connector-java-5.1.47.jar  root@slave1:/opt/spark/jars</span><br></pre></td></tr></table></figure><p>测试保存数据到hive</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">启动spark集群</span><br><span class="line">start-all.sh</span><br><span class="line"></span><br><span class="line">进入spark-shell</span><br><span class="line">spark-shell --master spark://master:7077</span><br><span class="line">创建一个dataframe</span><br><span class="line">val df = spark.createDataFrame(Seq((&quot;mt&quot;,&quot;bin&quot;),(&quot;adk&quot;,&quot;mxk&quot;))).toDF(&quot;app&quot;,&quot;author&quot;)</span><br><span class="line">//保存到hive表里，没有指定数据库，默认使用default数据库</span><br><span class="line"> df.write.saveAsTable(&quot;fromsparktable&quot;)</span><br><span class="line"> </span><br><span class="line"> 进入hive查看结果</span><br><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">fromsparktable</span><br><span class="line">Time taken: 0.668 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; select * from fromsparktable;</span><br><span class="line">OK</span><br><span class="line">SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.</span><br><span class="line">SLF4J: Defaulting to no-operation (NOP) logger implementation</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.</span><br><span class="line">mtbin</span><br><span class="line">adkmxk</span><br></pre></td></tr></table></figure><p><img src="/../Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E5%85%AD%EF%BC%89Spark%E9%83%A8%E7%BD%B2/1571839502090.png" alt="1571839502090"></p><p>参考</p><ul><li>spark ha <a href="https://developpaper.com/spark-ha-cluster-construction/" target="_blank" rel="noopener">https://developpaper.com/spark-ha-cluster-construction/</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Hadoop集群搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop集群搭建（五）Hive部署</title>
      <link href="/2019/10/23/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E4%BA%94%EF%BC%89Hive%E9%83%A8%E7%BD%B2/"/>
      <url>/2019/10/23/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E4%BA%94%EF%BC%89Hive%E9%83%A8%E7%BD%B2/</url>
      
        <content type="html"><![CDATA[<p>解压hive 安装包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-hive-1.1.0-bin.tar.gz -C /opt/</span><br><span class="line"> mv /opt/apache-hive-1.1.0-bin/ /opt/hive</span><br><span class="line">添加到环境变量</span><br></pre></td></tr></table></figure><p>修改配置文件</p><ul><li>hive-env.sh </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><ul><li><p>hive-site.xml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">修改hive-default.xml为hive-site.xml</span><br><span class="line">1. 设置jdbc路径</span><br><span class="line">修改</span><br></pre></td></tr></table></figure><ol><li>修改下面的配置项</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>password to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://192.168.3.11/hivemeta?useSSL=false&amp;amp;createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>JDBC connect string for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>Username to use against metastore database<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ol start="2"><li>修改java.io.tmp<br>把hive-site.xml里的<br>${system:java.io.tmpdir%}全部修改为/tmp,不然启动会报错</li></ol><p>上传mysql驱动到hive的lib目录中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">scp /root/mysql-connector-java-5.1.47.jar  /opt/hive/lib/</span><br></pre></td></tr></table></figure><p>启动后会有这个错误</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">Exception in thread &quot;main&quot; java.lang.IncompatibleClassChangeError: Found class jline.Terminal, but interface was expected</span><br><span class="line">at jline.console.ConsoleReader.&lt;init&gt;(ConsoleReader.java:230)</span><br><span class="line">at jline.console.ConsoleReader.&lt;init&gt;(ConsoleReader.java:221)</span><br><span class="line">at jline.console.ConsoleReader.&lt;init&gt;(ConsoleReader.java:209)</span><br><span class="line">at org.apache.hadoop.hive.cli.CliDriver.getConsoleReader(CliDriver.java:773)</span><br><span class="line">at org.apache.hadoop.hive.cli.CliDriver.executeDriver(CliDriver.java:715)</span><br><span class="line">at org.apache.hadoop.hive.cli.CliDriver.run(CliDriver.java:675)</span><br><span class="line">at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:615)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)</span><br><span class="line">at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)</span><br><span class="line">at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)</span><br><span class="line">at java.lang.reflect.Method.invoke(Method.java:498)</span><br><span class="line">at org.apache.hadoop.util.RunJar.run(RunJar.java:221)</span><br><span class="line">at org.apache.hadoop.util.RunJar.main(RunJar.java:136)</span><br></pre></td></tr></table></figure></li></ul><p>  类冲突,网上解决方法</p><p>  <a href="https://blog.csdn.net/qq_39507276/article/details/85367305" target="_blank" rel="noopener">https://blog.csdn.net/qq_39507276/article/details/85367305</a></p>  <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> mv    /opt/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar   /opt/hadoop/share/hadoop/yarn/lib/jline-0.9.94.jar.bak</span><br><span class="line"> cp /opt/hive/lib/jline-2.12.jar  /opt/hadoop/share/hadoop/yarn/lib/</span><br><span class="line">hive里的jline版本比hadoop里的高</span><br></pre></td></tr></table></figure><ul><li><p>初始化元数据</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">暂时不知道有什么用，没初始化直接使用hive命令也行</span><br><span class="line">schematool -dbType mysql -initSchema</span><br></pre></td></tr></table></figure></li><li><p>复制目录到其他节点</p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Hadoop集群搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop集群搭建（四）Centos7_Minimal安装MySQL5.7</title>
      <link href="/2019/10/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E5%9B%9B%EF%BC%89Centos7-Minimal%E5%AE%89%E8%A3%85MySQL5-7/"/>
      <url>/2019/10/21/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E5%9B%9B%EF%BC%89Centos7-Minimal%E5%AE%89%E8%A3%85MySQL5-7/</url>
      
        <content type="html"><![CDATA[<blockquote><p>安装到slave1,用来做hive元数据</p></blockquote><p>使用yum安装,现在官网下载mysql的源的rpm包</p><h3 id="下载rpm包安装源"><a href="#下载rpm包安装源" class="headerlink" title="下载rpm包安装源"></a>下载rpm包安装源</h3><p>mysql community下载</p><p><a href="https://dev.mysql.com/downloads/,centos" target="_blank" rel="noopener">https://dev.mysql.com/downloads/,centos</a> 选择Yum的</p><p><img src="Centos7-Minimal%E5%AE%89%E8%A3%85MySQL5-7/1571630874399.png" alt="1571630874399"></p><p>将下载的 <a href="https://repo.mysql.com//mysql80-community-release-el7-3.noarch.rpm" target="_blank" rel="noopener">mysql80-community-release-el7-3.noarch.rpm</a>复制到centos 系统,或者在centos里用wget下载</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">curl  https://repo.mysql.com//mysql80-community-release-el7-3.noarch.rpm -o mysql80-community-release-el7-3.noarch.rpm</span><br></pre></td></tr></table></figure><p>安装mysql源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -i  mysql80-community-release-el7-3.noarch.rpm</span><br></pre></td></tr></table></figure><p>安装后可以在<code>/etc/yum.repos.d/</code>找到mysql的源</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@slave1 ~]# ll /etc/yum.repos.d/</span><br><span class="line">total 40</span><br><span class="line">-rw-r--r--. 1 root root 1664 11月 23 2018 CentOS-Base.repo</span><br><span class="line">-rw-r--r--. 1 root root 1309 11月 23 2018 CentOS-CR.repo</span><br><span class="line">-rw-r--r--. 1 root root  649 11月 23 2018 CentOS-Debuginfo.repo</span><br><span class="line">-rw-r--r--. 1 root root  314 11月 23 2018 CentOS-fasttrack.repo</span><br><span class="line">-rw-r--r--. 1 root root  630 11月 23 2018 CentOS-Media.repo</span><br><span class="line">-rw-r--r--. 1 root root 1331 11月 23 2018 CentOS-Sources.repo</span><br><span class="line">-rw-r--r--. 1 root root 5701 11月 23 2018 CentOS-Vault.repo</span><br><span class="line">-rw-r--r--. 1 root root 2076 4月  24 13:35 mysql-community.repo</span><br><span class="line">-rw-r--r--. 1 root root 2108 4月  24 13:35 mysql-community-source.repo</span><br></pre></td></tr></table></figure><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><p><code>yum install mysql-server</code></p><h4 id="安装5-7版本"><a href="#安装5-7版本" class="headerlink" title="安装5.7版本"></a>安装5.7版本</h4><p>如果直接用上面命令，安装的是最新版的8.0,想要安装5.7版的MySQL需要修改repo文件</p><p>编辑<code>mysql-community.repo</code>文件</p><ul><li>将mysql 5.7段的enabled值改成1</li><li>80的改成0</li></ul><p>这样用上面命令安装的就是5.7版本的</p><h3 id="启动mysql服务"><a href="#启动mysql服务" class="headerlink" title="启动mysql服务"></a>启动mysql服务</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">刚安装完是没有启动的</span><br><span class="line">service mysqld start</span><br></pre></td></tr></table></figure><h3 id="密码设置"><a href="#密码设置" class="headerlink" title="密码设置"></a>密码设置</h3><ol><li>用临时密码登录</li><li>修改密码(这时只能修改成一个复杂密码 ,至少8个字符+有数字+有大写字符+有小写字符+有特殊符号</li><li>设置密码的安全策略为低(Low),设置密码最少长度为0 ,这样就能修改成任意密码了</li></ol><p>临时密码获取:</p><p>启动后会生成一个随机密码，查看<code>/var/log/mysqld.log</code>文件获取</p><p><img src="Centos7-Minimal%E5%AE%89%E8%A3%85MySQL5-7/1571634812278.png" alt="1571634812278"></p><p>使用随机密码登录</p><p><img src="Centos7-Minimal%E5%AE%89%E8%A3%85MySQL5-7/1571634839842.png" alt="1571634839842"></p><p>官方文档:</p><p><a href="https://dev.mysql.com/doc/refman/5.7/en/validate-password.html" target="_blank" rel="noopener">https://dev.mysql.com/doc/refman/5.7/en/validate-password.html</a></p><p>修改密码</p><p><img src="Centos7-Minimal%E5%AE%89%E8%A3%85MySQL5-7/1571634899883.png" alt="1571634899883"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">修改密码代码</span><br><span class="line">ALTER USER USER() IDENTIFIED BY &apos;Abc1234?&apos;;</span><br></pre></td></tr></table></figure><p>查看下密码策略</p><p><img src="Centos7-Minimal%E5%AE%89%E8%A3%85MySQL5-7/1571635770343.png" alt="1571635770343"></p><p>validate_password_policy 现在是 MEDIUM,其他的值是LOW和STRONG</p><p>是LOW时需要满足<a href="https://dev.mysql.com/doc/refman/5.7/en/validate-password-options-variables.html#sysvar_validate_password_length" target="_blank" rel="noopener"><code>validate_password_length</code></a>.这个变量的值</p><p>是MEDIUM时需要满足下面三个值</p><p><a href="https://dev.mysql.com/doc/refman/5.7/en/validate-password-options-variables.html#sysvar_validate_password_number_count" target="_blank" rel="noopener"><code>validate_password_number_count</code></a>,          <a href="https://dev.mysql.com/doc/refman/5.7/en/validate-password-options-variables.html#sysvar_validate_password_mixed_case_count" target="_blank" rel="noopener"><code>validate_password_mixed_case_count</code></a>,          <a href="https://dev.mysql.com/doc/refman/5.7/en/validate-password-options-variables.html#sysvar_validate_password_special_char_count" target="_blank" rel="noopener"><code>validate_password_special_char_count</code></a>.        </p><p>如果要修改密码为root,首先先将<code>validate_password_policy</code>改成LOW,在修改<code>validate_password_length</code>的值为0</p><p>修改全局变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">设置密码策略</span><br><span class="line">set global validate_password_policy=&apos;LOW&apos;</span><br><span class="line">修改最短长度</span><br><span class="line">set global validate_password_length=0</span><br><span class="line"></span><br><span class="line">修改为简单密码</span><br><span class="line">ALTER USER USER() IDENTIFIED BY &apos;root&apos;;</span><br><span class="line">mysql&gt; ALTER USER USER() IDENTIFIED BY &apos;root&apos;;</span><br><span class="line">Query OK, 0 rows affected (0.00 sec)</span><br><span class="line">成功</span><br></pre></td></tr></table></figure><h3 id="创建用户和设置root用户远程登录"><a href="#创建用户和设置root用户远程登录" class="headerlink" title="创建用户和设置root用户远程登录"></a>创建用户和设置root用户远程登录</h3><p>查看下当前的用户</p><p><img src="Centos7-Minimal%E5%AE%89%E8%A3%85MySQL5-7/1571636831156.png" alt="1571636831156"></p><ul><li>root用户现在只能localhost本地登录</li></ul><p>创建一个hive用户</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">create user &apos;hive&apos;@&apos;%&apos; identified by &apos;hive&apos;;</span><br><span class="line">创建一个数据库 hivemeta</span><br><span class="line">create database hivemeta ;</span><br><span class="line">//授予权限给hive</span><br><span class="line">grant all on hivemeta.* to hive;</span><br></pre></td></tr></table></figure><p>设置root远程登录</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grant  all privileges on *.* to &apos;root&apos;@&apos;%&apos; identified by &apos;root&apos; with grant option;</span><br></pre></td></tr></table></figure><p>查看用户</p><p><img src="Centos7-Minimal%E5%AE%89%E8%A3%85MySQL5-7/1571638136490.png" alt="1571638136490"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Hadoop集群搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop_Namenode高可用测试</title>
      <link href="/2019/10/21/Hadoop-Namenode%E9%AB%98%E5%8F%AF%E7%94%A8%E6%B5%8B%E8%AF%95/"/>
      <url>/2019/10/21/Hadoop-Namenode%E9%AB%98%E5%8F%AF%E7%94%A8%E6%B5%8B%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<p>测试方法:</p><p>使用kill 命令杀死namenode,模拟namenode停止</p><p>效果:</p><p>备用namenode转换为active,重新启动被杀死的namenode,状态变成standby</p><p>备用切换成active之前，要确保之前那个namenode凉透了，不然两个一起namenode active,会发生脑裂</p><p>切换的方式在hdfs-site.xml里配置</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence</span><br><span class="line">shell(/bin/true)<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><a id="more"></a><p>上面配置的两种方式,只要一个成功就可以转换为active,第一个sshfence,需要用到fuser命令,centos mini默认没有安装这个命令,肯定返回错误,第一个肯的返回true,所以上面这个配置namenode一停,另一个就马上转换成active</p><p>这里测试sshfence方式,修改配置文件为</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"> <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.ssh.private-key-files<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/.ssh/id_rsa<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>安装fuser命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install psmisc</span><br></pre></td></tr></table></figure><h3 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h3><p>查看哪个namenode是active的</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">[root@master hadoop]# hdfs haadmin -getServiceState nn1</span><br><span class="line">active</span><br><span class="line">[root@master hadoop]# hdfs haadmin -getServiceState nn2</span><br><span class="line">standby</span><br><span class="line"></span><br><span class="line">nn1 即master是active,在master上杀死namenode</span><br><span class="line">查看namenode的进程</span><br><span class="line"></span><br><span class="line">[root@master hadoop]# jps</span><br><span class="line">14065 JobHistoryServer</span><br><span class="line">19457 Jps</span><br><span class="line">19111 JournalNode</span><br><span class="line">18936 DataNode</span><br><span class="line">18841 NameNode</span><br><span class="line">19273 DFSZKFailoverController</span><br><span class="line">11678 QuorumPeerMain</span><br><span class="line"></span><br><span class="line">杀死</span><br><span class="line">[root@master hadoop]# kill -9 18841</span><br><span class="line">不存在了</span><br><span class="line">[root@master hadoop]# jps</span><br><span class="line">14065 JobHistoryServer</span><br><span class="line">19111 JournalNode</span><br><span class="line">18936 DataNode</span><br><span class="line">19273 DFSZKFailoverController</span><br><span class="line">11678 QuorumPeerMain</span><br><span class="line">19470 Jps</span><br><span class="line"></span><br><span class="line">查看下namenode状态</span><br><span class="line">[root@master hadoop]# hdfs haadmin -getServiceState nn1</span><br><span class="line">19/10/20 22:37:00 INFO ipc.Client: Retrying connect to server: master/192.168.3.10:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)</span><br><span class="line">Operation failed: Call From master/192.168.3.10 to master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</span><br><span class="line">[root@master hadoop]# hdfs haadmin -getServiceState nn2</span><br><span class="line">active</span><br><span class="line"></span><br><span class="line">nn2变成active了，成功</span><br></pre></td></tr></table></figure><p>查看下nn2的日志</p><p><code>cat  logs/hadoop-root-zkfc-slave1.log</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">2019-10-20 22:36:25,154 INFO org.apache.hadoop.ha.ZKFailoverController: Should fence: NameNode at master/192.168.3.10:9000</span><br><span class="line">2019-10-20 22:36:26,163 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: master/192.168.3.10:9000. Already tried 0 time(s); retry policy is RetryUpToMaximumCountWithFixedSleep(maxRetries=1, sleepTime=1000 MILLISECONDS)</span><br><span class="line">这里开始连接不到master了</span><br><span class="line">2019-10-20 22:36:26,179 WARN org.apache.hadoop.ha.FailoverController: Unable to gracefully make NameNode at master/192.168.3.10:9000 standby (unable to connect)</span><br><span class="line">java.net.ConnectException: Call From slave1/192.168.3.11 to master:9000 failed on connection exception: java.net.ConnectException: Connection refused; For more details see:  http://wiki.apache.org/hadoop/ConnectionRefused</span><br><span class="line">at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)</span><br><span class="line">at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)</span><br><span class="line">at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)</span><br><span class="line">at java.lang.reflect.Constructor.newInstance(Constructor.java:423)</span><br><span class="line">at org.apache.hadoop.net.NetUtils.wrapWithMessage(NetUtils.java:791)</span><br><span class="line">at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:731)</span><br><span class="line">at org.apache.hadoop.ipc.Client.call(Client.java:1472)</span><br><span class="line">at org.apache.hadoop.ipc.Client.call(Client.java:1399)</span><br><span class="line">at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)</span><br><span class="line">at com.sun.proxy.$Proxy9.transitionToStandby(Unknown Source)</span><br><span class="line">at org.apache.hadoop.ha.protocolPB.HAServiceProtocolClientSideTranslatorPB.transitionToStandby(HAServiceProtocolClientSideTranslatorPB.java:112)</span><br><span class="line">at org.apache.hadoop.ha.FailoverController.tryGracefulFence(FailoverController.java:172)</span><br><span class="line">at org.apache.hadoop.ha.ZKFailoverController.doFence(ZKFailoverController.java:512)</span><br><span class="line">at org.apache.hadoop.ha.ZKFailoverController.fenceOldActive(ZKFailoverController.java:503)</span><br><span class="line">at org.apache.hadoop.ha.ZKFailoverController.access$1100(ZKFailoverController.java:61)</span><br><span class="line">at org.apache.hadoop.ha.ZKFailoverController$ElectorCallbacks.fenceOldActive(ZKFailoverController.java:890)</span><br><span class="line">at org.apache.hadoop.ha.ActiveStandbyElector.fenceOldActive(ActiveStandbyElector.java:902)</span><br><span class="line">at org.apache.hadoop.ha.ActiveStandbyElector.becomeActive(ActiveStandbyElector.java:801)</span><br><span class="line">at org.apache.hadoop.ha.ActiveStandbyElector.processResult(ActiveStandbyElector.java:416)</span><br><span class="line">at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:599)</span><br><span class="line">at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:498)</span><br><span class="line">Caused by: java.net.ConnectException: Connection refused</span><br><span class="line">at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)</span><br><span class="line">at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:717)</span><br><span class="line">at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)</span><br><span class="line">at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:530)</span><br><span class="line">at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:494)</span><br><span class="line">at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:607)</span><br><span class="line">at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:705)</span><br><span class="line">at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)</span><br><span class="line">at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)</span><br><span class="line">at org.apache.hadoop.ipc.Client.call(Client.java:1438)</span><br><span class="line">... 14 more</span><br><span class="line">开始隔离</span><br><span class="line">2019-10-20 22:36:26,193 INFO org.apache.hadoop.ha.NodeFencer: ====== Beginning Service Fencing Process... ======</span><br><span class="line">尝试第一个sshfence方式</span><br><span class="line">2019-10-20 22:36:26,195 INFO org.apache.hadoop.ha.NodeFencer: Trying method 1/1: org.apache.hadoop.ha.SshFenceByTcpPort(null)</span><br><span class="line">2019-10-20 22:36:26,279 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Connecting to master...</span><br><span class="line">2019-10-20 22:36:26,280 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Connecting to master port 22</span><br><span class="line">2019-10-20 22:36:26,288 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Connection established</span><br><span class="line">2019-10-20 22:36:26,300 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Remote version string: SSH-2.0-OpenSSH_7.4</span><br><span class="line">2019-10-20 22:36:26,300 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Local version string: SSH-2.0-JSCH-0.1.42</span><br><span class="line">2019-10-20 22:36:26,300 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: CheckCiphers: aes256-ctr,aes192-ctr,aes128-ctr,aes256-cbc,aes192-cbc,aes128-cbc,3des-ctr,arcfour,arcfour128,arcfour256</span><br><span class="line">2019-10-20 22:36:26,699 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes256-ctr is not available.</span><br><span class="line">2019-10-20 22:36:26,699 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes192-ctr is not available.</span><br><span class="line">2019-10-20 22:36:26,699 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes256-cbc is not available.</span><br><span class="line">2019-10-20 22:36:26,699 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: aes192-cbc is not available.</span><br><span class="line">2019-10-20 22:36:26,699 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: arcfour256 is not available.</span><br><span class="line">2019-10-20 22:36:26,700 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXINIT sent</span><br><span class="line">2019-10-20 22:36:26,700 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXINIT received</span><br><span class="line">2019-10-20 22:36:26,700 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: kex: server-&gt;client aes128-ctr hmac-sha1 none</span><br><span class="line">2019-10-20 22:36:26,700 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: kex: client-&gt;server aes128-ctr hmac-sha1 none</span><br><span class="line">2019-10-20 22:36:26,717 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_KEXDH_INIT sent</span><br><span class="line">2019-10-20 22:36:26,717 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: expecting SSH_MSG_KEXDH_REPLY</span><br><span class="line">rsa 验证成功</span><br><span class="line">2019-10-20 22:36:26,741 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: ssh_rsa_verify: signature true</span><br><span class="line">2019-10-20 22:36:26,743 WARN org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Permanently added &apos;master&apos; (RSA) to the list of known hosts.</span><br><span class="line">2019-10-20 22:36:26,743 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_NEWKEYS sent</span><br><span class="line">2019-10-20 22:36:26,743 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_NEWKEYS received</span><br><span class="line">2019-10-20 22:36:26,766 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_SERVICE_REQUEST sent</span><br><span class="line">2019-10-20 22:36:26,768 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: SSH_MSG_SERVICE_ACCEPT received</span><br><span class="line">2019-10-20 22:36:26,770 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentications that can continue: gssapi-with-mic,publickey,keyboard-interactive,password</span><br><span class="line">2019-10-20 22:36:26,770 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Next authentication method: gssapi-with-mic</span><br><span class="line">2019-10-20 22:36:26,773 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentications that can continue: publickey,keyboard-interactive,password</span><br><span class="line">2019-10-20 22:36:26,773 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Next authentication method: publickey</span><br><span class="line">2019-10-20 22:36:26,896 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Authentication succeeded (publickey).</span><br><span class="line">2019-10-20 22:36:26,898 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Connected to master</span><br><span class="line">在master查找进程,</span><br><span class="line">2019-10-20 22:36:26,898 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Looking for process running on port 9000</span><br><span class="line">2019-10-20 22:36:27,291 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Indeterminate response from trying to kill service. Verifying whether it is running using nc...</span><br><span class="line">用nc命令判断是不是在运行</span><br><span class="line">2019-10-20 22:36:27,366 WARN org.apache.hadoop.ha.SshFenceByTcpPort: nc -z master 9000 via ssh: bash: nc: command not found</span><br><span class="line">找不到nc命令?</span><br><span class="line">2019-10-20 22:36:27,370 INFO org.apache.hadoop.ha.SshFenceByTcpPort: Verified that the service is down.</span><br><span class="line">2019-10-20 22:36:27,370 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Disconnecting from master port 22</span><br><span class="line">2019-10-20 22:36:27,372 INFO org.apache.hadoop.ha.NodeFencer: ====== Fencing successful by method org.apache.hadoop.ha.SshFenceByTcpPort(null) ======</span><br><span class="line">2019-10-20 22:36:27,372 INFO org.apache.hadoop.ha.ActiveStandbyElector: Writing znode /hadoop-ha/ns/ActiveBreadCrumb to indicate that the local node is the most recent active...</span><br><span class="line">2019-10-20 22:36:27,374 INFO org.apache.hadoop.ha.SshFenceByTcpPort.jsch: Caught an exception, leaving main loop due to Socket closed</span><br><span class="line">将nn2转换为active</span><br><span class="line">2019-10-20 22:36:27,599 INFO org.apache.hadoop.ha.ZKFailoverController: Trying to make NameNode at slave1/192.168.3.11:9000 active...</span><br><span class="line">2019-10-20 22:36:30,408 INFO org.apache.hadoop.ha.ZKFailoverController: Successfully transitioned NameNode at slave1/192.168.3.11:9000 to active state</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Hadoop集群搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop集群搭建（三）配置Hadoop组件等</title>
      <link href="/2019/10/20/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E4%B8%89%EF%BC%89%E9%85%8D%E7%BD%AEHadoop%E7%BB%84%E4%BB%B6%E7%AD%89/"/>
      <url>/2019/10/20/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E4%B8%89%EF%BC%89%E9%85%8D%E7%BD%AEHadoop%E7%BB%84%E4%BB%B6%E7%AD%89/</url>
      
        <content type="html"><![CDATA[<h3 id="JDK配置"><a href="#JDK配置" class="headerlink" title="JDK配置"></a>JDK配置</h3><p>解压配置环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf jdk-8u144-linux-x64.tar.gz -C /opt/</span><br><span class="line">mv /opt/jdk1.8.0_144/ /opt/jdk8</span><br><span class="line">修改~/.bash_profile文件,设置PATH变量</span><br></pre></td></tr></table></figure><p>.bash_profile文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># .bash_profile</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Get the aliases and functions</span></span><br><span class="line"><span class="keyword">if</span> [ -f ~/.bashrc ]; <span class="keyword">then</span></span><br><span class="line">. ~/.bashrc</span><br><span class="line"><span class="keyword">fi</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># User specific environment and startup programs</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/opt/jdk8</span><br><span class="line">PATH=<span class="variable">$JAVA_HOME</span>/bin:<span class="variable">$PATH</span>:<span class="variable">$HOME</span>/bin</span><br><span class="line"></span><br><span class="line"><span class="built_in">export</span> PATH</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="Zookeeper配置"><a href="#Zookeeper配置" class="headerlink" title="Zookeeper配置"></a>Zookeeper配置</h3><p>解压zookeeper</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf zookeeper-3.4.5.tar.gz -C /opt/</span><br><span class="line">mv /opt/zookeeper-3.4.5/ /opt/zookeeper</span><br></pre></td></tr></table></figure><p>zoo.cfg文件配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">tickTime=2000</span><br><span class="line">initLimit=10</span><br><span class="line">syncLimit=5</span><br><span class="line">clientPort=2181</span><br><span class="line"></span><br><span class="line">dataDir=/opt/data/zookeeper_data</span><br><span class="line"></span><br><span class="line">server.1=master:2888:3888</span><br><span class="line">server.2=slave1:2888:3888</span><br><span class="line">server.3=slave2:2888:3888</span><br><span class="line">scp /opt/zookeeper/conf/zoo.cfg  root@slave2:/opt/zookeeper/conf/zoo.cfg</span><br></pre></td></tr></table></figure><p>复制到其他节点</p><p>每台机器在<code>/opt/data/zookeeper_data</code>设置myid 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /opt/data/zookeeper_data</span><br><span class="line">master</span><br><span class="line">echo 1 &gt; /opt/data/zookeeper_data/myid</span><br><span class="line">slave1</span><br><span class="line">echo 2 &gt; /opt/data/zookeeper_data/myid</span><br><span class="line">slave2</span><br><span class="line">echo 3 &gt; /opt/data/zookeeper_data/myid</span><br></pre></td></tr></table></figure><p>添加可执行目录<code>/opt/zookeeper/bin</code>到环境变量</p><p>启动zookeeper测试</p><p>需要先关闭防火墙</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@master zookeeper]# systemctl stop firewalld</span><br><span class="line">[root@master zookeeper]# systemctl disable firewalld</span><br><span class="line">Removed symlink /etc/systemd/system/multi-user.target.wants/firewalld.service.</span><br><span class="line">Removed symlink /etc/systemd/system/dbus-org.fedoraproject.FirewallD1.service.</span><br><span class="line">[root@master zookeeper]#</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start启动zookeeper</span><br><span class="line">zkServer.sh status 查看身份</span><br></pre></td></tr></table></figure><p><img src="1571585208025.png" alt="1571585208025"></p><p><img src="1571585217826.png" alt="1571585217826"></p><p><img src="1571585229871.png" alt="1571585229871"></p><h3 id="Hadoop配置"><a href="#Hadoop配置" class="headerlink" title="Hadoop配置"></a>Hadoop配置</h3><p>解压配置hadoop</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> tar -zxvf hadoop-2.6.0.tar.gz -C /opt/^C</span><br><span class="line">mv /opt/hadoop-2.6.0/ /opt/hadoop</span><br><span class="line">添加可执行目录bin , sbin到PATH变量</span><br><span class="line"></span><br><span class="line">复制环境变量</span><br><span class="line">scp /root/.bash_profile root@slave1:/root/</span><br></pre></td></tr></table></figure><p>修改配置文件</p><ul><li><p>slaves</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">master</span><br><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>hadoop-env.sh</p><p>设置jdk路径即可</p></li><li><p>core-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- cluster1 为集群的 名字--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.ns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://ns<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:2181,slave1:2181,slave2:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop_tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul><ul><li><p>hdfs-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>ns<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.ns<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.ns.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.ns.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://master:8485;slave1:8485;slave2:8485/ns<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence</span><br><span class="line">shell(/bin/true)<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul><ul><li>mapred-site.xml</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>job web地址<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><p>yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">同一个ZooKeeper集群的不同Hadoop集群<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">同一个ZooKeeper集群的不同Hadoop集群<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>运行在nodemanager上的附属服务<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:2181,slave1:2181,slave2:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>是否启用HA，默认false<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>最少2个<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn-ha<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>集群HA的id，用于在ZooKeeper上创建节点，区分使用</span><br><span class="line"></span><br><span class="line">同一个ZooKeeper集群的不同Hadoop集群<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ul><p>初始化hadoop</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">scp -r /opt/hadoop/etc/hadoop/* root@slave1:/opt/hadoop/etc/hadoop/</span><br><span class="line">第一次启动需要按照下面的顺序初始化namenode</span><br><span class="line">1. 三台启动zookeeper</span><br><span class="line">zkServer.sh start</span><br><span class="line">2. 三台启动journal node</span><br><span class="line">hadoop-daemon.sh start journalnode</span><br><span class="line">3. 格式化zookeeper</span><br><span class="line">hdfs zkfc -formatZK</span><br><span class="line">4 master 格式化namenode</span><br><span class="line">hdfs namenode -format</span><br><span class="line">启动master上的namenode</span><br><span class="line">hadoop-daemon.sh start namenode</span><br><span class="line">5 slave1(备用namenode)复制master的namenode</span><br><span class="line"></span><br><span class="line">hdfs namenode -bootstrapStandby</span><br></pre></td></tr></table></figure><p>启动hadoop</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">1. 启动zookeeper</span><br><span class="line">2. 启动hadoop</span><br><span class="line">start-dfs.sh</span><br><span class="line">启动yarn</span><br><span class="line">start-yarn.sh</span><br><span class="line">3. 启动备用resourcemanager</span><br><span class="line">slave1 </span><br><span class="line">yarn-daemon.sh start resourcemanager</span><br><span class="line"></span><br><span class="line">4. 启动jobhistory进程</span><br><span class="line"> mr-jobhistory-daemon.sh start historyserver</span><br></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="查看进程"><a href="#查看进程" class="headerlink" title="查看进程"></a>查看进程</h3><p>master</p><p><img src="1571620265355.png" alt="1571620265355"></p><p>slave1</p><p><img src="1571620283076.png" alt="1571620283076"></p><p>slave2</p><p><img src="1571620295864.png" alt="1571620295864"></p><h3 id="查看web界面"><a href="#查看web界面" class="headerlink" title="查看web界面"></a>查看web界面</h3><ul><li><p>namenode主备</p><p>master:192.168.3.10:50070</p><p><img src="1571620455338.png" alt="1571620455338"></p><p>slave1:192.168.3.11:50070</p><p><img src="1571620473764.png" alt="1571620473764"></p></li><li><p>resourcemanager主备</p></li></ul><p>master:</p><p><a href="http://192.168.3.10:8088/cluster" target="_blank" rel="noopener">http://192.168.3.10:8088/cluster</a></p><p><img src="1571620537734.png" alt="1571620537734"></p><p>slave1上的备用</p><p><a href="http://192.168.3.11:8088/cluster" target="_blank" rel="noopener">http://192.168.3.11:8088/cluster</a></p><p>访问会自动重定向到master上的</p><h3 id="集群测试"><a href="#集群测试" class="headerlink" title="集群测试"></a>集群测试</h3><ol><li>wordcount测试</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">上传测试数据到hdfs</span><br><span class="line"> hdfs dfs -mkdir word_in</span><br><span class="line">  hdfs dfs -put /opt/hadoop/etc/hadoop/* /word_in</span><br><span class="line">运行hadoop自带的wordcount</span><br><span class="line"> hadoop jar  /opt/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.6.0.jar wordcount /word_in /word_out</span><br></pre></td></tr></table></figure><p><img src="1571622851366.png" alt="1571622851366"></p><p>jourhistory</p><p><img src="1571622893205.png" alt="1571622893205"></p><p><img src="1571622950115.png" alt="1571622950115"></p><p>输出结果</p><p><img src="1571622998104.png" alt="1571622998104"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Hadoop集群搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop集群搭建（二）配置ssh免密登录</title>
      <link href="/2019/10/20/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E9%85%8D%E7%BD%AEssh%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/"/>
      <url>/2019/10/20/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E4%BA%8C%EF%BC%89%E9%85%8D%E7%BD%AEssh%E5%85%8D%E5%AF%86%E7%99%BB%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<p>前提:</p><p>配置好网络,虚拟机可以互相ping</p><p>每台设置免密码自己和其他两台机器</p><p><img src="/1571581639806.png" alt="1571581639806"></p><a id="more"></a><ol><li><p>使用ssh-keygen命令生成公钥和私钥</p><p>输入ssh-keygen 全部选默认回车</p><p><img src="/1571581726603.png" alt="1571581726603"></p></li><li><p>使用ssh-copy-id 命令发送公钥到其他两台机器</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">master 为例</span><br><span class="line">ssh-copy-id master</span><br><span class="line">ssh-copy-id slave1</span><br><span class="line">ssh-copy-id slave2</span><br></pre></td></tr></table></figure></li></ol><p>   <img src="/1571581788086.png" alt="1571581788086"></p><ol start="3"><li>测试免密登录</li></ol><blockquote><p>直接输入ssh  slave1,不用输入密码即可登录就是成功</p></blockquote><p><img src="/1571581825834.png" alt="1571581825834"></p><p>依次设置三台机器免密登录三台机器</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Hadoop集群搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop集群搭建（一）Box搭建Hadoop集群网络配置</title>
      <link href="/2019/10/20/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E4%B8%80%EF%BC%89VBox%E6%90%AD%E5%BB%BAHadoop%E9%9B%86%E7%BE%A4%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/"/>
      <url>/2019/10/20/Hadoop%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%EF%BC%88%E4%B8%80%EF%BC%89VBox%E6%90%AD%E5%BB%BAHadoop%E9%9B%86%E7%BE%A4%E7%BD%91%E7%BB%9C%E9%85%8D%E7%BD%AE/</url>
      
        <content type="html"><![CDATA[<blockquote><p>之前集群用的是仅主机模式,不能连接外网,要用外网时切换成桥接，十分麻烦,现在可以设置成可以虚拟机互ping,同时也可以连接外网的配置了</p></blockquote><h2 id="集群规划"><a href="#集群规划" class="headerlink" title="集群规划"></a>集群规划</h2><blockquote><p>虚拟机系统:CentOS 7<br>宿主机:Ubuntu 或者 win</p></blockquote><p>机器    Ip    网络    主机名(hostname)<br>宿主机    172.18.17.241<br>Master虚拟机    192.168.3.10    Nat + 仅主机    master<br>Slave1 虚拟机    192.168.3.11    Nat + 仅主机    slave1<br>Slave2 虚拟机    192.168.3.12    Nat + 仅主机    slave2    </p><p>网络要求</p><ol><li><p>虚拟机设置静态ip地址,网段和宿主机的不一样 (用来设置host映射),</p></li><li><p>三台虚拟机互相ping(集群通讯)</p></li><li><p>主机ping三台虚拟机(传输文件)</p></li><li><p>三台虚拟机连接外网(用yum安装一些软件)</p></li><li><p>三台虚拟机ping主机</p><a id="more"></a></li></ol><p>其他不可行方案:</p><p>网络方式    </p><p>桥接模式    如果虚拟机设置的静态ip和net1的网段不一样，就无法连接外网<br>修改宿主机的网段为虚拟机需要的?没测试<br>Nat 模式    只能连接外网,无法虚拟机互ping<br>仅主机    虚拟机可以互ping,主机可以ping虚拟机,虚拟机应该可以ping主机<br>Nat + 仅主机 (虚拟机配置双网卡)    满足    </p><p><img src="1571542933047.png" alt="1571542933047"></p><h2 id="VBox-配置"><a href="#VBox-配置" class="headerlink" title="VBox 配置"></a>VBox 配置</h2><h3 id="VBox-仅主机网络设置"><a href="#VBox-仅主机网络设置" class="headerlink" title="VBox 仅主机网络设置"></a>VBox 仅主机网络设置</h3><p>创建一个仅主机网络(设置ip地址和集群的在统一网段)</p><p><img src="1571542957232.png" alt="1571542957232"></p><h2 id="虚拟机配置"><a href="#虚拟机配置" class="headerlink" title="虚拟机配置"></a>虚拟机配置</h2><h3 id="三台虚拟机设置"><a href="#三台虚拟机设置" class="headerlink" title="三台虚拟机设置"></a>三台虚拟机设置</h3><p>以master为例</p><p>使用两张网卡(网络适配器),第一张为NAT模式,第二张为仅主机(使用上面创建的)</p><p><img src="1571542972609.png" alt="1571542972609"></p><p><img src="1571542981892.png" alt="1571542981892"></p><h3 id="修改三台机器网卡配置文件"><a href="#修改三台机器网卡配置文件" class="headerlink" title="修改三台机器网卡配置文件"></a>修改三台机器网卡配置文件</h3><p>master为例</p><p>这里网卡名为enp0s3和enp0s8,分别对应nat模式的和仅主机模式的</p><p><strong>/etc/sysconfig/network-scripts/ifcfg-enp0s3</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">TYPE=Ethernet</span><br><span class="line"></span><br><span class="line">PROXY_METHOD=none</span><br><span class="line"></span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line"></span><br><span class="line">BOOTPROTO=dhcp</span><br><span class="line"></span><br><span class="line">DEFROUTE=yes</span><br><span class="line"></span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line"></span><br><span class="line">IPV6INIT=yes</span><br><span class="line"></span><br><span class="line">IPV6_AUTOCONF=yes</span><br><span class="line"></span><br><span class="line">IPV6_DEFROUTE=yes</span><br><span class="line"></span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line"></span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line"></span><br><span class="line">NAME=enp0s3</span><br><span class="line"></span><br><span class="line">UUID=af6d5285-33f4-4151-9687-246645365916</span><br><span class="line"></span><br><span class="line">DEVICE=enp0s3</span><br><span class="line"></span><br><span class="line">ONBOOT=yes</span><br></pre></td></tr></table></figure><p><strong>/etc/sysconfig/network-scripts/ifcfg-enp0s8</strong></p><p>这个的网关设置为nat模式那的网关</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">TYPE=Ethernet</span><br><span class="line"></span><br><span class="line">PROXY_METHOD=none</span><br><span class="line"></span><br><span class="line">BROWSER_ONLY=no</span><br><span class="line"></span><br><span class="line">BOOTPROTO=static</span><br><span class="line"></span><br><span class="line">DEFROUTE=yes</span><br><span class="line"></span><br><span class="line">IPV4_FAILURE_FATAL=no</span><br><span class="line"></span><br><span class="line">IPV6INIT=yes</span><br><span class="line"></span><br><span class="line">IPV6_AUTOCONF=yes</span><br><span class="line"></span><br><span class="line">IPV6_DEFROUTE=yes</span><br><span class="line"></span><br><span class="line">IPV6_FAILURE_FATAL=no</span><br><span class="line"></span><br><span class="line">IPV6_ADDR_GEN_MODE=stable-privacy</span><br><span class="line"></span><br><span class="line">NAME=enp0s8</span><br><span class="line"></span><br><span class="line">UUID=af6d5285-33f4-4151-9687-246645365916</span><br><span class="line"></span><br><span class="line">DEVICE=enp0s8</span><br><span class="line"></span><br><span class="line">ONBOOT=yes</span><br><span class="line"></span><br><span class="line">IPADDR=192.168.3.11</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">GATEWAY=10.0.2.2</span><br></pre></td></tr></table></figure><p>## </p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p><code>service network restart 重启网络</code></p><blockquote><p>下面是个人理解,某些词语用的不是很准确,表达的可能不够专业</p></blockquote><h3 id="1．查看路由"><a href="#1．查看路由" class="headerlink" title="1．查看路由"></a>1．查看路由</h3><h3 id="2-查看ip-地址"><a href="#2-查看ip-地址" class="headerlink" title="2. 查看ip 地址"></a>2. 查看ip 地址</h3><h3 id="3．效果"><a href="#3．效果" class="headerlink" title="3．效果"></a>3．效果</h3><ol><li><p>虚拟机互Ping</p><p><img src="1571543341398.png" alt="1571543341398"></p></li><li><p>虚拟机ping外网</p><p><img src="1571543350682.png" alt="1571543350682"></p></li><li><p>虚拟机Ping主机</p><p>(主机ip为172.18.17.241)</p><p><img src="1571543361943.png" alt="1571543361943"></p></li><li><p>主机ping虚拟机</p><p>(这里宿主机时ubuntu系统的)</p></li></ol><p><img src="1571543374188.png" alt="1571543374188"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Hadoop集群搭建 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hive自定义函数UDF</title>
      <link href="/2019/10/15/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0UDF/"/>
      <url>/2019/10/15/Hive%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0UDF/</url>
      
        <content type="html"><![CDATA[<h3 id="创建UDF-函数流程"><a href="#创建UDF-函数流程" class="headerlink" title="创建UDF 函数流程"></a>创建UDF 函数流程</h3><ol><li>编写jar</li><li>添加jar 到hive</li><li>注册函数</li></ol><p>函数有临时函数和永久函数,永久函数注册如果不指定数据库会在当前数据库下，名字为dbname.functionname</p><h3 id="1-编写打包jar-包"><a href="#1-编写打包jar-包" class="headerlink" title="1. 编写打包jar 包"></a>1. 编写打包jar 包</h3><p>hive maven 的配置(2.7.7为对应的hadoop版本,1.2.2为hive版本)</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.2.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-common<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>2.7.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h4><p>UDF为处理某列数据</p><ul><li><p>编写一个类继承UDF类</p></li><li><p>定义<code>evaluate</code>函数</p><p>如</p></li></ul><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">final</span> <span class="class"><span class="keyword">class</span> <span class="title">Lower</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">public</span> Text <span class="title">evaluate</span><span class="params">(<span class="keyword">final</span> Text s)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (s == <span class="keyword">null</span>) &#123; <span class="keyword">return</span> <span class="keyword">null</span>; &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> Text(s.toString().toLowerCase());</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="2-添加jar包到hive-中"><a href="#2-添加jar包到hive-中" class="headerlink" title="2. 添加jar包到hive 中"></a>2. 添加jar包到hive 中</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">添加本地jar</span><br><span class="line">add jar /root/xx.jar;</span><br><span class="line">添加hdfs上的jar</span><br><span class="line">add jar hdfs:/xx.jar</span><br></pre></td></tr></table></figure><h3 id="3-注册函数"><a href="#3-注册函数" class="headerlink" title="3. 注册函数"></a>3. 注册函数</h3><p>创建临时函数语法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">class_name 为上面创建的继承UDF的类</span><br><span class="line">CREATE TEMPORARY FUNCTION function_name AS class_name;</span><br></pre></td></tr></table></figure><p>创建永久函数语法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">可以使用using jar 添加jar包,如果hive不是本地模式,using jar 的文件要在不是本地的路径,比如hdfs上</span><br><span class="line">CREATE FUNCTION [db_name.]function_name AS class_name</span><br><span class="line"> [USING JAR|FILE|ARCHIVE &apos;file_uri&apos; [, JAR|FILE|ARCHIVE &apos;file_uri&apos;] ];</span><br></pre></td></tr></table></figure><p>删除函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">DROP TEMPORARY FUNCTION [IF EXISTS] function_name;</span><br><span class="line">DROP FUNCTION [IF EXISTS] function_name;</span><br></pre></td></tr></table></figure><p>查看创建的函数</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">查看所有函数</span><br><span class="line">show functions;</span><br><span class="line">过滤查看函数(通过正则表达式)</span><br><span class="line">show functions like &apos;sub*&apos;;</span><br></pre></td></tr></table></figure><p>注册后,就可以直接在hql里使用了</p><p>参考连接</p><ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/HivePlugins" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/HivePlugins</a></li><li></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive_DDL</title>
      <link href="/2019/10/11/Hive-DDL/"/>
      <url>/2019/10/11/Hive-DDL/</url>
      
        <content type="html"><![CDATA[<h2 id="DDL-Data-Definition-Language"><a href="#DDL-Data-Definition-Language" class="headerlink" title="DDL(Data Definition Language)"></a>DDL(Data Definition Language)</h2><h3 id="创建数据库"><a href="#创建数据库" class="headerlink" title="创建数据库"></a>创建数据库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name</span><br><span class="line">  [COMMENT database_comment]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [WITH DBPROPERTIES (property_name=property_value, ...)];</span><br></pre></td></tr></table></figure><h3 id="删除数据库"><a href="#删除数据库" class="headerlink" title="删除数据库"></a>删除数据库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DROP (DATABASE|SCHEMA) [IF EXISTS] database_name [RESTRICT|CASCADE];</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="修改数据库"><a href="#修改数据库" class="headerlink" title="修改数据库"></a>修改数据库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ALTER (DATABASE|SCHEMA) database_name SET DBPROPERTIES (property_name=property_value, ...);   -- (Note: SCHEMA added in Hive 0.14.0)</span><br><span class="line"> </span><br><span class="line">ALTER (DATABASE|SCHEMA) database_name SET OWNER [USER|ROLE] user_or_role;   -- (Note: Hive 0.13.0 and later; SCHEMA added in Hive 0.14.0)</span><br><span class="line">  </span><br><span class="line">ALTER (DATABASE|SCHEMA) database_name SET LOCATION hdfs_path; -- (Note: Hive 2.2.1, 2.4.0 and later)</span><br></pre></td></tr></table></figure><h3 id="使用数据库"><a href="#使用数据库" class="headerlink" title="使用数据库"></a>使用数据库</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">USE database_name;</span><br><span class="line">USE DEFAULT;</span><br><span class="line">查看当前数据库</span><br><span class="line">SELECT current_database() (as of Hive 0.13.0).</span><br></pre></td></tr></table></figure><h2 id="表"><a href="#表" class="headerlink" title="表"></a>表</h2><h3 id="创建表"><a href="#创建表" class="headerlink" title="创建表"></a>创建表</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br></pre></td><td class="code"><pre><span class="line">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name    -- (Note: TEMPORARY available in Hive 0.14.0 and later)</span><br><span class="line">  [(col_name data_type [column_constraint_specification] [COMMENT col_comment], ... [constraint_specification])]</span><br><span class="line">  [COMMENT table_comment]</span><br><span class="line">  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]</span><br><span class="line">  [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]</span><br><span class="line">  [SKEWED BY (col_name, col_name, ...)                  -- (Note: Available in Hive 0.10.0 and later)]</span><br><span class="line">     ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)</span><br><span class="line">     [STORED AS DIRECTORIES]</span><br><span class="line">  [</span><br><span class="line">   [ROW FORMAT row_format] </span><br><span class="line">   [STORED AS file_format]</span><br><span class="line">     | STORED BY &apos;storage.handler.class.name&apos; [WITH SERDEPROPERTIES (...)]  -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  ]</span><br><span class="line">  [LOCATION hdfs_path]</span><br><span class="line">  [TBLPROPERTIES (property_name=property_value, ...)]   -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  [AS select_statement];   -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)</span><br><span class="line"> </span><br><span class="line">CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name</span><br><span class="line">  LIKE existing_table_or_view_name</span><br><span class="line">  [LOCATION hdfs_path];</span><br><span class="line"> </span><br><span class="line">data_type</span><br><span class="line">  : primitive_type</span><br><span class="line">  | array_type</span><br><span class="line">  | map_type</span><br><span class="line">  | struct_type</span><br><span class="line">  | union_type  -- (Note: Available in Hive 0.7.0 and later)</span><br><span class="line"> </span><br><span class="line">primitive_type</span><br><span class="line">  : TINYINT</span><br><span class="line">  | SMALLINT</span><br><span class="line">  | INT</span><br><span class="line">  | BIGINT</span><br><span class="line">  | BOOLEAN</span><br><span class="line">  | FLOAT</span><br><span class="line">  | DOUBLE</span><br><span class="line">  | DOUBLE PRECISION -- (Note: Available in Hive 2.2.0 and later)</span><br><span class="line">  | STRING</span><br><span class="line">  | BINARY      -- (Note: Available in Hive 0.8.0 and later)</span><br><span class="line">  | TIMESTAMP   -- (Note: Available in Hive 0.8.0 and later)</span><br><span class="line">  | DECIMAL     -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line">  | DECIMAL(precision, scale)  -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line">  | DATE        -- (Note: Available in Hive 0.12.0 and later)</span><br><span class="line">  | VARCHAR     -- (Note: Available in Hive 0.12.0 and later)</span><br><span class="line">  | CHAR        -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line"> </span><br><span class="line">array_type</span><br><span class="line">  : ARRAY &lt; data_type &gt;</span><br><span class="line"> </span><br><span class="line">map_type</span><br><span class="line">  : MAP &lt; primitive_type, data_type &gt;</span><br><span class="line"> </span><br><span class="line">struct_type</span><br><span class="line">  : STRUCT &lt; col_name : data_type [COMMENT col_comment], ...&gt;</span><br><span class="line"> </span><br><span class="line">union_type</span><br><span class="line">   : UNIONTYPE &lt; data_type, data_type, ... &gt;  -- (Note: Available in Hive 0.7.0 and later)</span><br><span class="line"> </span><br><span class="line">row_format</span><br><span class="line">  : DELIMITED [FIELDS TERMINATED BY char [ESCAPED BY char]] [COLLECTION ITEMS TERMINATED BY char]</span><br><span class="line">        [MAP KEYS TERMINATED BY char] [LINES TERMINATED BY char]</span><br><span class="line">        [NULL DEFINED AS char]   -- (Note: Available in Hive 0.13 and later)</span><br><span class="line">  | SERDE serde_name [WITH SERDEPROPERTIES (property_name=property_value, property_name=property_value, ...)]</span><br><span class="line"> </span><br><span class="line">file_format:</span><br><span class="line">  : SEQUENCEFILE</span><br><span class="line">  | TEXTFILE    -- (Default, depending on hive.default.fileformat configuration)</span><br><span class="line">  | RCFILE      -- (Note: Available in Hive 0.6.0 and later)</span><br><span class="line">  | ORC         -- (Note: Available in Hive 0.11.0 and later)</span><br><span class="line">  | PARQUET     -- (Note: Available in Hive 0.13.0 and later)</span><br><span class="line">  | AVRO        -- (Note: Available in Hive 0.14.0 and later)</span><br><span class="line">  | JSONFILE    -- (Note: Available in Hive 4.0.0 and later)</span><br><span class="line">  | INPUTFORMAT input_format_classname OUTPUTFORMAT output_format_classname</span><br><span class="line"> </span><br><span class="line">column_constraint_specification:</span><br><span class="line">  : [ PRIMARY KEY|UNIQUE|NOT NULL|DEFAULT [default_value]|CHECK  [check_expression] ENABLE|DISABLE NOVALIDATE RELY/NORELY ]</span><br><span class="line"> </span><br><span class="line">default_value:</span><br><span class="line">  : [ LITERAL|CURRENT_USER()|CURRENT_DATE()|CURRENT_TIMESTAMP()|NULL ] </span><br><span class="line"> </span><br><span class="line">constraint_specification:</span><br><span class="line">  : [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE RELY/NORELY ]</span><br><span class="line">    [, PRIMARY KEY (col_name, ...) DISABLE NOVALIDATE RELY/NORELY ]</span><br><span class="line">    [, CONSTRAINT constraint_name FOREIGN KEY (col_name, ...) REFERENCES table_name(col_name, ...) DISABLE NOVALIDATE </span><br><span class="line">    [, CONSTRAINT constraint_name UNIQUE (col_name, ...) DISABLE NOVALIDATE RELY/NORELY ]</span><br><span class="line">    [, CONSTRAINT constraint_name CHECK [check_expression] ENABLE|DISABLE NOVALIDATE RELY/NORELY ]</span><br></pre></td></tr></table></figure><ul><li>表名和字段名是不区分大小写的,但是SerDe和property name 是区分大小写的</li><li>Hive 0.13 后,列名可以包含Unicode 字符串,但 <code>.</code>和<code>:</code>会在查询时报错,在hive 1.2里不允许</li><li>表和字段的注释是字符串常量(用单引号)</li><li>不用EXTERNAL 语句创建的表被hive管理，叫做managed table,查看表是不是external的,用describle extend table_name</li></ul><h4 id="Managed-and-External-Tables"><a href="#Managed-and-External-Tables" class="headerlink" title="Managed and External Tables"></a>Managed and External Tables</h4>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Hive数据类型</title>
      <link href="/2019/10/11/Hive%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/"/>
      <url>/2019/10/11/Hive%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-HandlingofNULLValues" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-HandlingofNULLValues</a></p><h3 id="数字类型"><a href="#数字类型" class="headerlink" title="数字类型"></a>数字类型</h3><ul><li><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-tinyint" target="_blank" rel="noopener"><code>TINYINT</code></a> (1-byte signed integer, from <code>-128</code> to <code>127</code>)</p></li><li><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-smallint" target="_blank" rel="noopener"><code>SMALLINT</code></a> (2-byte signed integer, from <code>-32,768</code> to <code>32,767</code>)</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">INT/INTEGER (4-byte signed integer, from -2,147,483,648 to 2,147,483,647)</span><br></pre></td></tr></table></figure></li><li><p><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-bigint" target="_blank" rel="noopener"><code>BIGINT</code></a> (8-byte signed integer, from <code>-9,223,372,036,854,775,808</code> to <code>9,223,372,036,854,775,807</code>)</p></li><li><p><code>FLOAT</code> (4-byte single precision floating point number)</p></li><li><p><code>DOUBLE</code> (8-byte double precision floating point number)</p></li><li><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DOUBLE PRECISION (alias for DOUBLE, only available starting with Hive 2.2.0)</span><br></pre></td></tr></table></figure></li><li><p><code>DECIMAL</code></p><ul><li>Introduced in Hive <a href="https://issues.apache.org/jira/browse/HIVE-2693" target="_blank" rel="noopener">0.11.0</a> with a precision of 38 digits</li><li>Hive <a href="https://issues.apache.org/jira/browse/HIVE-3976" target="_blank" rel="noopener">0.13.0</a> introduced user-definable precision and scale</li></ul></li><li><p><code>NUMERIC</code> (same as <code>DECIMAL</code>, starting with <a href="https://issues.apache.org/jira/browse/HIVE-16764" target="_blank" rel="noopener">Hive 3.0.0</a>)</p></li></ul><p>Date/Time 类型</p><ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-timestamp" target="_blank" rel="noopener"><code>TIMESTAMP</code></a> (Note: Only available starting with Hive <a href="https://issues.apache.org/jira/browse/HIVE-2272" target="_blank" rel="noopener">0.8.0</a>)</li><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-date" target="_blank" rel="noopener"><code>DATE</code></a> (Note: Only available starting with Hive <a href="https://issues.apache.org/jira/browse/HIVE-4055" target="_blank" rel="noopener">0.12.0</a>)</li><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-Intervals" target="_blank" rel="noopener"><code>INTERVAL</code></a> (Note: Only available starting with Hive <a href="https://issues.apache.org/jira/browse/HIVE-9792" target="_blank" rel="noopener">1.2.0</a>)</li></ul><a id="more"></a><h3 id="String-Types"><a href="#String-Types" class="headerlink" title="String Types"></a>String Types</h3><ul><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-string" target="_blank" rel="noopener"><code>STRING</code></a></li><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-varchar" target="_blank" rel="noopener"><code>VARCHAR</code></a> (Note: Only available starting with Hive <a href="https://issues.apache.org/jira/browse/HIVE-4844" target="_blank" rel="noopener">0.12.0</a>)</li><li><a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Types#LanguageManualTypes-char" target="_blank" rel="noopener"><code>CHAR</code></a> (Note: Only available starting with Hive <a href="https://issues.apache.org/jira/browse/HIVE-5191" target="_blank" rel="noopener">0.13.0</a>)</li></ul><h3 id="其他类型"><a href="#其他类型" class="headerlink" title="其他类型"></a>其他类型</h3><ul><li><code>BOOLEAN</code></li><li><code>BINARY</code> (Note: Only available starting with Hive <a href="https://issues.apache.org/jira/browse/HIVE-2380" target="_blank" rel="noopener">0.8.0</a>)</li></ul><h3 id="复杂类型"><a href="#复杂类型" class="headerlink" title="复杂类型"></a>复杂类型</h3><ul><li>arrays: <code>ARRAY&lt;data_type&gt;</code> (Note: negative values and non-constant expressions are allowed as of <a href="https://issues.apache.org/jira/browse/HIVE-7325" target="_blank" rel="noopener">Hive 0.14</a>.)</li><li>maps: <code>MAP&lt;primitive_type, data_type&gt;</code> (Note: negative values and non-constant expressions are allowed as of <a href="https://issues.apache.org/jira/browse/HIVE-7325" target="_blank" rel="noopener">Hive 0.14</a>.)</li><li>structs: <code>STRUCT&lt;col_name : data_type [COMMENT col_comment], ...&gt;</code></li><li>union: <code>UNIONTYPE&lt;data_type, data_type, ...&gt;</code> (Note: Only available starting with Hive <a href="https://issues.apache.org/jira/browse/HIVE-537" target="_blank" rel="noopener">0.7.0</a>.)</li></ul><h2 id="列类型"><a href="#列类型" class="headerlink" title="列类型"></a>列类型</h2><h3 id="整数"><a href="#整数" class="headerlink" title="整数"></a>整数</h3><h3 id="字符串string"><a href="#字符串string" class="headerlink" title="字符串string"></a>字符串string</h3><p>字符串可以用单引号或者双引号表示</p><h3 id="Varchar"><a href="#Varchar" class="headerlink" title="Varchar"></a>Varchar</h3><p>varchar 创建时指定最大的长度(在1到65535之间),超过的会被丢弃</p><p>varchar和string一样,字符串后面的空格会影响比较的结果</p><h3 id="char"><a href="#char" class="headerlink" title="char"></a>char</h3><p>char 和 varchar类似,最大的长度是255,小于的会用空格补充</p><p>后面的空格不会影响比较的结果</p><h3 id="timestamps"><a href="#timestamps" class="headerlink" title="timestamps"></a>timestamps</h3><p>支持传统的UNIX时间戳,和可选的纳秒精度</p><p>支持的转换</p><ul><li>整数数字类型:解析成秒的UNIX时间戳</li><li>浮点数字类型:解析成带有小数的秒的UNIX时间戳</li><li>字符串:遵循JDBC java.sql.Timestamp 格式”<code>YYYY-MM-DD HH:MM:SS.fffffffff</code>“ (9 decimal place precision)</li></ul><h3 id="Dates"><a href="#Dates" class="headerlink" title="Dates"></a>Dates</h3><p>Date类型表示特定的year/month/day,用YYYY-MM-DD的形式,比如’<code>2019-09-02</code>,支持的范围为0000-01-01到9999-12-31</p><blockquote><p>Date类型在Hive 0.12里引入</p></blockquote><h3 id="Casting-Dates"><a href="#Casting-Dates" class="headerlink" title="Casting Dates"></a>Casting Dates</h3><p>Date只能由Date,Timestamp,字符串 转换来或者转换成</p><p>指定转换的格式:<a href="https://cwiki.apache.org/confluence/display/Hive/CAST...FORMAT+with+SQL%3A2016+datetime+formats" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/CAST...FORMAT+with+SQL%3A2016+datetime+formats</a></p><table><thead><tr><th>有效的转换</th><th></th></tr></thead><tbody><tr><td>cast(timestamp as date)</td><td>The year/month/day of the timestamp is determined, based on the local timezone, and returned as a date value.</td></tr><tr><td>cast(string as date)</td><td>If  the string is in the form ‘YYYY-MM-DD’, then a date value corresponding  to that year/month/day is returned. If the string value does not match  this formate, then NULL is returned.</td></tr><tr><td>cast(date as timestamp)</td><td>A timestamp value is generated corresponding to midnight of the year/month/day of the date value, based on the local timezone.</td></tr><tr><td>cast(date as string)</td><td>The year/month/day represented by the Date is formatted as a string in the form ‘YYYY-MM-DD’.</td></tr><tr><td>cast(date as date)</td><td>Same date value</td></tr></tbody></table><h3 id="Decimal-常量"><a href="#Decimal-常量" class="headerlink" title="Decimal 常量"></a>Decimal 常量</h3><p>大于BIGINT的整数常量必须用Decimal(39,0)处理,要用前缀BD</p><p>例如</p><p><code>select CAST(18446744073709001000BD AS DECIMAL(38,0)) from my_table limit 1;</code></p><h3 id="空值处理"><a href="#空值处理" class="headerlink" title="空值处理"></a>空值处理</h3><p>缺失值用特殊的NULL值代表. 导入有NULL值的字段表使用的SerDe的文档(默认文本格式使用LazySimpleSerDe ,导入数据时将字符串\N解析为NULL)</p>]]></content>
      
      
      <categories>
          
          <category> hive笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
            <tag> hive笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive_Cli</title>
      <link href="/2019/10/11/Hive-Cli/"/>
      <url>/2019/10/11/Hive-Cli/</url>
      
        <content type="html"><![CDATA[<p>Hive 客户端 client</p><p>$HIVE_HOME/bin/hive is a shell utility which can be used to run Hive queries in either interactive or batch mode.</p><p>$HIVE_HOME/bin/hive 可以用来交互式shell和批处理模式里执行HQL 查询</p><p>HiveServer2 (在Hive 0.11里引入) 有自己的CLI叫做 Beeline,是个基于SQLLine的 JDBC客户端</p><p>Hive CLi 将来可能会过期,用HiveServer2代替</p><h3 id="Hive命令行参数"><a href="#Hive命令行参数" class="headerlink" title="Hive命令行参数"></a>Hive命令行参数</h3><p>使用<code>hive -H或者hive --help</code>查看帮助</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">usage: hive</span><br><span class="line"> -d,--define &lt;key=value&gt;          Variable substitution to apply to Hive</span><br><span class="line">                                  commands. e.g. -d A=B or --define A=B</span><br><span class="line"> -e &lt;quoted-query-string&gt;         SQL from command line</span><br><span class="line"> -f &lt;filename&gt;                    SQL from files</span><br><span class="line"> -H,--help                        Print help information</span><br><span class="line"> -h &lt;hostname&gt;                    连接到远程host的Hive ServerConnecting to Hive Server on remote host</span><br><span class="line"> </span><br><span class="line">    --hiveconf &lt;property=value&gt;   Use value for given property</span><br><span class="line">    --hivevar &lt;key=value&gt;         Variable substitution to apply to hive</span><br><span class="line">                                  commands. e.g. --hivevar A=B</span><br><span class="line"> -i &lt;filename&gt;                    Initialization SQL file</span><br><span class="line"> -p &lt;port&gt;                        Connecting to Hive Server on port number</span><br><span class="line"> -S,--silent                      Silent mode in interactive shell</span><br><span class="line"> -v,--verbose                     Verbose mode (echo executed SQL to the</span><br><span class="line">                                  console)</span><br><span class="line">从Hive 0.10.0开始有个额外的命令</span><br><span class="line">--database 指定使用的数据库</span><br><span class="line">支持</span><br><span class="line">-hiveconf和 --hiveconf</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="hiverc文件"><a href="#hiverc文件" class="headerlink" title="hiverc文件"></a>hiverc文件</h3><p>CLI 不带 -i 选项时会默认加载 $HIVE_HOME/bin/.hiverc和$HOME/.hiverc 作为初始化文件</p><p>Logging 日志</p><p>Hive 使用log4j来打印日志,默认日志不会输出到标准输出,但会输出到Hive 的log4j 配置文件指定的文件.</p><p>默认使用的配置文件是 Hive 安装目录的conf/下的<code>hive-log4j.default</code>,日志文件写出到/temp/<userid>/hive.log,日志等级是WARN</userid></p><p>如果想输出日志到标准输出,和改变日志等级，可以在命令行里指定</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$HIVE_HOME/bin/hive --hiveconf hive.root.logger=INFO,console</span><br></pre></td></tr></table></figure><p>hive.root.logger 指定日志的等级和日志的输出位置,</p><h3 id="Hive-批处理模式命令"><a href="#Hive-批处理模式命令" class="headerlink" title="Hive 批处理模式命令"></a>Hive 批处理模式命令</h3><p>用 -e 或者 -f选项时会用批处理模式执行SQL命令</p><ul><li>hive -e ‘<query-string>‘</query-string></li><li>hive -f <filepath></filepath></li></ul><blockquote><p>从Hive 0.14 开始<filepath>支持时Hadoop 支持的文件系统里的文件(hdfs)</filepath></p></blockquote><h3 id="Hive-Resources"><a href="#Hive-Resources" class="headerlink" title="Hive Resources"></a>Hive Resources</h3><p>hive 可以管理一些执行查询时用到的文件，比如普通文件,jars,archives压缩包,本地访问的文件都可以添加到Hive的会话里</p><p>用法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ADD &#123; FILE[S] | JAR[S] | ARCHIVE[S] &#125; &lt;filepath1&gt; [&lt;filepath2&gt;]*</span><br><span class="line">   LIST &#123; FILE[S] | JAR[S] | ARCHIVE[S] &#125; [&lt;filepath1&gt; &lt;filepath2&gt; ..]</span><br><span class="line">   DELETE &#123; FILE[S] | JAR[S] | ARCHIVE[S] &#125; [&lt;filepath1&gt; &lt;filepath2&gt; ..]</span><br></pre></td></tr></table></figure><ul><li>FILES 文件只是添加到distrubuted cache,可能是一些用来执行的转换(transform)脚本</li><li>JARS 资源被添加到Java 的classpath,比如用UDF时</li><li>ARCHIVE 压缩包资源会被自动解压,</li></ul><p>例子</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; add FILE /tmp/tt.py;</span><br><span class="line">  hive&gt; list FILES;</span><br><span class="line">  /tmp/tt.py</span><br><span class="line">  hive&gt; select from networks a </span><br><span class="line">               MAP a.networkid </span><br><span class="line">               USING &apos;python tt.py&apos; as nn where a.ds = &apos;2009-01-04&apos; limit 10;</span><br></pre></td></tr></table></figure><h3 id="HCatalog-CLI"><a href="#HCatalog-CLI" class="headerlink" title="HCatalog CLI"></a>HCatalog CLI</h3>]]></content>
      
      
      <categories>
          
          <category> hive笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
            <tag> hive笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive命令行命令</title>
      <link href="/2019/10/11/Hive%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%91%BD%E4%BB%A4/"/>
      <url>/2019/10/11/Hive%E5%91%BD%E4%BB%A4%E8%A1%8C%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<ul><li>可以在hive shell 里直接使用dfs命令操作hdfs上的文件</li></ul><table><thead><tr><th>命令</th><th>说明</th></tr></thead><tbody><tr><td>quit   exit</td><td>退出shell  Use quit or exit to leave the interactive shell.</td></tr><tr><td>reset</td><td>Resets the configuration to the default values (as of Hive 0.10: see <a href="https://issues.apache.org/jira/browse/HIVE-3202" target="_blank" rel="noopener">HIVE-3202</a>).  Any configuration parameters that were set using the set command or  -hiveconf parameter in hive commandline will get reset to default value.Note  that this does not apply to configuration parameters that were set in  set command using the “hiveconf:” prefix for the key name (for historic  reasons).</td></tr><tr><td>set <key>=<value></value></key></td><td>Sets the value of a particular configuration variable (key).   <strong>Note:</strong> If you misspell the variable name, the CLI will not show an error.</td></tr><tr><td>set</td><td>Prints a list of configuration variables that are overridden by the user or Hive.</td></tr><tr><td>set -v</td><td>Prints all Hadoop and Hive configuration variables.</td></tr><tr><td>add FILE[S] <filepath> <filepath>*   add JAR[S] <filepath> <filepath>*   add ARCHIVE[S] <filepath> <filepath>*</filepath></filepath></filepath></filepath></filepath></filepath></td><td>Adds one or more files, jars, or archives to the list of resources in the distributed cache. See <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-HiveResources" target="_blank" rel="noopener">Hive Resources</a> for more information.</td></tr><tr><td>add FILE[S] <ivyurl> <ivyurl>*  add JAR[S] <ivyurl> <ivyurl>*  add ARCHIVE[S]<ivyurl> <ivyurl>*</ivyurl></ivyurl></ivyurl></ivyurl></ivyurl></ivyurl></td><td>As of <a href="https://issues.apache.org/jira/browse/HIVE-9664" target="_blank" rel="noopener">Hive 1.2.0</a>, adds one or more files, jars or archives to the list of resources in the distributed cache using an <a href="http://ant.apache.org/ivy/" target="_blank" rel="noopener">Ivy</a> URL of the form ivy://group:module:version?query_string. See <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-HiveResources" target="_blank" rel="noopener">Hive Resources</a> for more information.</td></tr><tr><td>list FILE[S]   list JAR[S]   list ARCHIVE[S]</td><td>Lists the resources already added to the distributed cache. See <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-HiveResources" target="_blank" rel="noopener">Hive Resources</a> for more information.</td></tr><tr><td>list FILE[S] <filepath>*   list JAR[S] <filepath>*   list ARCHIVE[S] <filepath>*</filepath></filepath></filepath></td><td>Checks whether the given resources are already added to the distributed cache or not. See <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-HiveResources" target="_blank" rel="noopener">Hive Resources</a> for more information.</td></tr><tr><td>delete FILE[S] <filepath>*   delete JAR[S] <filepath>*   delete ARCHIVE[S] <filepath>*</filepath></filepath></filepath></td><td>Removes the resource(s) from the distributed cache.</td></tr><tr><td>delete FILE[S] <ivyurl> <ivyurl>*  delete JAR[S] <ivyurl> <ivyurl>*  delete ARCHIVE[S] <ivyurl> <ivyurl>*</ivyurl></ivyurl></ivyurl></ivyurl></ivyurl></ivyurl></td><td>As of <a href="https://issues.apache.org/jira/browse/HIVE-9664" target="_blank" rel="noopener">Hive 1.2.0</a>, removes the resource(s) which were added using the <ivyurl> from the distributed cache. See <a href="https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Cli#LanguageManualCli-HiveResources" target="_blank" rel="noopener">Hive Resources</a> for more information.</ivyurl></td></tr><tr><td>! <command></td><td>Executes a shell command from the Hive shell.</td></tr><tr><td>dfs <dfs command></dfs></td><td>在hive shell 里执行 dfs命令  Executes a dfs command from the Hive shell.</td></tr><tr><td><query string></query></td><td>Executes a Hive query and prints results to standard output.</td></tr><tr><td>source FILE <filepath></filepath></td><td>Executes a script file inside the CLI.</td></tr><tr><td>compile <code>&lt;groovy string&gt;</code> AS GROOVY NAMED <name></name></td><td>This allows inline Groovy code to be compiled and be used as a UDF (as of Hive <a href="https://issues.apache.org/jira/browse/HIVE-5252" target="_blank" rel="noopener">0.13.0</a>). For a usage example, see <a href="https://cwiki.apache.org/confluence/download/attachments/27362054/HiveContrib-Nov13-groovy_plus_hive.pptx?version=1&modificationDate=1385171856000&api=v2" target="_blank" rel="noopener">Nov. 2013 Hive Contributors Meetup Presentations – Using Dynamic Compilation with Hive</a>.</td></tr></tbody></table>]]></content>
      
      
      <categories>
          
          <category> hive笔记 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
            <tag> hive笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive基本介绍</title>
      <link href="/2019/10/11/Hive%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/"/>
      <url>/2019/10/11/Hive%E5%9F%BA%E6%9C%AC%E4%BB%8B%E7%BB%8D/</url>
      
        <content type="html"><![CDATA[<p>文章连接:</p><p><a href="https://cwiki.apache.org/confluence/display/Hive/Tutorial#Tutorial-HiveTutorial" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/Tutorial#Tutorial-HiveTutorial</a></p><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>Hive 是一个基于Hadoop 的数据仓库</p><h3 id="数据单位"><a href="#数据单位" class="headerlink" title="数据单位"></a>数据单位</h3><p>按照粒度的大小可以分为</p><ul><li><p>DataBases</p><p>数据库</p></li><li><p>Tables</p><p>表</p></li><li><p>Partitions</p><p>分区</p></li><li><p>Buckets</p><p>桶</p></li></ul><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><p>数据类型分为原始数据类型，复杂数据类型</p><a id="more"></a><h4 id="原始数据类型"><a href="#原始数据类型" class="headerlink" title="原始数据类型"></a>原始数据类型</h4><ul><li><p>Integers</p><p>整数</p><ul><li>TINYINT—1 byte integer</li><li>SMALLINT—2 byte integer</li><li>INT—4 byte integer</li><li>BIGINT—8 byte integer</li></ul></li><li><p>Boolean type</p><ul><li>BOOLEAN—TRUE/FALSE</li></ul></li><li><p>浮点数</p><ul><li>FLOAT—single precision</li><li>DOUBLE—Double precision</li></ul></li><li><p>Fixed point numbers</p><ul><li><p>DECIMAL—a fixed point value of user defined scale and precision</p><p>定点数 还是指定 规格的小数</p></li></ul></li><li><p>String types</p><ul><li>STRING—sequence of characters in a specified character set</li><li>VARCHAR— 有最大长度 sequence of characters in a specified character set with a maximum length</li><li>CHAR—指定长度的 sequence of characters in a specified character set with a defined length </li></ul></li><li><p>Date and time types</p><ul><li>TIMESTAMP — A date and time without a timezone (“LocalDateTime” semantics) 没有时区日期时间</li><li>TIMESTAMP WITH LOCAL TIME ZONE — A point in time measured down to nanoseconds (“Instant” semantics)</li><li>DATE—a date 日期</li></ul></li><li><p>Binary types</p><ul><li>BINARY—a sequence of bytes</li></ul></li></ul><h3 id="复杂数据类型"><a href="#复杂数据类型" class="headerlink" title="复杂数据类型"></a>复杂数据类型</h3><p>复杂类型由原始类型或者其他混合类型组成</p><ul><li><p>Structs</p><p>结构体 , 元素通过.来访问,如<code>xx.a</code></p></li><li><p>Maps </p><p>键值对的</p></li><li><p>Arrays</p><p>数组,必须是相同类型的,通过 [n]索引来访问</p></li></ul><h3 id="内置功能和函数"><a href="#内置功能和函数" class="headerlink" title="内置功能和函数"></a>内置功能和函数</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">查看所有函数</span><br><span class="line">show functions</span><br><span class="line">查看函数信息</span><br><span class="line">describle function &lt;function_name&gt;</span><br><span class="line">查看函数详情</span><br><span class="line">describle function extended &lt;function_name&gt;</span><br></pre></td></tr></table></figure><blockquote><p>hive的关键词是不区分大小写的</p></blockquote><h3 id="内置操作"><a href="#内置操作" class="headerlink" title="内置操作"></a>内置操作</h3><ul><li>关系操作</li></ul><table><thead><tr><th>关系操作符</th><th>操作数类型</th><th>描述</th></tr></thead><tbody><tr><td>A = B</td><td>所有类型</td><td></td></tr><tr><td>A != B</td><td></td><td></td></tr><tr><td>A &lt; B</td><td></td><td></td></tr><tr><td>A &lt;= B</td><td></td><td></td></tr><tr><td>A &gt; B</td><td></td><td></td></tr><tr><td>A &gt;= B</td><td></td><td></td></tr><tr><td>A IS NULL</td><td></td><td></td></tr><tr><td>A IS NOT NULL</td><td></td><td></td></tr><tr><td>A LIKE B</td><td>strings</td><td></td></tr><tr><td>A RLIKE B</td><td>strings</td><td>如果A满足简单的SQL 规则B _符号匹配任意字符,%匹配任意数量的字符 %和;转义用\</td></tr><tr><td>A REGEXP B</td><td>strings</td><td>和RLIKE一样</td></tr></tbody></table><table><thead><tr><th>算术操作符</th><th>操作数类型</th><th></th></tr></thead><tbody><tr><td>A + B</td><td>所有类型</td><td></td></tr><tr><td>A - B</td><td></td><td></td></tr><tr><td>A * B</td><td></td><td></td></tr><tr><td>A / B</td><td></td><td></td></tr><tr><td>A %  B</td><td></td><td></td></tr><tr><td>A &amp; B</td><td></td><td></td></tr><tr><td>A | B</td><td></td><td></td></tr><tr><td>A ^ B</td><td></td><td></td></tr><tr><td>~A</td><td></td><td></td></tr></tbody></table><table><thead><tr><th>逻辑操作符</th><th></th><th></th></tr></thead><tbody><tr><td>A AND B</td><td>boolean</td><td></td></tr><tr><td>A &amp;&amp; B</td><td></td><td></td></tr><tr><td>A OR B</td><td></td><td></td></tr><tr><td>A||B</td><td></td><td></td></tr><tr><td>NOT A</td><td></td><td></td></tr><tr><td>!A</td><td></td><td></td></tr></tbody></table><table><thead><tr><th>复杂类型</th><th></th><th></th></tr></thead><tbody><tr><td>A[n]</td><td>数组</td><td></td></tr><tr><td>M[key]</td><td>M是Map&lt;K,V&gt;类型,key是K类型</td><td></td></tr><tr><td>S.x</td><td>s是struct</td><td></td></tr></tbody></table><h3 id="内置函数"><a href="#内置函数" class="headerlink" title="内置函数"></a>内置函数</h3><table><thead><tr><th>返回类型</th><th>函数签名</th><th>描述</th></tr></thead><tbody><tr><td>T</td><td>round(double a)</td><td>returns the rounded BIGINT value of the double</td></tr><tr><td>BIGINT</td><td>floor(double a)</td><td>returns the maximum BIGINT value that is equal or less than the double</td></tr><tr><td>BIGINT</td><td>ceil(double a)</td><td>returns the minimum BIGINT value that is equal or greater than the double</td></tr><tr><td>double</td><td>rand(), rand(int seed)</td><td>returns  a random number (that changes from row to row). Specifiying the seed  will make sure the generated random number sequence is deterministic.</td></tr><tr><td>string</td><td>concat(string A, string B,…)</td><td>returns  the string resulting from concatenating B after A. For example,  concat(‘foo’, ‘bar’) results in ‘foobar’. This function accepts  arbitrary number of arguments and return the concatenation of all of  them.</td></tr><tr><td>string</td><td>substr(string A, int start)</td><td>returns  the substring of A starting from start position till the end of string  A. For example, substr(‘foobar’, 4) results in ‘bar’</td></tr><tr><td>string</td><td>substr(string A, int start, int length)</td><td>returns the substring of A starting from start position with the given length, for example,  substr(‘foobar’, 4, 2) results in ‘ba’</td></tr><tr><td>string</td><td>upper(string A)</td><td>returns the string resulting from converting all characters of A to upper case, for example, upper(‘fOoBaR’) results in ‘FOOBAR’</td></tr><tr><td>string</td><td>ucase(string A)</td><td>Same as upper</td></tr><tr><td>string</td><td>lower(string A)</td><td>returns the string resulting from converting all characters of B to lower case, for example, lower(‘fOoBaR’) results in ‘foobar’</td></tr><tr><td>string</td><td>lcase(string A)</td><td>Same as lower</td></tr><tr><td>string</td><td>trim(string A)</td><td>returns the string resulting from trimming spaces from both ends of A, for example, trim(‘ foobar ‘) results in ‘foobar’</td></tr><tr><td>string</td><td>ltrim(string A)</td><td>returns  the string resulting from trimming spaces from the beginning(left hand  side) of A. For example, ltrim(‘ foobar ‘) results in ‘foobar ‘</td></tr><tr><td>string</td><td>rtrim(string A)</td><td>returns  the string resulting from trimming spaces from the end(right hand side)  of A. For example, rtrim(‘ foobar ‘) results in ‘ foobar’</td></tr><tr><td>string</td><td>regexp_replace(string A, string B, string C)</td><td>returns the string resulting from replacing all substrings in B that match the Java regular expression syntax(See <a href="http://java.sun.com/j2se/1.4.2/docs/api/java/util/regex/Pattern.html" target="_blank" rel="noopener">Java regular expressions syntax</a>) with C. For example, regexp_replace(‘foobar’, ‘oo|ar’, ) returns ‘fb’</td></tr><tr><td>int</td><td>size(Map&lt;K.V&gt;)</td><td>returns the number of elements in the map type</td></tr><tr><td>int</td><td>size(Array<t>)</t></td><td>returns the number of elements in the array type</td></tr><tr><td><em>value of <type></type></em></td><td>cast(*<expr>* as *<type>*)</type></expr></td><td>converts  the results of the expression expr to <type>, for example,  cast(‘1’ as BIGINT) will convert the string ‘1’ to it integral  representation. A null is returned if the conversion does not succeed.</type></td></tr><tr><td>string</td><td>from_unixtime(int unixtime)</td><td>convert  the number of seconds from the UNIX epoch (1970-01-01 00:00:00 UTC) to a  string representing the timestamp of that moment in the current system  time zone in the format of “1970-01-01 00:00:00”</td></tr><tr><td>string</td><td>to_date(string timestamp)</td><td>返回时间戳字符串的日期部分  Return the date part of a timestamp string: to_date(“1970-01-01 00:00:00”) = “1970-01-01”</td></tr><tr><td>int</td><td>year(string date)</td><td>Return the year part of a date or a timestamp string: year(“1970-01-01 00:00:00”) = 1970, year(“1970-01-01”) = 1970</td></tr><tr><td>int</td><td>month(string date)</td><td>Return the month part of a date or a timestamp string: month(“1970-11-01 00:00:00”) = 11, month(“1970-11-01”) = 11</td></tr><tr><td>int</td><td>day(string date)</td><td>Return the day part of a date or a timestamp string: day(“1970-11-01 00:00:00”) = 1, day(“1970-11-01”) = 1</td></tr><tr><td>string</td><td>get_json_object(string json_string, string path)</td><td>Extract  json object from a json string based on json path specified, and return  json string of the extracted json object. It will return null if the  input json string is invalid.</td></tr></tbody></table><h3 id="内置聚合函数"><a href="#内置聚合函数" class="headerlink" title="内置聚合函数"></a>内置聚合函数</h3><table><thead><tr><th>返回类型</th><th>函数签名</th><th>描述</th></tr></thead><tbody><tr><td>T</td><td>count(*), count(expr), count(DISTINCT expr[, expr_.])</td><td>count(*)—Returns the total number of retrieved rows, including rows containing NULL values; count(expr)—Returns the number of rows for which the supplied expression is non-NULL; count(DISTINCT expr[, expr])—Returns the number of rows for which the supplied expression(s) are unique and non-NULL.</td></tr><tr><td>DOUBLE</td><td>sum(col), sum(DISTINCT col)</td><td>returns the sum of the elements in the group or the sum of the distinct values of the column in the group</td></tr><tr><td>DOUBLE</td><td>avg(col), avg(DISTINCT col)</td><td>returns the average of the elements in the group or the average of the distinct values of the column in the group</td></tr><tr><td>DOUBLE</td><td>min(col)</td><td>returns the minimum value of the column in the group</td></tr><tr><td>DOUBLE</td><td>max(col)</td><td>returns the maximum value of the column in the group</td></tr></tbody></table><h3 id="Hive-提供基本的SQL操作"><a href="#Hive-提供基本的SQL操作" class="headerlink" title="Hive 提供基本的SQL操作"></a>Hive 提供基本的SQL操作</h3><ul><li><p>使用where条件过滤任意的行</p></li><li><p>使用select语句从表里选择合适的列</p></li><li><p>使用join连接两张表</p></li><li><p>在一个表里使用多个group by 求值</p></li><li><p>保存查询结果到其他表</p></li><li><p>下载查询结果到本地目录</p></li><li><p>保存查询结果到hadoop dfs 目录</p></li><li><p>管理表和分区</p></li><li><p>Ability to plug in custom scripts in the language of choice for custom map/reduce jobs.</p><hr></li></ul>]]></content>
      
      
      <categories>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive笔记 </tag>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive学习</title>
      <link href="/2019/10/11/Hive%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/10/11/Hive%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>hive 官方文档:</p><p><a href="https://cwiki.apache.org/confluence/display/Hive/Home#Home-UserDocumentation" target="_blank" rel="noopener">https://cwiki.apache.org/confluence/display/Hive/Home#Home-UserDocumentation</a></p><ul><li><p>hive基本介绍</p></li><li><p>使用教程</p><ul><li><p>Hive 命令行命令</p></li><li><p>Hive SQL Language HQL </p><p>命令,客户端,数据类型</p><p>DDL 操作 (create/drop/alter/truncate/show/describle),静态分析,索引,Archiving</p><p>DML操作(load/insert/update/delete/merge,import /export ,explain plan)</p></li><li><p>文件格式和压缩</p></li><li><p>程序语言:  Hive HPL/SQL</p></li><li><p>Hive配置</p></li><li><p>Hive客户端</p></li><li><p>Hive Web 接口</p></li><li><p>Hive Serdec</p></li><li><p>Hive 集合Accumulo</p></li><li><p>Hive 集成Hbase</p></li><li><p>Druid 集成</p></li><li><p>Hive Transations,Streaming Data Ingest和Streaming Mutation API</p></li><li><p>Hive Counters</p></li></ul></li><li><p>安装和管理的教程</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> hive </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
            <tag> 大数据 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SparkRDD学习</title>
      <link href="/2019/10/09/SparkRDD%E5%AD%A6%E4%B9%A0/"/>
      <url>/2019/10/09/SparkRDD%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<p>RDD 操作</p><p>官方文档:</p><p><a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html" target="_blank" rel="noopener">http://spark.apache.org/docs/latest/rdd-programming-guide.html</a></p><blockquote><p>RDD 操作分为 转换(Transformation)和行动(Action)</p></blockquote><p>Transformations</p><table><thead><tr><th>sformation</th><th>Meaning</th></tr></thead><tbody><tr><td><strong>map</strong>(<em>func</em>)</td><td>Return a new distributed dataset formed by passing each element of the source through a function <em>func</em>. 将每个元素用<code>func</code>函数处理后返回一个新的RDD</td></tr><tr><td><strong>filter</strong>(<em>func</em>)</td><td>Return a new dataset formed by selecting those elements of the source on which <em>func</em> returns true. 将每个元素用func处理,将返回结果是true的结果组合成一个新的RDD返回</td></tr><tr><td><strong>flatMap</strong>(<em>func</em>)</td><td>Similar to map, but each input item can be mapped to 0 or more output items (so <em>func</em> should return a Seq rather than a single item).  和map函数类似,每个元素可以输出1个以上的元素(所以func应该返回一个Seq,而不是一个值)</td></tr><tr><td><strong>mapPartitions</strong>(<em>func</em>)</td><td>Similar to map, but runs separately on each partition (block) of the RDD, so <em>func</em> must be of type     Iterator<t> =&gt; Iterator<u> when running on an RDD of type T.  和map函数类是,func接收的参数是RDD的每个分区,在类型是T的RDD上运行时,func的输入输出参数要是 Iterator<t>=&gt;Iterator<t></t></t></u></t></td></tr><tr><td><strong>mapPartitionsWithIndex</strong>(<em>func</em>)</td><td>Similar to mapPartitions, but also provides <em>func</em> with an integer value representing the index of   the partition, so <em>func</em> must be of type (Int, Iterator<t>) =&gt; Iterator<u> when running on an RDD of type T.    和mapPartitions类似,func函数的参数多一个整数表示分区的索引,参数类型是 (Int,Iterator)=&gt;(Iterator)</u></t></td></tr><tr><td><strong>sample</strong>(<em>withReplacement</em>, <em>fraction</em>, <em>seed</em>)</td><td>Sample a fraction <em>fraction</em> of the data, with or without replacement, using a given random number generator seed.</td></tr><tr><td><strong>union</strong>(<em>otherDataset</em>)</td><td>Return a new dataset that contains the union of the elements in the source dataset and the argument. 原RDD和参数的RDD联合</td></tr><tr><td><strong>intersection</strong>(<em>otherDataset</em>)</td><td>Return a new RDD that contains the intersection of elements in the source dataset and the argument.</td></tr><tr><td><strong>distinct</strong>([<em>numPartitions</em>]))</td><td>Return a new dataset that contains the distinct elements of the source dataset. 返回一个没有重复元素的RDD</td></tr><tr><td><strong>groupByKey</strong>([<em>numPartitions</em>])</td><td>When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<v>) pairs.       <strong>Note:</strong> If you are grouping in order to perform an aggregation (such as a sum or       average) over each key, using <code>reduceByKey</code> or <code>aggregateByKey</code> will yield much better       performance.           <strong>Note:</strong> By default, the level of parallelism in the output depends on the number of partitions of the parent RDD.       You can pass an optional <code>numPartitions</code> argument to set a different number of tasks.   在(K,V)类型的键值对上调用,返回一个(K,Iterable<v>)的键值对 Note:如果为了在每个key上聚合(如求和,平均),使用<code>reduceByKey</code>或<code>aggregateBykey</code>,性能会更好</v></v></td></tr><tr><td><strong>reduceByKey</strong>(<em>func</em>, [<em>numPartitions</em>])</td><td>When called on a dataset of (K, V) pairs, returns a dataset of  (K, V) pairs where the values for each key are aggregated using the  given reduce function <em>func</em>, which must be of type (V,V) =&gt; V. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. 在(K,V)的键值对RDD上使用 ,返回一个RDD ,key的每个value用给定的reduce函数func聚合,聚合函数func的类型要是(V,V)=&gt;V,向<code>groupByKey</code>一样,reduce任务通过可选的第二参数设置</td></tr><tr><td><strong>aggregateByKey</strong>(<em>zeroValue</em>)(<em>seqOp</em>, <em>combOp</em>, [<em>numPartitions</em>])</td><td>When called on a dataset of (K, V) pairs, returns a dataset of  (K, U) pairs where the values for each key are aggregated using the  given combine functions and a neutral “zero” value. Allows an aggregated  value type that is different than the input value type, while avoiding  unnecessary allocations. Like in <code>groupByKey</code>, the number of reduce tasks is configurable through an optional second argument. 在(K,V)键值对的RDD上使用,返回(K,U)键值对,每个键的值使用指定的combine函数和一个值聚合.聚合的值的类型可以和输入的值类型不一样,和<code>groupByKey</code>函数一样,reduce task的数量通过第二个可选参数指定</td></tr><tr><td><strong>sortByKey</strong>([<em>ascending</em>], [<em>numPartitions</em>])</td><td>When called on a dataset of (K, V) pairs where K implements  Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending  or descending order, as specified in the boolean <code>ascending</code> argument.在实现了Ordered的 (K,V)RDD上使用</td></tr><tr><td><strong>join</strong>(<em>otherDataset</em>, [<em>numPartitions</em>])</td><td>When called on datasets of type (K, V) and (K, W), returns a  dataset of (K, (V, W)) pairs with all pairs of elements for each key.     Outer joins are supported through <code>leftOuterJoin</code>, <code>rightOuterJoin</code>, and <code>fullOuterJoin</code>.   在(K,V)类型的RDD上使用,将键一样的放在一起 (K,V)+(K,W)=(K,(V,W)</td></tr><tr><td><strong>cogroup</strong>(<em>otherDataset</em>, [<em>numPartitions</em>])</td><td>When called on datasets of type (K, V) and (K, W), returns a  dataset of (K, (Iterable<v>, Iterable<w>)) tuples. This  operation is also called <code>groupWith</code>. 在 (K,V)和(K,W)类型的RDD上调用,返回一个(K,(Iterable<v>,Iterable<w>))元祖的</w></v></w></v></td></tr><tr><td><strong>cartesian</strong>(<em>otherDataset</em>)</td><td>When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).</td></tr><tr><td><strong>pipe</strong>(<em>command</em>, <em>[envVars]</em>)</td><td>Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the     process’s stdin and lines output to its stdout are returned as an RDD of strings.</td></tr><tr><td><strong>coalesce</strong>(<em>numPartitions</em>)</td><td>Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently     after filtering down a large dataset.</td></tr><tr><td><strong>repartition</strong>(<em>numPartitions</em>)</td><td>Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them.     This always shuffles all data over the network.</td></tr><tr><td><strong>repartitionAndSortWithinPartitions</strong>(<em>partitioner</em>)</td><td>Repartition the RDD according to the given partitioner and, within each resulting partition,   sort records by their keys. This is more efficient than calling <code>repartition</code> and then sorting within   each partition because it can push the sorting down into the shuffle machinery.</td></tr></tbody></table><a id="more"></a><p>Actions</p><table><thead><tr><th>Action</th><th>Meaning</th></tr></thead><tbody><tr><td><strong>reduce</strong>(<em>func</em>)</td><td>Aggregate the elements of the dataset using a function <em>func</em>  (which takes two arguments and returns one). The function should be  commutative and associative so that it can be computed correctly in  parallel.</td></tr><tr><td><strong>collect</strong>()</td><td>Return all the elements of the dataset as an array at the driver  program. This is usually useful after a filter or other operation that  returns a sufficiently small subset of the data.</td></tr><tr><td><strong>count</strong>()</td><td>Return the number of elements in the dataset.</td></tr><tr><td><strong>first</strong>()</td><td>Return the first element of the dataset (similar to take(1)).</td></tr><tr><td><strong>take</strong>(<em>n</em>)</td><td>Return an array with the first <em>n</em> elements of the dataset.</td></tr><tr><td><strong>takeSample</strong>(<em>withReplacement</em>, <em>num</em>, [<em>seed</em>])</td><td>Return an array with a random sample of <em>num</em> elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.</td></tr><tr><td><strong>takeOrdered</strong>(<em>n</em>, <em>[ordering]</em>)</td><td>Return the first <em>n</em> elements of the RDD using either their natural order or a custom comparator.</td></tr><tr><td><strong>saveAsTextFile</strong>(<em>path</em>)</td><td>Write the elements of the dataset as a text file (or set of text  files) in a given directory in the local filesystem, HDFS or any other  Hadoop-supported file system. Spark will call toString on each element  to convert it to a line of text in the file.</td></tr><tr><td><strong>saveAsSequenceFile</strong>(<em>path</em>)   (Java and Scala)</td><td>Write the elements of the dataset as a Hadoop SequenceFile in a  given path in the local filesystem, HDFS or any other Hadoop-supported  file system. This is available on RDDs of key-value pairs that implement  Hadoop’s Writable interface. In Scala, it is also    available on types that are implicitly convertible to Writable (Spark  includes conversions for basic types like Int, Double, String, etc).</td></tr><tr><td><strong>saveAsObjectFile</strong>(<em>path</em>)   (Java and Scala)</td><td>Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using     <code>SparkContext.objectFile()</code>.</td></tr><tr><td><strong>countByKey</strong>()</td><td>Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.</td></tr><tr><td><strong>foreach</strong>(<em>func</em>)</td><td>Run a function <em>func</em> on each element of the dataset. This is usually done for side effects such as updating an <a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#accumulators" target="_blank" rel="noopener">Accumulator</a> or interacting with external storage systems.    <strong>Note</strong>: modifying variables other than Accumulators outside of the <code>foreach()</code> may result in undefined behavior. See <a href="http://spark.apache.org/docs/latest/rdd-programming-guide.html#understanding-closures-a-nameclosureslinka" target="_blank" rel="noopener">Understanding closures </a> for more details.</td></tr></tbody></table>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Flume搭建</title>
      <link href="/2019/10/04/Flume%E6%90%AD%E5%BB%BA/"/>
      <url>/2019/10/04/Flume%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h3 id="解压"><a href="#解压" class="headerlink" title="解压"></a>解压</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf apache-flume-1.8.0-bin.tar.gz -C /opt/</span><br><span class="line">mv /opt/apache-flume-1.8.0-bin/ /opt/flume  </span><br><span class="line">修改配置文件 flume-env.sh 设置java_home路径</span><br></pre></td></tr></table></figure><p>添加可执行目录bin到环境变量PATH</p><a id="more"></a><p>拷贝到其他节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r /opt/flume/ root@slave1:/opt/</span><br><span class="line">scp -r /opt/flume/ root@slave2:/opt/</span><br></pre></td></tr></table></figure><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>查看版本</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">[root@master flume]# flume-ng version</span><br><span class="line">Flume 1.8.0</span><br><span class="line">Source code repository: https://git-wip-us.apache.org/repos/asf/flume.git</span><br><span class="line">Revision: 99f591994468633fc6f8701c5fc53e0214b6da4f</span><br><span class="line">Compiled by denes on Fri Sep 15 14:58:00 CEST 2017</span><br><span class="line">From source with checksum fbb44c8c8fb63a49be0a59e27316833d</span><br><span class="line">[root@master flume]#</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Docker搭建Hadoop环境集群 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>HBase搭建</title>
      <link href="/2019/10/04/HBase%E6%90%AD%E5%BB%BA/"/>
      <url>/2019/10/04/HBase%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><ul><li>Hadoop HA</li></ul><p>HBase 版本 : 1.2.2</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><ol><li><p>解压</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hbase-1.2.12-bin.tar.gz  -C /opt/</span><br><span class="line">mv /opt/hbase-1.2.12/ /opt/hbase</span><br></pre></td></tr></table></figure></li><li><p>设置环境变量</p></li><li><p>修改配置文件</p></li></ol><ul><li><p>hbase-env.sh<br>设置JAVA_HOME<br>export HBASE_MANAGES_ZK=false<br>export JAVA_HOME=/opt/jdk_1.8</p><a id="more"></a></li><li><p>hbase-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.rootdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://cluster1/hbase<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.cluster.distributed<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">   <span class="tag">&lt;<span class="name">value</span>&gt;</span>master,slave1,slave2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>regionservers</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure></li><li><p>backup-masters<br>输入备用的master主机名</p></li></ul><ol start="4"><li>配置hdfs<br>复制hadoop的配置文件 到 hbase安装目录的conf目录下<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"> cp /opt/hadoop/etc/hadoop/core-site.xml  /opt/hbase/conf/     </span><br><span class="line"> cp /opt/hadoop/etc/hadoop/hdfs-site.xml  /opt/hbase/conf/</span><br><span class="line">5. 拷贝到其他节点</span><br></pre></td></tr></table></figure></li></ol><p>拷贝到hbase目录到其他两个节点:<br>``<br>scp -r /opt/hbase/ root@slave1:/opt/<br>scp -r /opt/hbase/ root@slave2:/opt/       </p><ol start="6"><li>启动hbase<br><code>start-hbase.sh</code></li></ol><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="1-web界面截图"><a href="#1-web界面截图" class="headerlink" title="1. web界面截图"></a>1. web界面截图</h3><p><img src="1570160408051.png" alt="1570160408051"></p><h3 id="2-hbase-shell"><a href="#2-hbase-shell" class="headerlink" title="2. hbase shell"></a>2. hbase shell</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase-shell 进入shell</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Docker搭建Hadoop环境集群 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop手册</title>
      <link href="/2019/10/04/Hadoop%E6%89%8B%E5%86%8C/"/>
      <url>/2019/10/04/Hadoop%E6%89%8B%E5%86%8C/</url>
      
        <content type="html"><![CDATA[<h3 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h3>]]></content>
      
      
      <categories>
          
          <category> 手册 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Spark安装</title>
      <link href="/2019/10/03/Spark%E5%AE%89%E8%A3%85/"/>
      <url>/2019/10/03/Spark%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<h2 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h2><ul><li><p>安装了Hadoop HA的环境</p></li><li><p>安装了Hive</p></li></ul><p>Spark 版本:2.3.3</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="1-解压安装包"><a href="#1-解压安装包" class="headerlink" title="1. 解压安装包"></a>1. 解压安装包</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxf spark-2.3.3-bin-hadoop2.7.tgz  -C /opt/</span><br><span class="line">mv /opt/spark-2.3.3-bin-hadoop2.7/ /opt/spark</span><br></pre></td></tr></table></figure><h3 id="2-添加可执行目录bin和sbin到环境变量PATH"><a href="#2-添加可执行目录bin和sbin到环境变量PATH" class="headerlink" title="2. 添加可执行目录bin和sbin到环境变量PATH"></a>2. 添加可执行目录bin和sbin到环境变量PATH</h3><p>要放在hadoop 的 前面，因为start-all.sh命令hadoop sbin目录也有，放在前面会优先使用spark的</p><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">如</span><br><span class="line">export PYSPARK_PYTHON=/opt/python3/bin/python3</span><br><span class="line">alias python3=&apos;/opt/python3/bin/python3&apos;</span><br><span class="line">alias pip=&apos;/opt/python3/bin/pip3&apos;</span><br><span class="line">export JAVA_HOME=/opt/jdk1.8</span><br><span class="line">export HADOOP_HOME=/opt/hadoop</span><br><span class="line">export ZOOKEEPER_HOME=/opt/zookeeper</span><br><span class="line">export HIVE_HOME=/opt/hive</span><br><span class="line">export SPARK_HOME=/opt/spark</span><br><span class="line">export HBASE_HOME=/opt/hbase</span><br><span class="line">export FLUME_HOME=/opt/flume</span><br><span class="line">export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;FLUME_HOME&#125;/bin:$&#123;HBASE_HOME&#125;/bin:$&#123;SPARK_HOME&#125;/sbin:$&#123;SPARK_HOME&#125;/bin:$&#123;HIVE_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$&#123;ZOOKEEPER_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure><h3 id="3-修改spark-env-sh"><a href="#3-修改spark-env-sh" class="headerlink" title="3.  修改spark-env.sh"></a>3.  修改spark-env.sh</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv conf/spark-env.sh.template  conf/spark-env.sh    </span><br><span class="line">里面的内容暂时没有修改</span><br></pre></td></tr></table></figure><h3 id="4-修改slaves文件"><a href="#4-修改slaves文件" class="headerlink" title="4. 修改slaves文件"></a>4. 修改slaves文件</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mv conf/slaves.template conf/slaves</span><br><span class="line">编辑slaves文件，设置为slave1 slave2</span><br></pre></td></tr></table></figure><h3 id="5-复制spark目录到其他节点-slave1-slave2"><a href="#5-复制spark目录到其他节点-slave1-slave2" class="headerlink" title="5. 复制spark目录到其他节点(slave1,slave2)"></a>5. 复制spark目录到其他节点(slave1,slave2)</h3><h3 id="6-启动"><a href="#6-启动" class="headerlink" title="6. 启动"></a>6. 启动</h3><p>   执行/opt/spark/sbin/start-all.sh </p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><h3 id="1-查看web界面"><a href="#1-查看web界面" class="headerlink" title="1. 查看web界面"></a>1. 查看web界面</h3><p><img src="1570154705436.png" alt="1570154705436"></p><p><img src="1570154818090.png" alt="1570154818090"></p><p><img src="1570154830209.png" alt="1570154830209"></p><h3 id="2-spark-shell"><a href="#2-spark-shell" class="headerlink" title="2. spark-shell"></a>2. spark-shell</h3><p>直接输入<code>spark-shell</code></p><p><img src="1570154933284.png" alt="1570154933284"></p><p>这里的 master=local[],应该是默认本地模式</p><p>指定master来启动spark-shell</p><p><img src="1570155016633.png" alt="1570155016633"></p><p>这时在web界面也有对应的Running Application了</p><h3 id="3-PySpark-测试"><a href="#3-PySpark-测试" class="headerlink" title="3. PySpark 测试"></a>3. PySpark 测试</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">默认使用的是python2</span><br><span class="line">root@slave1 logs]# pyspark </span><br><span class="line">Python 2.7.5 (default, Jun 20 2019, 20:27:34) </span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-36)] on linux2</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">2019-10-04 04:34:24 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Setting default log level to &quot;WARN&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</span><br><span class="line">   /__ / .__/\_,_/_/ /_/\_\   version 2.3.3</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Python version 2.7.5 (default, Jun 20 2019 20:27:34)</span><br><span class="line">SparkSession available as &apos;spark&apos;.</span><br><span class="line">&gt;&gt;&gt; exit()</span><br><span class="line">[root@slave1 logs]#</span><br></pre></td></tr></table></figure><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="1-和hadoop-的-hdfs-结合"><a href="#1-和hadoop-的-hdfs-结合" class="headerlink" title="1. 和hadoop 的 hdfs 结合"></a>1. 和hadoop 的 hdfs 结合</h3><blockquote><p>结合后读取文件时默认从hdfs上读取</p></blockquote><p>将hadoop的配置文件core-site.xml 和hdfs-site.xml复制到spark安装目录的conf目录下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">复制完后用spark-shell 执行时会从hdfs上读取/in.txt文件</span><br><span class="line">val text = spark.read.textFile(&quot;/in.txt&quot;)</span><br></pre></td></tr></table></figure><h3 id="2-和hive结合"><a href="#2-和hive结合" class="headerlink" title="2. 和hive结合"></a>2. 和hive结合</h3><blockquote><p>spark-sql 的表的写入读取 那些，设置成已经安装了的hive </p></blockquote><p>不结合hive的话,spark其实内置了一个数据库，没有指定的话，就会使用那个(数据写入在spark安装目录下的文件夹里)</p><ul><li>没结合hive前<br>创建一个dataframe ,保存成表测试</li></ul><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//定义一个case类</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">App</span>(<span class="params">name:<span class="type">String</span>,download:<span class="type">Int</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">//数据列表</span></span></span><br><span class="line"><span class="class"><span class="title">val</span> <span class="title">data</span> </span>= <span class="type">List</span>(<span class="type">App</span>(<span class="string">"Eclipse"</span>,<span class="number">19</span>),<span class="type">App</span>(<span class="string">"Inteli IDEA Ulitimate"</span>,<span class="number">1009</span>),<span class="type">App</span>(<span class="string">"Visual Studio"</span>,<span class="number">2000</span>))</span><br><span class="line"><span class="comment">//创建dataframe</span></span><br><span class="line"><span class="keyword">val</span> df = spark.createDataFrame(data)</span><br><span class="line"></span><br><span class="line"><span class="comment">//查看</span></span><br><span class="line">scala&gt; df.show()</span><br><span class="line">+--------------------+--------+</span><br><span class="line">|                name|download|</span><br><span class="line">+--------------------+--------+</span><br><span class="line">|             <span class="type">Eclipse</span>|      <span class="number">19</span>|</span><br><span class="line">|<span class="type">Inteli</span> <span class="type">IDEA</span> <span class="type">Uliti</span>...|    <span class="number">1009</span>|</span><br><span class="line">|       <span class="type">Visual</span> <span class="type">Studio</span>|    <span class="number">2000</span>|</span><br><span class="line">+--------------------+--------+</span><br><span class="line"></span><br><span class="line">先创建数据库</span><br><span class="line"> spark.sql(<span class="string">"create database test"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">//保存数据成表,test数据库下的apps表</span></span><br><span class="line">df.write.saveAsTable(<span class="string">"test.apps"</span>)</span><br><span class="line"></span><br><span class="line">这里没有和hive连接起来时，保存的数据都是在spark安装目录下的</span><br><span class="line">spark-warehouse文件夹</span><br></pre></td></tr></table></figure><p>设置hive连接</p><ul><li><p>将hive的配置文件 hive-site.xml 复制到 spark安装目录下的conf文件夹</p></li><li><p>拷贝mysql驱动到spark安装目录的jars文件夹下</p></li></ul><p>再运行上面的测试代码,先在hive里创建test数据库</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">create database test;</span><br></pre></td></tr></table></figure><p>df.write.saveAsTable(“test.apps”)后在hive里查看是不是有apps表</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; show tables;</span><br><span class="line">OK</span><br><span class="line">apps</span><br><span class="line">Time taken: 0.119 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; select * from apps;</span><br><span class="line">OK</span><br><span class="line">SLF4J: Failed to load class &quot;org.slf4j.impl.StaticLoggerBinder&quot;.</span><br><span class="line">SLF4J: Defaulting to no-operation (NOP) logger implementation</span><br><span class="line">SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.</span><br><span class="line">//成功查询到数据 , 成功写入数据到hive</span><br><span class="line">Eclipse19</span><br><span class="line">Inteli IDEA Ulitimate1009</span><br><span class="line">Visual Studio2000</span><br><span class="line">Time taken: 1.45 seconds, Fetched: 3 row(s)</span><br><span class="line">hive&gt;</span><br></pre></td></tr></table></figure><h3 id="3-PySpark使用python3"><a href="#3-PySpark使用python3" class="headerlink" title="3. PySpark使用python3"></a>3. PySpark使用python3</h3><p>默认安装的Python 为 2.7,现在修改为3.7版本</p><ul><li>使用源码编译的方式安装 Python3</li><li>安装python3到/opt/python3文件夹</li><li>切换Spark的pyspark为python3版本</li><li>在master上编译,然后将编译好的/opt/python3 直接发送到其他节点</li></ul><ol><li><p>使用yum安装依赖包 编译python用</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">yum install gcc</span><br><span class="line">yum install zlib-static</span><br><span class="line">yum install libffi-devel -y </span><br><span class="line">yum install  readline-devel(不装这个方向键用不了)</span><br><span class="line">yum install -y openssl-devel</span><br><span class="line">libffi-devel</span><br></pre></td></tr></table></figure><p>解压源码包</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf Python-3.7.3.tar.xz </span><br><span class="line">cd 进入解压出来的文件夹, 下面命令设置安装路径</span><br><span class="line">./configure --prefix=/opt/python3</span><br><span class="line">//docker里的centos 镜像没有make命令,使用yum install make命令安装</span><br><span class="line">编译</span><br><span class="line">make</span><br><span class="line"></span><br><span class="line">编译安装</span><br><span class="line">make install</span><br></pre></td></tr></table></figure><p>修改./bashrc 添加别名来在命令行里快速启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">alias python3=&apos;/opt/python3/bin/python3&apos;</span><br><span class="line">alias pip=&apos;/opt/python3/bin/pip3&apos;</span><br></pre></td></tr></table></figure><p>设置pyspark使用的python3为python3</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PYSPARK_PYTHON=/opt/python3/bin/python3</span><br></pre></td></tr></table></figure></li></ol><ul><li><p>复制编译好的python3到其他两个节点</p></li><li><p>复制环境变量.bashrc到其他两个节点</p><p>启动pyspark测试</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">成功是python3 </span><br><span class="line">Python 3.7.3 (default, Oct  4 2019, 04:22:20) </span><br><span class="line">[GCC 4.8.5 20150623 (Red Hat 4.8.5-39)] on linux</span><br><span class="line">Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.</span><br><span class="line">2019-10-04 04:38:07 WARN  NativeCodeLoader:62 - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable</span><br><span class="line">Setting default log level to &quot;WARN&quot;.</span><br><span class="line">To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).</span><br><span class="line">Welcome to</span><br><span class="line">      ____              __</span><br><span class="line">     / __/__  ___ _____/ /__</span><br><span class="line">    _\ \/ _ \/ _ `/ __/  &apos;_/</span><br><span class="line">   /__ / .__/\_,_/_/ /_/\_\   version 2.3.3</span><br><span class="line">      /_/</span><br><span class="line"></span><br><span class="line">Using Python version 3.7.3 (default, Oct  4 2019 04:22:20)</span><br><span class="line">SparkSession available as &apos;spark&apos;.</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure></li></ul><p>Spark 学习:</p><ul><li><p>RDD学习</p></li><li><p>Spark-SQL DataSet / DataFrame 学习</p></li><li><p>Spark MLlib</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Docker搭建Hadoop环境集群 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hive安装</title>
      <link href="/2019/10/03/Hive%E5%AE%89%E8%A3%85/"/>
      <url>/2019/10/03/Hive%E5%AE%89%E8%A3%85/</url>
      
        <content type="html"><![CDATA[<p>hive版本 1.2 </p><p>下载地址<a href="https://mirrors.tuna.tsinghua.edu.cn/apache/hive/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/apache/hive/</a></p><p>环境准备:</p><ol><li><p>已经安装了Hadoop </p></li><li><p>一台安装有mysql的主机 (用于保存hive的元数据)</p></li></ol><hr><p>docker 创建mysql 5.7的 镜像bigdata_mysql，用来保存hive元数据</p><ul><li><p>指定root密码为root</p></li><li><p>创建容器时指定创建一个数据库hivedata</p></li><li><p>创建一个用户hive,密码为hive</p></li><li><p>添加到网络hadoopCluster</p><a id="more"></a></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">docker create --network hadoopCluster --name &quot;bigdata_mysql&quot;   -e MYSQL_ROOT_PASSWORD=root -e MYSQL_DATABASE=hivedata -e MYSQL_USER=hive -e MYSQL_PASSWORD=hive  mysql:5.7 </span><br><span class="line"></span><br><span class="line">//连接测试 创建一个临时mysql客户端容器</span><br><span class="line">docker  run  -it --network hadoopCluster --rm mysql mysql -hbigdata_mysql  -uroot  -p</span><br><span class="line">js@ljh-X441UVK:~$ docker  run  -it --network hadoopCluster --rm mysql mysql -hbigdata_mysql  -uhive  -p</span><br><span class="line">Enter password: </span><br><span class="line">Welcome to the MySQL monitor.  Commands end with ; or \g.</span><br><span class="line">Your MySQL connection id is 3</span><br><span class="line">Server version: 5.7.27 MySQL Community Server (GPL)</span><br><span class="line"></span><br><span class="line">Copyright (c) 2000, 2019, Oracle and/or its affiliates. All rights reserved.</span><br><span class="line"></span><br><span class="line">Oracle is a registered trademark of Oracle Corporation and/or its</span><br><span class="line">affiliates. Other names may be trademarks of their respective</span><br><span class="line">owners.</span><br><span class="line"></span><br><span class="line">Type &apos;help;&apos; or &apos;\h&apos; for help. Type &apos;\c&apos; to clear the current input statement.</span><br><span class="line">打开后发现，命令里指定的数据库成功创建了</span><br><span class="line">mysql&gt; show databases;</span><br><span class="line">+--------------------+</span><br><span class="line">| Database           |</span><br><span class="line">+--------------------+</span><br><span class="line">| information_schema |</span><br><span class="line">| hivedata           |</span><br><span class="line">+--------------------+</span><br><span class="line">2 rows in set (0.01 sec)</span><br><span class="line"></span><br><span class="line">mysql&gt;</span><br></pre></td></tr></table></figure><hr><p>解压</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# tar -zxf apache-hive-1.2.2-bin.tar.gz -C  /opt/</span><br><span class="line">[root@master ~]# mv /opt/apache-hive-1.2.2-bin/ /opt/hive</span><br></pre></td></tr></table></figure><p>修改~/.bashrc 文件 添加bin目录到PATH变量</p><p>在安装目录下的conf下新建hive-site.xml文件 </p><p>jdbc url里直接用bigdata_mysql,因为时在docker里配置的，都在统一网络里</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://bigdata_mysql:3306/hivedata?useSSL=false&amp;amp;createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassWord<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>上传mysql驱动到hive安装目录的lib文件夹</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp /root/mysql-connector-java-5.1.47.jar  /opt/hive/lib/</span><br></pre></td></tr></table></figure><p>hive 调试模式,在某个命令卡住时,开启调试可以看到真正的错误信息</p><p> hive -hiveconf hive.root.logger=DEBUG,console</p><p>测试</p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Docker搭建Hadoop环境集群 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop HA(高可用)搭建</title>
      <link href="/2019/10/03/Hadoop-HA-%E9%AB%98%E5%8F%AF%E7%94%A8-%E6%90%AD%E5%BB%BA/"/>
      <url>/2019/10/03/Hadoop-HA-%E9%AB%98%E5%8F%AF%E7%94%A8-%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<h1 id="Hadoop-HA-高可用-搭建"><a href="#Hadoop-HA-高可用-搭建" class="headerlink" title="Hadoop HA(高可用)搭建"></a>Hadoop HA(高可用)搭建</h1><h2 id="环境准备和集群规划"><a href="#环境准备和集群规划" class="headerlink" title="环境准备和集群规划"></a>环境准备和集群规划</h2><blockquote><p>Hadoop 是安装其他组件的基础,必须先配置好Hadoop环境</p></blockquote><h3 id="1-环境"><a href="#1-环境" class="headerlink" title="1. 环境"></a>1. 环境</h3><p>需要准备三台centos 7虚拟机，配置好下面条件</p><ol><li>设置好hostname主机名(master,slave1,slave2)</li><li>网络可以互相ping通</li><li>关闭防火墙</li></ol><p>如果是用Docker 搭建,使用里面的centos:7镜像的话,还需要用yum安装下面的东西(因为默认没有安装)</p><ol><li>安装好网络工具ss 和ifconfig</li><li>安装sshd服务</li></ol><p>宿主机使用ssh工具连接到三台主机,和对应的上传文件工具(上传安装到到集群)</p><p>这里宿主机是Ubuntu 系统的,直接使用内置的终端ssh连接到集群(Docker的可以直接exec进入bash也行)</p><h3 id="2-集群规划"><a href="#2-集群规划" class="headerlink" title="2. 集群规划"></a>2. 集群规划</h3><table><thead><tr><th>软件</th><th>版本</th><th></th></tr></thead><tbody><tr><td>hadoop</td><td>2.7.7</td><td></td></tr><tr><td>jdk</td><td>1.8</td><td></td></tr><tr><td>系统</td><td>CentOS 7</td><td></td></tr></tbody></table><p>一台master+两台slave</p><table><thead><tr><th>主机名</th><th>角色</th><th>其他</th></tr></thead><tbody><tr><td>master</td><td>namenode</td><td></td></tr><tr><td>slave1</td><td>datanode</td><td></td></tr><tr><td>slave2</td><td>datanode</td><td></td></tr></tbody></table><p>集群软件目录:</p><table><thead><tr><th>项目</th><th>安装目录</th><th></th></tr></thead><tbody><tr><td>hadoop组件等</td><td>/opt/</td><td></td></tr><tr><td>hadoop 数据目录</td><td>/opt/data/</td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table><h3 id="3-下载连接"><a href="#3-下载连接" class="headerlink" title="3. 下载连接"></a>3. 下载连接</h3><p>hadoop等:apache 镜像站:<a href="https://mirrors.tuna.tsinghua.edu.cn/apache/" target="_blank" rel="noopener">https://mirrors.tuna.tsinghua.edu.cn/apache/</a></p><h2 id="搭建过程"><a href="#搭建过程" class="headerlink" title="搭建过程"></a>搭建过程</h2><h4 id="1-配置-ssh-免密登陆"><a href="#1-配置-ssh-免密登陆" class="headerlink" title="1. 配置 ssh 免密登陆"></a>1. 配置 ssh 免密登陆</h4><blockquote><p>设置主机之间可以免密码登录</p></blockquote><p>两种方式设置免密登陆,一种是手动将公钥追加到要登录的主机的~/.ssh/authorized_keys文件下，一种是使用ssh-copy-id命令(推荐)</p><a id="more"></a><p>​    a. 手动设置免密登陆</p><p>​    手动设置免密登录,这里以设置master免密登录两个slave(和自己)为例</p><ul><li><p>先在master 生成公钥和私钥</p><p>输入<code>ssh-keygen</code> 命令,一直回车,生成的密钥保存在/root/.ssh目录</p></li><li><p>添加公钥到slave1和slave2节点</p></li></ul><p>master,slave1和slave2节点默认没有<code>~/.ssh</code>文件夹,在两个节点执行<code>ssh localhost</code>就会生成<code>~/.ssh</code>文件夹(或者手动建立)</p><ul><li><p>将master的公钥(id_rsa.pub)添加到 三台主机的~/.ssh/authorized_keys 文件</p><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">在master执行复制命令</span><br><span class="line"></span><br><span class="line">//复制公钥到slave1节点的/root目录</span><br><span class="line">[root@3de59086e794 ~]# scp ~/.ssh/id_rsa.pub  root@slave1:/root/</span><br><span class="line">The authenticity of host 'slave1 (172.19.0.3)' can't be established.</span><br><span class="line">ECDSA key fingerprint is SHA256:JKinnsafdpyJ4e9NB3nvdNz5sHASU0whnVLWL9nyGlg.</span><br><span class="line">ECDSA key fingerprint is MD5:07:b5:34:4e:14:63:2c:3a:8a:52:88:4c:bf:07:58:71.</span><br><span class="line">Are you sure you want to continue connecting (yes/no)? yes</span><br><span class="line">Warning: Permanently added 'slave1,172.19.0.3' (ECDSA) to the list of known hosts.</span><br><span class="line">root@slave1's password: </span><br><span class="line">id_rsa.pub                                                                                                  100%  399     1.8MB/s   00:00    </span><br><span class="line">//复制公钥到slave2节点的/root目录</span><br><span class="line">[root@3de59086e794 ~]# scp ~/.ssh/id_rsa.pub  root@slave2:/root/</span><br><span class="line">root@slave2's password: </span><br><span class="line">id_rsa.pub                                                                                                  100%  399     1.2MB/s   00:00    </span><br><span class="line">[root@3de59086e794 ~]#</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">在master,slave1 和 slave2 节点执行(这里为读取id_ras.pub文件的内容追加到authorized_keys文件)</span><br><span class="line"></span><br><span class="line"> cat id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys</span><br><span class="line"> //查看下添加后的authorized_keys文件</span><br><span class="line">[root@8569789b1bc0 ~]# cat ~/.ssh/authorized_keys </span><br><span class="line">ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCkG4760Jb3jf8ARJ6knvVkXpixJ0lEhyu2GfFDEGF9SYtdtK6/rZ33noGCoK50PmZCIOrVcvYrVit5NyiyWtcQljkwowLPP6K2SRLkh7nCTEHZjfOhyW1qjyteHazKpF36iDli37zcKFzsIQz2IfoaNoyghE3y6eqXsaByO3kooPJlhjvRsb/AbuPDIMgZPeQ++gfEsBALBhfqo4xnQDvCuo8Ibi5Pw9gSs2Kik8JXmxju1R6IHmzQYnhXQy2pVfrvIgyznfWrWRxAPd9xNfTfnZ51hSc0t1Gfnpgi1ptknTfXqm0DYa+TFWzgle98SjZ58plf1Ca9vzWmfc5qJfld root@3de59086e794</span><br><span class="line"></span><br><span class="line">authorized_keys文件的权限,权限需要设置为600 </span><br><span class="line">[root@8569789b1bc0 ~]# ll ~/.ssh/authorized_keys </span><br><span class="line">-rw-r--r-- 1 root root 399 Oct  2 03:17 /root/.ssh/authorized_keys</span><br><span class="line">[root@8569789b1bc0 ~]# </span><br><span class="line">可以看到上面的权限是 644,需要修改为 600(测试 644也能登录,但777不行)</span><br><span class="line">修改为 600的命令为</span><br><span class="line">chmod 600 ~/.ssh/authorized_keys</span><br></pre></td></tr></table></figure></li></ul><p>  b. 使用ssh-copyid命令</p><p>  用这个命令就不用复制公钥再追加到文件，直接一个命令就行，以slave1免密登录三台主机为例</p><ul><li><p>生成公钥和私钥</p><p><code>ssh-keygen</code></p></li><li><p>发送公钥到其他节点</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ssh-copy-id master</span><br><span class="line">ssh-copy-id slave1</span><br><span class="line">ssh-copy-id slave2</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">测试免密登录</span><br><span class="line">ssh master(第一次可能要验证主机,输入yes就行)</span><br><span class="line">直接登录，不用输入密码</span><br></pre></td></tr></table></figure><p><img src="1569986981320.png" alt="1569986981320"></p></li></ul><h4 id="2-安装jdk和hadoop"><a href="#2-安装jdk和hadoop" class="headerlink" title="2. 安装jdk和hadoop"></a>2. 安装jdk和hadoop</h4><blockquote><p>用scp 或者 第三方工具将jdk和hadoop的安装包上传到主机里</p><p>先在master 解压和修改好配置 文件,然后直接发送到其他两个slave主机</p></blockquote><ul><li>解压</li></ul><p>jdk解压</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf jdk-8u191-linux-x64.tar.gz  -C /opt/</span><br><span class="line">mv /opt/jdk1.8.0_191/ /opt/jdk1.8</span><br></pre></td></tr></table></figure><p>hadoop解压</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hadoop-2.7.7.tar.gz -C  /opt/</span><br><span class="line"></span><br><span class="line">mv /opt/hadoop-2.7.7/ /opt/hadoop</span><br></pre></td></tr></table></figure><ul><li>修改配置文件</li></ul><p>配置文件位于hadoop安装目录下的<code>etc/hadoop/</code>文件夹下</p><ol><li><p>hadoop-evn.sh</p><p>这个文件只要修改JAVA_HOME变量</p></li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"></span><br><span class="line"># The java implementation to use.</span><br><span class="line">export JAVA_HOME=/opt/jdk1.8</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">...</span><br></pre></td></tr></table></figure><ol start="2"><li><p>core-site.xml</p><p>配置文件里出现的cluster1 为fs.default.FS里指定的namenode的地址,如果修改了<code>fs.default.FS</code>,其他的地方也要对应修改</p></li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8"?&gt;</span></span><br><span class="line"><span class="meta">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- cluster1 为集群的 名字--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.client.failover.proxy.provider.cluster1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://cluster1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>ha.zookeeper.quorum<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:2181,slave1:2181,slave2:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    临时目录,如果没有指定namenode或者datanode的数据目录，默认会在$&#123;hadoop.tmp.dir&#125;目录下,这个目录默认在???,重启可能会被清除，这里换成指定的hadoop.tmp.dir</span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/hadoop_tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ol start="3"><li>hdfs-site.xml</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--   --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.nameservices<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>cluster1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.namenodes.cluster1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>nn1,nn2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.cluster1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.rpc-address.cluster1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.cluster1.nn1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address.cluster1.nn2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.shared.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>qjournal://master:8485;slave1:8485;slave2:8485/cluster1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.fencing.methods<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>sshfence</span><br><span class="line">shell(/bin/true)<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.ha.automatic-failover.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    </span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/data/hadoop_data/namenode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>/opt/data/hadoop_data/datanode<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ol start="3"><li>yarn-site.xml</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>运行在nodemanager上的附属服务<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services.mapreduce.shuffle.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.mapred.ShuffleHandler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.recovery.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.zk-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>master:2181,slave1:2181,slave2:2181<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>是否启用HA，默认false<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.ha.rm-ids<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>rm1,rm2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>最少2个<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm1<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>master<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname.rm2<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>slave1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.cluster-id<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn-ha<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">description</span>&gt;</span>集群HA的id，用于在ZooKeeper上创建节点，区分使用</span><br><span class="line"></span><br><span class="line">同一个ZooKeeper集群的不同Hadoop集群<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line"> <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ol start="4"><li><strong>mapred-site.xml</strong></li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">   <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>master:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>job web地址<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    </span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li><p>slaves 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">slave1</span><br><span class="line">slave2</span><br></pre></td></tr></table></figure></li></ul><h3 id="zookeeper-安装"><a href="#zookeeper-安装" class="headerlink" title="zookeeper 安装"></a>zookeeper 安装</h3><blockquote><p>高可用需要用到zookeeper</p></blockquote><p>解压</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf zookeeper-3.4.5.tar.gz  -C /opt/</span><br><span class="line">mv /opt/zookeeper-3.4.5/ /opt/zookeeper</span><br></pre></td></tr></table></figure><p>修改zoo.cfg文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"> mv conf/zoo_sample.cfg  conf/zoo.cfg</span><br><span class="line">添加修改conf.cfg文件</span><br><span class="line"></span><br><span class="line">dataDir=/opt/data/zookeeper_data</span><br><span class="line"></span><br><span class="line">server.1=master:2888:3888</span><br><span class="line">server.2=slave1:2888:3888</span><br><span class="line">server.3=slave2:2888:3888</span><br></pre></td></tr></table></figure><h3 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h3><p>修改<code>/root/.bashrc</code>文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">后面添加下面的变量</span><br><span class="line">export JAVA_HOME=/opt/jdk1.8</span><br><span class="line">export HADOOP_HOME=/opt/hadoop</span><br><span class="line">export ZOOKEEPER_HOME=/opt/zookeeper</span><br><span class="line">export PATH=$&#123;JAVA_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/bin:$&#123;HADOOP_HOME&#125;/sbin:$&#123;ZOOKEEPER_HOME&#125;/bin:$PATH</span><br></pre></td></tr></table></figure><p>在三台主机 建立要手动建立的文件夹(namenode数据目录,datanode数据目录,zookeeper的数据目录)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">mkdir -p /opt/data/hadoop_data/datanode</span><br><span class="line">mkdir -p /opt/data/hadoop_data/namenode</span><br><span class="line">zookeeper 数据目录</span><br><span class="line">mkdir -p /opt/data/zookeeper_data</span><br></pre></td></tr></table></figure><p>复制下面的数据到其他节点,第一次可以 直接将整个/opt/下的复制过去，因为组件和上面建的文件夹都在/opt下面</p><ul><li><p>复制环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp /root/.bashrc root@slave1:/root/</span><br><span class="line">scp /root/.bashrc root@slave2:/root/</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>复制组件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scp -r  /opt/* root@slave1:/opt/</span><br><span class="line">scp -r  /opt/* root@slave2:/opt/</span><br></pre></td></tr></table></figure></li></ul><p>测试环境变量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@d7998eb32bfb ~]# source /root/.bashrc</span><br><span class="line">[root@d7998eb32bfb ~]# java -version</span><br><span class="line">java version &quot;1.8.0_191&quot;</span><br><span class="line">Java(TM) SE Runtime Environment (build 1.8.0_191-b12)</span><br><span class="line">Java HotSpot(TM) 64-Bit Server VM (build 25.191-b12, mixed mode)</span><br><span class="line">[root@d7998eb32bfb ~]#</span><br></pre></td></tr></table></figure><h3 id="修改需要单独配置的文件"><a href="#修改需要单独配置的文件" class="headerlink" title="修改需要单独配置的文件"></a>修改需要单独配置的文件</h3><ul><li>zookeeper 的 myid文件</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">master,slave1,slave2分别执行</span><br><span class="line">echo 1 &gt; /opt/data/zookeeper_data/myid</span><br><span class="line">echo 2 &gt; /opt/data/zookeeper_data/myid</span><br><span class="line">echo 3 &gt; /opt/data/zookeeper_data/myid</span><br></pre></td></tr></table></figure><h3 id="初始化hadoop"><a href="#初始化hadoop" class="headerlink" title="初始化hadoop"></a>初始化hadoop</h3><ol><li><p>三个主机都启动zookeeper</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zkServer.sh start</span><br></pre></td></tr></table></figure><p>使用<code>zkServer.sh status</code>查看身份，如果有一个leader和两个follower就成功</p><p><img src="1569990314156.png" alt="1569990314156"></p><p><img src="1569990323756.png" alt="1569990323756"></p></li></ol><p><img src="1569990305940.png" alt="1569990305940"></p><ol start="2"><li>启动journal node 进程</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">三台机器</span><br><span class="line">hadoop-daemon.sh start journalnode</span><br></pre></td></tr></table></figure><ol start="3"><li>格式化zookeeper节点</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">格式化前查看(通过zkCli.sh进入)</span><br><span class="line">[zk: localhost:2181(CONNECTED) 1] ls /</span><br><span class="line">[zookeeper]</span><br><span class="line">[zk: localhost:2181(CONNECTED) 2]</span><br></pre></td></tr></table></figure><p><code>hdfs zkfc -formatZK</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">19/10/02 04:34:50 INFO zookeeper.ZooKeeper: Client environment:java.library.path=/opt/hadoop/lib/native</span><br><span class="line">19/10/02 04:34:50 INFO zookeeper.ZooKeeper: Client environment:java.io.tmpdir=/tmp</span><br><span class="line">19/10/02 04:34:50 INFO zookeeper.ZooKeeper: Client environment:java.compiler=&lt;NA&gt;</span><br><span class="line">19/10/02 04:34:50 INFO zookeeper.ZooKeeper: Client environment:os.name=Linux</span><br><span class="line">19/10/02 04:34:50 INFO zookeeper.ZooKeeper: Client environment:os.arch=amd64</span><br><span class="line">19/10/02 04:34:50 INFO zookeeper.ZooKeeper: Client environment:os.version=5.0.0-29-generic</span><br><span class="line">19/10/02 04:34:50 INFO zookeeper.ZooKeeper: Client environment:user.name=root</span><br><span class="line">19/10/02 04:34:50 INFO zookeeper.ZooKeeper: Client environment:user.home=/root</span><br><span class="line">19/10/02 04:34:50 INFO zookeeper.ZooKeeper: Client environment:user.dir=/opt</span><br><span class="line">19/10/02 04:34:50 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=master:2181,slave1:2181,slave2:2181 sessionTimeout=5000 watcher=org.apache.hadoop.ha.ActiveStandbyElector$WatcherWithClientRef@37afeb11</span><br><span class="line">19/10/02 04:34:50 INFO zookeeper.ClientCnxn: Opening socket connection to server 3de59086e794/172.19.0.4:2181. Will not attempt to authenticate using SASL (unknown error)</span><br><span class="line">19/10/02 04:34:50 INFO zookeeper.ClientCnxn: Socket connection established to 3de59086e794/172.19.0.4:2181, initiating session</span><br><span class="line">19/10/02 04:34:50 INFO zookeeper.ClientCnxn: Session establishment complete on server 3de59086e794/172.19.0.4:2181, sessionid = 0x16d8ab6613c0001, negotiated timeout = 5000</span><br><span class="line">19/10/02 04:34:50 INFO ha.ActiveStandbyElector: Session connected.</span><br><span class="line">19/10/02 04:34:50 INFO ha.ActiveStandbyElector: Successfully created /hadoop-ha/cluster1 in ZK.</span><br><span class="line">19/10/02 04:34:50 INFO zookeeper.ZooKeeper: Session: 0x16d8ab6613c0001 closed</span><br><span class="line">19/10/02 04:34:50 INFO zookeeper.ClientCnxn: EventThread shut down</span><br><span class="line">[root@3de59086e794 opt]#</span><br></pre></td></tr></table></figure><ol start="4"><li>格式化namenode节点</li></ol><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">master上执行</span><br><span class="line">hdfs namenode -format</span><br><span class="line">格式化完后有文件了namenode的目录</span><br><span class="line">[root@3de59086e794 opt]# ll data/hadoop_data/namenode/current/</span><br><span class="line">total 16</span><br><span class="line">-rw-r--r-- 1 root root 201 Oct  2 04:38 VERSION</span><br><span class="line">-rw-r--r-- 1 root root 321 Oct  2 04:38 fsimage_0000000000000000000</span><br><span class="line">-rw-r--r-- 1 root root  62 Oct  2 04:38 fsimage_0000000000000000000.md5</span><br><span class="line">-rw-r--r-- 1 root root   2 Oct  2 04:38 seen_txid</span><br><span class="line">[root@3de59086e794 opt]#</span><br></pre></td></tr></table></figure><p>master上的namenode格式化后，先启动master上的namenode节点(下面同步需要，不然会报错说连接不到master:9000)</p><p><code>hadoop-daemon.sh  start namenode</code></p><p>接下来要在第二个namenode上把master上格式化完的数据同步到第二个namenode上,在slave1(备用namenode节点)上执行</p><p><code>hdfs namenode -bootstrapStandby</code></p><hr><h3 id="启动集群"><a href="#启动集群" class="headerlink" title="启动集群"></a>启动集群</h3><ol><li><p>启动zookeeper(三台主机)</p><p><code>zkServer.sh start</code></p></li><li><p>启动hdfs</p><p><code>start-dfs.sh</code></p></li><li><p>启动yarn</p><p><code>start-yarn.sh</code></p></li></ol><p>4.手动启动yarn的备用节点</p><p><code>yarn-daemon.sh start resourcemanager</code></p><ol start="5"><li>在master启动mr 历史进程</li></ol><p><code>mr-jobhistory-daemon.sh start historyserver</code></p><p>开启jobhistory ,可以在任务结束后查看任务运行情况</p><p>jobhistory 的web端口在mapred-site.xml里设置,为19888</p><h3 id="查看进程"><a href="#查看进程" class="headerlink" title="查看进程"></a>查看进程</h3><p>master</p><p><img src="1569992007816.png" alt="1569992007816"></p><p>slave1</p><p><img src="1569992018604.png" alt="1569992018604"></p><p>slave2</p><p><img src="1569992027208.png" alt="1569992027208"></p><p>查看开放端口</p><p><img src="1570063825179.png" alt="1570063825179"></p><h3 id="停止集群"><a href="#停止集群" class="headerlink" title="停止集群"></a>停止集群</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">stop-dfs.sh</span><br><span class="line">stop-yarn.sh</span><br></pre></td></tr></table></figure><h2 id="集群测试"><a href="#集群测试" class="headerlink" title="集群测试"></a>集群测试</h2><h3 id="1-web界面查看"><a href="#1-web界面查看" class="headerlink" title="1. web界面查看"></a>1. web界面查看</h3><p>namenode</p><p><img src="1569992108322.png" alt="1569992108322"></p><p>备用节点slave1</p><p><img src="1569992161841.png" alt="1569992161841"></p><p>​    ResouceManager 主备节点</p><p><img src="1570064382082.png" alt="1570064382082"></p><p>进入备用的会重定向到master的,由于宿主机没有设置master对应的ip,所以访问不了</p><p><img src="1570064476894.png" alt="1570064476894"></p><h3 id="2-namenode-高可用测试"><a href="#2-namenode-高可用测试" class="headerlink" title="2. namenode 高可用测试"></a>2. namenode 高可用测试</h3><blockquote><ol><li>杀掉master上的namenode,查看slave1上的namenode的状态</li><li>启动杀死的namenode,查看它的状态</li></ol></blockquote><p>集群刚启动的情况, 现在master是standby,slave1是active </p><p><img src="1570063926217.png" alt="1570063926217"></p><p><img src="1570063935599.png" alt="1570063935599"></p><p>杀死active的namenode,在slave1上执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">通过jps命令查看到namenode的pid,使用kill -9 pid命令杀死</span><br><span class="line">[root@slave1 /]# jps</span><br><span class="line">51 QuorumPeerMain</span><br><span class="line">372 DFSZKFailoverController</span><br><span class="line">117 NameNode</span><br><span class="line">262 JournalNode</span><br><span class="line">536 NodeManager</span><br><span class="line">185 DataNode</span><br><span class="line">697 Jps</span><br><span class="line">namenode的pid是117</span><br><span class="line">[root@slave1 /]# kill -9 117</span><br><span class="line">杀死后再看jps已经没有了</span><br><span class="line">[root@slave1 /]# jps</span><br><span class="line">51 QuorumPeerMain</span><br><span class="line">372 DFSZKFailoverController</span><br><span class="line">262 JournalNode</span><br><span class="line">536 NodeManager</span><br><span class="line">712 Jps</span><br><span class="line">185 DataNode</span><br><span class="line">[root@slave1 /]#</span><br></pre></td></tr></table></figure><p>这时查看web 界面,master由standby变成active</p><p><img src="1570064080387.png" alt="1570064080387"></p><p>因为namenode进程被杀死,web服务也没了，slave1的打不开</p><p><img src="1570064129505.png" alt="1570064129505"></p><p>然后重新启动被杀死的namenode</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">启动namenode进程</span><br><span class="line">[root@slave1 /]# hadoop-daemon.sh start namenode</span><br><span class="line">starting namenode, logging to /opt/hadoop/logs/hadoop--namenode-slave1.out</span><br><span class="line">[root@slave1 /]# jps</span><br><span class="line">51 QuorumPeerMain</span><br><span class="line">836 Jps</span><br><span class="line">372 DFSZKFailoverController</span><br><span class="line">262 JournalNode</span><br><span class="line">742 NameNode</span><br><span class="line">536 NodeManager</span><br><span class="line">185 DataNode</span><br><span class="line">[root@slave1 /]#</span><br></pre></td></tr></table></figure><p>web界面恢复，由active变成现在的standby</p><p><img src="1570064227489.png" alt="1570064227489"></p><h3 id="3-mapreduce-运行测试"><a href="#3-mapreduce-运行测试" class="headerlink" title="3. mapreduce 运行测试"></a>3. mapreduce 运行测试</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">[root@master ~]# hadoop jar wordcount.jar WordCount</span><br><span class="line">19/10/03 01:21:40 INFO input.FileInputFormat: Total input paths to process : 1</span><br><span class="line">19/10/03 01:21:41 INFO mapreduce.JobSubmitter: number of splits:1</span><br><span class="line">19/10/03 01:21:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1570063799017_0001</span><br><span class="line">19/10/03 01:21:42 INFO impl.YarnClientImpl: Submitted application application_1570063799017_0001</span><br><span class="line">19/10/03 01:21:42 INFO mapreduce.Job: The url to track the job: http://master:8088/proxy/application_1570063799017_0001/</span><br><span class="line">19/10/03 01:21:42 INFO mapreduce.Job: Running job: job_1570063799017_0001</span><br><span class="line">19/10/03 01:21:51 INFO mapreduce.Job: Job job_1570063799017_0001 running in uber mode : false</span><br><span class="line">19/10/03 01:21:51 INFO mapreduce.Job:  map 0% reduce 0%</span><br><span class="line">19/10/03 01:21:59 INFO mapreduce.Job:  map 100% reduce 0%</span><br><span class="line">19/10/03 01:22:05 INFO mapreduce.Job:  map 100% reduce 100%</span><br><span class="line">19/10/03 01:22:07 INFO mapreduce.Job: Job job_1570063799017_0001 completed successfully</span><br><span class="line">19/10/03 01:22:07 INFO mapreduce.Job: Counters: 49</span><br><span class="line">File System Counters</span><br><span class="line">FILE: Number of bytes read=1731</span><br><span class="line">FILE: Number of bytes written=253541</span><br><span class="line">FILE: Number of read operations=0</span><br><span class="line">FILE: Number of large read operations=0</span><br><span class="line">FILE: Number of write operations=0</span><br><span class="line">HDFS: Number of bytes read=1259</span><br><span class="line">HDFS: Number of bytes written=1063</span><br><span class="line">HDFS: Number of read operations=6</span><br><span class="line">HDFS: Number of large read operations=0</span><br><span class="line">HDFS: Number of write operations=2</span><br><span class="line">Job Counters </span><br><span class="line">Launched map tasks=1</span><br><span class="line">Launched reduce tasks=1</span><br><span class="line">Data-local map tasks=1</span><br><span class="line">Total time spent by all maps in occupied slots (ms)=4626</span><br><span class="line">Total time spent by all reduces in occupied slots (ms)=3956</span><br><span class="line">Total time spent by all map tasks (ms)=4626</span><br><span class="line">Total time spent by all reduce tasks (ms)=3956</span><br><span class="line">Total vcore-milliseconds taken by all map tasks=4626</span><br><span class="line">Total vcore-milliseconds taken by all reduce tasks=3956</span><br><span class="line">Total megabyte-milliseconds taken by all map tasks=4737024</span><br><span class="line">Total megabyte-milliseconds taken by all reduce tasks=4050944</span><br><span class="line">Map-Reduce Framework</span><br><span class="line">Map input records=45</span><br><span class="line">Map output records=114</span><br><span class="line">Map output bytes=1497</span><br><span class="line">Map output materialized bytes=1731</span><br><span class="line">Input split bytes=87</span><br><span class="line">Combine input records=0</span><br><span class="line">Combine output records=0</span><br><span class="line">Reduce input groups=89</span><br><span class="line">Reduce shuffle bytes=1731</span><br><span class="line">Reduce input records=114</span><br><span class="line">Reduce output records=89</span><br><span class="line">Spilled Records=228</span><br><span class="line">Shuffled Maps =1</span><br><span class="line">Failed Shuffles=0</span><br><span class="line">Merged Map outputs=1</span><br><span class="line">GC time elapsed (ms)=116</span><br><span class="line">CPU time spent (ms)=1830</span><br><span class="line">Physical memory (bytes) snapshot=432754688</span><br><span class="line">Virtual memory (bytes) snapshot=3911442432</span><br><span class="line">Total committed heap usage (bytes)=348127232</span><br><span class="line">Shuffle Errors</span><br><span class="line">BAD_ID=0</span><br><span class="line">CONNECTION=0</span><br><span class="line">IO_ERROR=0</span><br><span class="line">WRONG_LENGTH=0</span><br><span class="line">WRONG_MAP=0</span><br><span class="line">WRONG_REDUCE=0</span><br><span class="line">File Input Format Counters </span><br><span class="line">Bytes Read=1172</span><br><span class="line">File Output Format Counters </span><br><span class="line">Bytes Written=1063</span><br><span class="line">[root@master ~]#</span><br></pre></td></tr></table></figure><p>查看job history </p><p><img src="1570065773875.png" alt="1570065773875"></p><p><img src="1570065796139.png" alt="1570065796139"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Docker搭建Hadoop环境集群 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 大数据 </tag>
            
            <tag> Docker </tag>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker搭建Hadoop环境容器准备</title>
      <link href="/2019/10/03/Docker%E6%90%AD%E5%BB%BAHadoop%E7%8E%AF%E5%A2%83%E5%AE%B9%E5%99%A8%E5%87%86%E5%A4%87/"/>
      <url>/2019/10/03/Docker%E6%90%AD%E5%BB%BAHadoop%E7%8E%AF%E5%A2%83%E5%AE%B9%E5%99%A8%E5%87%86%E5%A4%87/</url>
      
        <content type="html"><![CDATA[<p>Docker 安装Hadoop HA 之前的准备</p><p>系统Ubuntu,安装有Docker,下载了centos:7的镜像</p><p><img src="1570195392528.png" alt="1570195392528"></p><table><thead><tr><th></th><th></th><th></th></tr></thead><tbody><tr><td>宿主机</td><td>Ubuntu 18:04</td><td></td></tr><tr><td>Docker 版本</td><td>19.03.2</td><td></td></tr><tr><td>安装Hadoop的镜像</td><td>centos:7</td><td></td></tr><tr><td></td><td></td><td></td></tr><tr><td></td><td></td><td></td></tr></tbody></table><h2 id="Docker-环境准备"><a href="#Docker-环境准备" class="headerlink" title="Docker 环境准备"></a>Docker 环境准备</h2><ul><li><p>创建一个docker 网络，用于集群使用</p></li><li><p>安装必要的包(如sshd,which(hdfs命令里用到), iproute),配置ssh</p></li></ul><h3 id="1-创建一个docker-网络-名字为hadoopCluster"><a href="#1-创建一个docker-网络-名字为hadoopCluster" class="headerlink" title="1. 创建一个docker 网络,名字为hadoopCluster"></a>1. 创建一个docker 网络,名字为<code>hadoopCluster</code></h3>   <figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker network create hadoopCluster</span><br></pre></td></tr></table></figure><a id="more"></a>  <p><img src="1569980257731.png" alt="1569980257731"></p><h3 id="2-创建三个容器-名字分别为master-slave1-slave2"><a href="#2-创建三个容器-名字分别为master-slave1-slave2" class="headerlink" title="2. 创建三个容器, 名字分别为master,slave1,slave2"></a>2. 创建三个容器, 名字分别为master,slave1,slave2</h3><p>要求</p><ul><li><p>使用centos:7镜像创建</p></li><li><p>创建时指定网络 为 hadoopCluster</p></li><li><p>指定运行的命令为bash (不指定一个命令的话,centos 启动后没有前台程序会自动退出),且要指定-it,不然一样会退出</p><p><strong>10-9补充,命令可以改成<code>/usr/sbin/init</code></strong></p></li><li><p>指定容器的名字为master,slave1,slave2</p></li><li><p><strong>(后面新增)使用-h指定容器的hostname</strong></p></li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">docker create -it --network  hadoopCluster -h master --name master centos:7 bash</span><br><span class="line">docker create -it --network  hadoopCluster -h slave1 --name slave1 centos:7 bash</span><br><span class="line">docker create -it --network -h  hadoopCluster slave2 --name slave2 centos:7 bash</span><br><span class="line"></span><br><span class="line">docker create -it --network -h slave2 hadoopCluster --name slave3 centos:7 bash</span><br></pre></td></tr></table></figure><p><img src="1569980703538.png" alt="1569980703538"></p><p>启动容器，安装镜像缺少的服务和软件,刚创建完的容器缺少一些必须的包,需要用yum安装</p><p>启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker start master</span><br><span class="line">docker start slave1</span><br><span class="line">docker start slave2</span><br></pre></td></tr></table></figure><p>在命令行里进入容器命令</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker exec -it master bash</span><br><span class="line">docker exec -it slave1 bash</span><br><span class="line">docker exec -it slave2 bash</span><br></pre></td></tr></table></figure><p>三台都安装下面的工具,直接用yum安装，</p><ul><li><p>iproute 网络工具</p><p><code>yum install iproute</code></p></li><li><p>which 命令</p><p><code>yum install which</code></p></li><li><p><del>passwd命令</del></p><p>默认有的</p></li></ul><p>使用passwd命令设置三个容器的密码为passwd</p><p><code>passwd</code></p><p>因为在同一个network中，所以可以直接通过容器名互相ping</p><p><img src="1569981835911.png" alt="1569981835911"></p><p>安装完iproute后，可以使用ip addr 命令查看容器的地址</p><p><img src="1569981887415.png" alt="1569981887415"></p><p>安装ssh服务端和客户端，安装完的ssh不会自启动，需要手动启动</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">//ssh服务端</span><br><span class="line">yum install openssh-server </span><br><span class="line">//ssh客户端</span><br><span class="line"> yum install openssh-clients</span><br><span class="line"> </span><br><span class="line">启动ssh服务端的命令为 ,不会自启,每次启动容器后要手动输入下面的命令启动ssh服务端</span><br><span class="line">/usr/sbin/sshd</span><br><span class="line">第一次启动会报错，缺少密钥</span><br><span class="line">[root@d7998eb32bfb yum.repos.d]# /usr/sbin/sshd     </span><br><span class="line">Could not load host key: /etc/ssh/ssh_host_rsa_key</span><br><span class="line">Could not load host key: /etc/ssh/ssh_host_ecdsa_key</span><br><span class="line">Could not load host key: /etc/ssh/ssh_host_ed25519_key</span><br><span class="line">sshd: no hostkeys available -- exiting.</span><br><span class="line"></span><br><span class="line">创建三种密钥</span><br><span class="line"> ssh-keygen -t rsa -b 2048 -f /etc/ssh/ssh_host_rsa_key</span><br><span class="line"> ssh-keygen -t ecdsa  -f /etc/ssh/ssh_host_ecdsa_key</span><br><span class="line"> ssh-keygen -t ed25519 -f /etc/ssh/ssh_host_ed25519_key</span><br></pre></td></tr></table></figure><p>创建密钥后重新启动,<code>ss -tnlp</code>查看开放的端口，发现22端口已经有了，就是成功</p><p><img src="1569982613981.png" alt="1569982613981"></p><p>容器添加中文支持,默认没有中文字符集,vi编辑器等无法输入中文，输入会乱码</p><p><img src="/1570598582558.png" alt="1570598582558"></p><ol><li><p>查看有的</p><p><code>locale -a</code></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line">[root@slave3 ~]# locale -a</span><br><span class="line">C</span><br><span class="line">en_AG</span><br><span class="line">en_AG.utf8</span><br><span class="line">en_AU</span><br><span class="line">en_AU.iso88591</span><br><span class="line">en_AU.utf8</span><br><span class="line">en_BW</span><br><span class="line">en_BW.iso88591</span><br><span class="line">en_BW.utf8</span><br><span class="line">en_CA</span><br><span class="line">en_CA.iso88591</span><br><span class="line">en_CA.utf8</span><br><span class="line">en_DK</span><br><span class="line">en_DK.iso88591</span><br><span class="line">en_DK.utf8</span><br><span class="line">en_GB</span><br><span class="line">en_GB.iso88591</span><br><span class="line">en_GB.iso885915</span><br><span class="line">en_GB.utf8</span><br><span class="line">en_HK</span><br><span class="line">en_HK.iso88591</span><br><span class="line">en_HK.utf8</span><br><span class="line">en_IE</span><br><span class="line">en_IE@euro</span><br><span class="line">en_IE.iso88591</span><br><span class="line">en_IE.iso885915@euro</span><br><span class="line">en_IE.utf8</span><br><span class="line">en_IN</span><br><span class="line">en_IN.utf8</span><br><span class="line">en_NG</span><br><span class="line">en_NG.utf8</span><br><span class="line">en_NZ</span><br><span class="line">en_NZ.iso88591</span><br><span class="line">en_NZ.utf8</span><br><span class="line">en_PH</span><br><span class="line">en_PH.iso88591</span><br><span class="line">en_PH.utf8</span><br><span class="line">en_SG</span><br><span class="line">en_SG.iso88591</span><br><span class="line">en_SG.utf8</span><br><span class="line">en_US</span><br><span class="line">en_US.iso88591</span><br><span class="line">en_US.iso885915</span><br><span class="line">en_US.utf8</span><br><span class="line">en_ZA</span><br><span class="line">en_ZA.iso88591</span><br><span class="line">en_ZA.utf8</span><br><span class="line">en_ZM</span><br><span class="line">en_ZM.utf8</span><br><span class="line">en_ZW</span><br><span class="line">en_ZW.iso88591</span><br><span class="line">en_ZW.utf8</span><br><span class="line">POSIX</span><br></pre></td></tr></table></figure></li></ol><ol start="2"><li>修改环境变量,设置LC_ALL 为 en_US.utf8</li></ol><p>这里修改~/.bashrc 文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">export LC_ALL=en_US.utf8</span><br></pre></td></tr></table></figure><p>成功</p><p>  <img src="/1570599163681.png" alt="1570599163681"></p>]]></content>
      
      
      <categories>
          
          <category> 大数据 </category>
          
          <category> Docker搭建Hadoop环境集群 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Matplotlib设置显示中文</title>
      <link href="/2019/09/28/Matplotlib%E8%AE%BE%E7%BD%AE%E6%98%BE%E7%A4%BA%E4%B8%AD%E6%96%87/"/>
      <url>/2019/09/28/Matplotlib%E8%AE%BE%E7%BD%AE%E6%98%BE%E7%A4%BA%E4%B8%AD%E6%96%87/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Vuejs待定</title>
      <link href="/2019/09/26/Vuejs%E5%BE%85%E5%AE%9A/"/>
      <url>/2019/09/26/Vuejs%E5%BE%85%E5%AE%9A/</url>
      
        <content type="html"><![CDATA[<ol><li>使用vuecli 创建工程</li></ol><ul><li>命令行方式<br>vue create xxx</li><li>ui 方式<br>启动ui界面 vue ui</li></ul>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Scrapy手册</title>
      <link href="/2019/09/24/Scrapy%E6%89%8B%E5%86%8C/"/>
      <url>/2019/09/24/Scrapy%E6%89%8B%E5%86%8C/</url>
      
        <content type="html"><![CDATA[<h3 id="基本命令"><a href="#基本命令" class="headerlink" title="基本命令"></a>基本命令</h3><ul><li><p>genspider</p><p>使用预定义的模板生成一个spider类</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">Usage</span><br><span class="line">=====</span><br><span class="line">  scrapy genspider [options] &lt;name&gt; &lt;domain&gt;</span><br><span class="line"></span><br><span class="line">Generate new spider using pre-defined templates</span><br><span class="line"></span><br><span class="line">Options</span><br><span class="line">=======</span><br><span class="line">--help, -h              show this help message and exit </span><br><span class="line">--list, -l              List available templates  列出可用的模板</span><br><span class="line">--edit, -e              Edit spider after creating it创建spider后编辑它</span><br><span class="line">--dump=TEMPLATE, -d TEMPLATE</span><br><span class="line">                        Dump template to standard output输出模板到标准输出</span><br><span class="line">--template=TEMPLATE, -t TEMPLATE</span><br><span class="line">                        Uses a custom template.</span><br><span class="line">--force                 If the spider already exists, overwrite it with the</span><br><span class="line">                        template 如果spider存在,就强制使用模板覆盖</span><br><span class="line"></span><br><span class="line">Global Options全局选项A</span><br><span class="line">--------------</span><br><span class="line">--logfile=FILE          log file. if omitted stderr will be used日志文件</span><br><span class="line">--loglevel=LEVEL, -L LEVEL</span><br><span class="line">                        log level (default: DEBUG)日志登记</span><br><span class="line">--nolog                 disable logging completely</span><br><span class="line">--profile=FILE          write python cProfile stats to FILE</span><br><span class="line">--pidfile=FILE          write process ID to FILE</span><br><span class="line">--set=NAME=VALUE, -s NAME=VALUE</span><br><span class="line">                        set/override setting (may be repeated)覆盖setting的值</span><br><span class="line">--pdb                   enable pdb on failure失败时开启pdb</span><br></pre></td></tr></table></figure><p>内置的模板有</p><p>  basic<br>  crawl<br>  csvfeed<br>  xmlfeed</p><p>这几个</p></li><li></li></ul><h3 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h3><ul><li><p>在IDE里设置response的类型,来代码提示</p><p>回调函数里的ide识别不了类型，手动指定类型即可</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scrapy.http.response.html.HtmlResponse</span><br><span class="line"><span class="keyword">from</span> scrapy.http.response.html <span class="keyword">import</span> HtmlResponse</span><br></pre></td></tr></table></figure></li></ul><ul><li><p>设置运行日志输出等级</p><p>调试完后正式运行，不想输出太多日志</p></li></ul><h3 id="使用示例"><a href="#使用示例" class="headerlink" title="使用示例"></a>使用示例</h3><ol><li><p>创建工程</p></li><li><p>关闭机器人协议</p></li><li><p>生成spider,生成一个叫wallpaper的spider,域名为<code>wall.alphacoders.com</code></p><p>``</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> 手册 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Flask手册</title>
      <link href="/2019/09/23/Flask%E6%89%8B%E5%86%8C/"/>
      <url>/2019/09/23/Flask%E6%89%8B%E5%86%8C/</url>
      
        <content type="html"><![CDATA[<p>最新版本为1.1</p><h3 id="资源"><a href="#资源" class="headerlink" title="资源"></a>资源</h3><ul><li><p>微软 vscode 的 Flask 教程</p><p><a href="https://code.visualstudio.com/docs/python/tutorial-flask?WT.mc_id=python-c9-niner#_prerequisites" target="_blank" rel="noopener">https://code.visualstudio.com/docs/python/tutorial-flask?WT.mc_id=python-c9-niner#_prerequisites</a></p></li><li><p>Flask 官方文档</p><p><a href="https://flask.palletsprojects.com/en/1.1.x/" target="_blank" rel="noopener">https://flask.palletsprojects.com/en/1.1.x/</a></p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 手册 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Flask </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>对象关系教程(Object Relational Tutorial)</title>
      <link href="/2019/09/21/%E5%AF%B9%E8%B1%A1%E5%85%B3%E7%B3%BB%E6%95%99%E7%A8%8B-Object-Relational-Tutorial/"/>
      <url>/2019/09/21/%E5%AF%B9%E8%B1%A1%E5%85%B3%E7%B3%BB%E6%95%99%E7%A8%8B-Object-Relational-Tutorial/</url>
      
        <content type="html"><![CDATA[<p>SQLAlchemy Object Relational Mapper 提出了一个将用户自定义Python类和数据库表，类实例和对应的表里的行结合起来-的方法，包含一个同步在对象和相关的行的状态的所有改变的系统,叫做<code>unit of work</code>,以及一个通过用户定义的类来表达数据库查询和定义相互的关系的系统.</p><p>ORM 对于SQLAlchemy Expression Language 是结构化的.然而SQLAlchemy Expression Language ,在 SQLAlchemy Tutorial 里介绍的,提出了一个直接表达关系型数据库原始结构的系统,ORM 提出了一个高等级的,抽象的使用方式,它自己就是Expression Language的applyied usage的一个例子.</p><p>尽管在ROM和Expression Language的用法上有一些重叠,但相似之处比他们一开始显现的要肤浅.</p><p>用户自定义的domain model(领域模型)</p><p>一个成功的应用可能仅仅使用Object Relation Mapper .在高级的场合,一个由ORM构成的应用偶尔的在指定数据库交互适当的地方使用Expression Language ,</p>]]></content>
      
      
      <categories>
          
          <category> 翻译 </category>
          
          <category> SQLAlchemy官方文档 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>Docker手册</title>
      <link href="/2019/09/21/Docker%E6%89%8B%E5%86%8C/"/>
      <url>/2019/09/21/Docker%E6%89%8B%E5%86%8C/</url>
      
        <content type="html"><![CDATA[<h3 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h3><h4 id="进入容器命令行"><a href="#进入容器命令行" class="headerlink" title="进入容器命令行"></a>进入容器命令行</h4><p><code>docker exec -it dockertest  bash</code></p><h4 id="获取容器IP地址"><a href="#获取容器IP地址" class="headerlink" title="获取容器IP地址"></a>获取容器IP地址</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">docker inspect --format=&apos;&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;&apos;  dockertest</span><br></pre></td></tr></table></figure><p>本机上传文件到docker 容器里</p><p><code>docker cp 本地目录 容器ID:容器内目录</code></p><h3 id="Image-镜像"><a href="#Image-镜像" class="headerlink" title="Image 镜像"></a>Image 镜像</h3><h4 id="查看所有的镜像"><a href="#查看所有的镜像" class="headerlink" title="查看所有的镜像"></a>查看所有的镜像</h4><p>docker images</p><h4 id="搜索镜像"><a href="#搜索镜像" class="headerlink" title="搜索镜像"></a>搜索镜像</h4><p>docker search 关键字</p><a id="more"></a><h3 id="Network-网络"><a href="#Network-网络" class="headerlink" title="Network 网络"></a>Network 网络</h3><h4 id="查看所有网络"><a href="#查看所有网络" class="headerlink" title="查看所有网络"></a>查看所有网络</h4><p><code>docker network ls</code></p><h4 id="创建网络"><a href="#创建网络" class="headerlink" title="创建网络"></a>创建网络</h4><p><code>docker network create -d bridge net_mysql</code></p><h4 id="创建容器时指定网络"><a href="#创建容器时指定网络" class="headerlink" title="创建容器时指定网络"></a>创建容器时指定网络</h4><p><code>--network net_mysql</code></p><h4 id="手动添加容器到网络"><a href="#手动添加容器到网络" class="headerlink" title="手动添加容器到网络"></a>手动添加容器到网络</h4><p><code>docker network connect 网络 容器</code></p><h3 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h3><h4 id="创建容器"><a href="#创建容器" class="headerlink" title="创建容器"></a>创建容器</h4><p><code>docker create [选项] 镜像 [命令][参数]</code></p><ul><li>–name 镜像名字</li><li></li></ul><h4 id="导出容器文件系统到本地"><a href="#导出容器文件系统到本地" class="headerlink" title="导出容器文件系统到本地"></a>导出容器文件系统到本地</h4><p><code>docker export master &gt; master.tar</code></p>]]></content>
      
      
      <categories>
          
          <category> 手册 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手册 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker部署MySQL</title>
      <link href="/2019/09/21/Docker%E9%83%A8%E7%BD%B2MySQL/"/>
      <url>/2019/09/21/Docker%E9%83%A8%E7%BD%B2MySQL/</url>
      
        <content type="html"><![CDATA[<p>镜像下载:<br>docker search mysql,pull第一个官方的就行</p><p>支持的tag</p><ul><li>8.0.17, 8.0, 8, latest</li><li>5.7.27, 5.7, 5</li><li>5.6.45, 5.6</li></ul><p>翻译自Docker MySQL官方镜像文档</p><p>原文链接:<a href="https://github.com/docker-library/docs/tree/master/mysql" target="_blank" rel="noopener">https://github.com/docker-library/docs/tree/master/mysql</a></p><p>其他的官方镜像的文档都可以在<a href="https://github.com/docker-library/docs/里找到" target="_blank" rel="noopener">https://github.com/docker-library/docs/里找到</a></p><h3 id="启动mysql服务"><a href="#启动mysql服务" class="headerlink" title="启动mysql服务"></a>启动<code>mysql</code>服务</h3><p>启动MySQL很简单</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">用法:</span><br><span class="line">docker run --name some-mysql   -e MYSQL_ROOT_PASSWORD=用户密码 -d mysql:tag </span><br><span class="line">--name 指定容器的名字</span><br><span class="line">-e 指定容器的环境变量</span><br><span class="line">-d 设置容器在后台运行并输出容器id</span><br><span class="line">tag 指定想要的mysql版本,不加默认使用最新</span><br><span class="line"></span><br><span class="line">创建并运行一个叫mysqlserver的mysql容器</span><br><span class="line">docker run --name  mysqlserver  -e MYSQL_ROOT_PASSWORD=123456  -d mysql</span><br><span class="line"></span><br><span class="line">ps 命令看到mysqlserver正在运行</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">创建mysql镜像</span><br><span class="line">docker create --name js   -e MYSQL_ROOT_PASSWORD=r mysql:5.7</span><br><span class="line">启动容器</span><br><span class="line">docker start js</span><br><span class="line">查看js容器ip地址</span><br><span class="line">创建一个临时客户端</span><br><span class="line">docker  run  -it  --rm mysql mysql -h172.19.0.5 -uroot -proot</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="从MySQL命令行客户端里连接mysql"><a href="#从MySQL命令行客户端里连接mysql" class="headerlink" title="从MySQL命令行客户端里连接mysql"></a>从MySQL命令行客户端里连接mysql</h3><p>这个镜像不仅可以做上面那个mysql服务端，也可以用来做客户端来连接其他容器里的mysql，或者其他地方的mysql</p><p>上面的启动的容器是mysql服务端,下面的命令可以启动一个作为mysql命令行客户端的容器，并且连接到上面启动的那个容器里的mysql</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">1.  docker  run  -it  --rm mysql mysql -h容器的ip地址 -uexample-user -p</span><br><span class="line">- 查看mysqlserver容器的ip地址</span><br><span class="line">bigdata@ljh-X441UVK:~$ docker inspect --format='&#123;&#123;range .NetworkSettings.Networks&#125;&#125;&#123;&#123;.IPAddress&#125;&#125;&#123;&#123;end&#125;&#125;'  mysqlserver</span><br><span class="line">172.17.0.2</span><br><span class="line">通过指定ip连接(mysql命令的-h参数)</span><br><span class="line"> docker  run  -it  --rm mysql mysql -h172.17.0.2  -uroot  -p</span><br><span class="line"></span><br><span class="line">2. 通过指定 network连接</span><br><span class="line">可以直接通过容器名字连接，不用输入ip地址</span><br><span class="line">- 创建一个network 网络</span><br><span class="line">```shell</span><br><span class="line">docker network create -d bridge mysqlnet</span><br><span class="line">bigdata@ljh-X441UVK:~$ docker network ls</span><br><span class="line">NETWORK ID          NAME                DRIVER              SCOPE</span><br><span class="line">71dfa846b9c6        bridge              bridge              local</span><br><span class="line">9fef614764d3        host                host                local</span><br><span class="line">87bc32c1bc3b        mysqlnet            bridge              local</span><br><span class="line">6d83addc4da3        none                null                local</span><br><span class="line">```</span><br><span class="line">- 将mysqlserver添加到mysqlnet网络里</span><br><span class="line">```shell</span><br><span class="line">docker network connect mysqlnet mysqlserver</span><br><span class="line">```</span><br><span class="line">- mysql客户端命令行 启动时指定mysqlnet网络</span><br><span class="line">启动的容器连接到网络mysqlnet后</span><br><span class="line">运行的命令是 mysql -hmysqlserver  -uroot  -p</span><br><span class="line">mysql -h 指定为mysqlserver这个容器名字，因为mysqlserver在这个网络里,不用写ip地址(可能在同一个网络里有host映射)</span><br><span class="line">docker  run  -it --network mysqlnet --rm mysql mysql -hmysqlserver  -uroot  -p</span><br><span class="line"></span><br><span class="line">3 . 通过--link参数 指定来连接</span><br><span class="line"></span><br><span class="line">docker run -it  --link mysqlserver  --rm mysql  mysql -hmysqlserver -uroot -p</span><br></pre></td></tr></table></figure><p>连接其他的mysql,修改-h对应的即可</p><p>关于MySQL命令行的更多信息请查看:[MySQL documentation][<a href="https://dev.mysql.com/doc/en/mysql.html]" target="_blank" rel="noopener">https://dev.mysql.com/doc/en/mysql.html]</a></p><h3 id="访问容器shell和浏览MySQL日志"><a href="#访问容器shell和浏览MySQL日志" class="headerlink" title="访问容器shell和浏览MySQL日志"></a>访问容器shell和浏览MySQL日志</h3><p><code>docker exec</code>命令可以在Docker容器里运行一个命令.下面这个命令在你的<code>mysql</code>容器里给启动一个交互式的(-it)bash shell</p><p><code>docker exec -it some-mysql bash</code></p><p>可以通过Docker的容器日志查看日志</p><p><code>docker logs some-mysql</code></p><h3 id="使用自定义的MySQL配置文件"><a href="#使用自定义的MySQL配置文件" class="headerlink" title="使用自定义的MySQL配置文件"></a>使用自定义的MySQL配置文件</h3><p>默认的配置文件在<code>/etc/mysql/my.cnf</code>,<code>!includedir</code> 里可能有额外的目录,比如<code>/etc/mysql/conf.d</code>或者<code>/etc/mysql/mysql.conf.d</code>.请在<code>mysql</code>镜像里检查相关文件</p><p>如果<code>/my/custom/config-file.cnf</code>是自定义的配置文件的目录和文件名,可以这样启动<code>mysql</code>容器(注意这个命令里只使用了自定义配置文件的目录的路径)</p><p><code>docker run -name some-mysql -v /my/custom:/etc/mysql/conf.d -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag</code></p><p>这个命令会启动一个新的<code>mysql</code>容器,使用<code>/etc/mysql/my.cnf</code>和<code>/etc/mysql/conf.d/config-file.cnf</code>这两个配置文件的合并结果,后面那个的配置文件的配置优先</p><h3 id="不用cnf文件来配置配置"><a href="#不用cnf文件来配置配置" class="headerlink" title="不用cnf文件来配置配置"></a>不用<code>cnf</code>文件来配置配置</h3><p>很多配置选项可以作为标记通过<code>mysqld</code>传递.这样可以很灵活的不用<code>cnf</code>文件来自定义容器.比如,你只想改变默认的编码和Collation字符集 成UTF-8(utf8mb4)，只需运行下面的</p><p><code>docker run -name some-mysql -e  MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag --character-set-server=utf8m4 --collation-server=utf8m4_unicode_ci</code></p><p>如果想查看全部可以用的选项，只需要运行:</p><p><code>docker run -it --rm mysql:tag --verbose --help</code></p><h3 id="环境变量"><a href="#环境变量" class="headerlink" title="环境变量"></a>环境变量</h3><p>当你启动<code>mysql</code>镜像,你可以在<code>docker run</code>命令行里传递一个或者更多环境变量来调整MySQL的配置.注意如果启动的容器有一个已经包含数据库的数据目录,下面的变量将不会有效果:已存在的数据库不会有变化.</p><p>查看这个连接<a href="https://dev.mysql.com/doc/refman/5.7/en/environment-variables.html" target="_blank" rel="noopener">https://dev.mysql.com/doc/refman/5.7/en/environment-variables.html</a> 查看MySQL本身支持的环境变量的文档(特殊的变量像<code>MSQL_HOST</code>,和这个镜像使用的时候会出现问题)</p><p><code>MYSQL_ROOT_PASSWORD</code></p><p>这个变量必须存在,指定的密码将会设置为MySQL<code>root</code>用户的密码.在上面的例子里，他是设置成<code>my-secret-pw</code>.</p><p><code>MYSQL_DATABASE</code></p><p>这个变量是可选的,如果指定了，在镜像启动时会用这个的值创建一个数据库.如果也指定了 user/password (下面),这个用户会被授予这个数据库的超级权限</p><p><code>MYSQL_USER</code>,<code>MYSQL_PASSWORD</code></p><p>这些变量是可选的，结合在一起使用来创建一个新用户和设置用户的密码.这个用户会被授予上面通</p><p>注意这里不需要用这个机制来创建root用户，root用户会用<code>MYSQL_ROOT_PASSWORD</code>变量指定的密码来创建.</p><p><code>MYSQL_ALLOW_EMPTY_PASSWORD</code></p><p>这是个可选的变量.设置<code>yes</code> 就允许root用户用一个空白的密码来启动容器.注意:不推荐设置成<code>yes</code>除非你知道你在干什么,因为这个会让你的MySQL实例完全没有保护,任何人都能获得完整的超级用户权限</p><p><code>MYSQL_RANDOM_ROOT_PASSWORD</code><br>s<br>这是一个可选变量.设置成<code>yes</code>会随机产生一个初始化密码给root用户(使用pwgen).这个生成的密码会输出到标准输出里.</p><p><code>MYSQL_ONETIME_PASSWORD</code></p><p>设置root 用户初始化完后就过期,第一次登录时强制修改密码.注意:这个功能只支持MySQL 5.6+.在MySQL 5.5使用会在初始化时抛出一个错误.</p><hr><p>后面的未认真翻译</p><h3 id="Docker-隐私"><a href="#Docker-隐私" class="headerlink" title="Docker 隐私"></a>Docker 隐私</h3><p>作为通过环境变量来传递敏感信息的代替,<code>_FILE</code>可能是拼接到之前列出的环境变量里,初始化脚本从一个容器的文件里加载这些变量的值,这个可以用从保存在<code>/run/secrets/&lt;secret_name&gt;</code>文件的Docker secrets 加载密码</p><p>比如</p><p><code>docker run -name some-mysql -e MYSQL_ROOT_PASSWORD_FILE=/run/secrets/mysql-root -d mysql:tag</code></p><p>当前只支持<code>MYSQL_ROOT_PASSWORD</code>,<code>MYSQL_ROOT_HOST</code>,<code>MYSQL_DATABASE</code>,<code>MYSQL_USER</code>和<code>MYSQL_PASSWORD</code></p><h3 id="初始化一个新鲜的实例"><a href="#初始化一个新鲜的实例" class="headerlink" title="初始化一个新鲜的实例"></a>初始化一个新鲜的实例</h3><p>当容器第一次启动时,将会创建一个指定名字的数据库和用提供的配置变量来初始化.此外,还会执行在<code>/docker-entrypoint-initdb.d</code>里的<code>.sh</code>,<code>.sql</code>和<code>.sql.gz</code>文件.文件会按照字母排序执行.你可以通过<a href="https://docs.docker.com/engine/tutorials/dockervolumes/#mount-a-host-file-as-a-data-volume" target="_blank" rel="noopener">mounting a SQL dump into that directory</a>和提供一个<a href="https://docs.docker.com/reference/builder/" target="_blank" rel="noopener">custom images</a>带有贡献的数据.SQL 文件会默认导入通过<code>MYSQL_DATABASE</code>变量指定的数据库.</p><h3 id="注意事项"><a href="#注意事项" class="headerlink" title="注意事项"></a>注意事项</h3><h3 id="在哪里保存数据"><a href="#在哪里保存数据" class="headerlink" title="在哪里保存数据"></a>在哪里保存数据</h3><p>重要的提示:这里有几个方式来保存在Docker 容器上运行的应用使用的数据,我们鼓励<code>mysql</code>镜像的用户熟悉下面可用的选项,包括:</p><ul><li>让Docker 管理数据库数据的保存<a href="https://docs.docker.com/engine/tutorials/dockervolumes/#adding-a-data-volume" target="_blank" rel="noopener">通过用他内部的volume management来写入数据库文件到宿主机</a>,这个是默认的，并且对于用户来说很容易和公平透明.缺点是这些文件很难被直接运行在宿主机上的工具和应用定位.i.e outside container</li><li>创建一个数据目录在宿主机上(容器外)，<a href="https://docs.docker.com/engine/tutorials/dockervolumes/#mount-a-host-directory-as-a-data-volume" target="_blank" rel="noopener">在容器里挂载这个目录可见</a>,这样把数据库文件放在宿主机上知道的位置,并且容器宿主机上的工具和应用访问这些文件.缺点是用户需要确定目录存在，和比如权限和其他安全机制配置正确</li></ul><p>Docker 文件是个好起点来理解不同的存储选项和变化,这里有很多blog和论坛帖子讨论和给建议 。简单展示下上面后面选项的基本流程</p><ol><li><p>在宿主系统的合适volume创建一个目录,比如<code>/my/own/datadir</code>.</p></li><li><p>启动<code>mysql</code>容器<br>docker run –name some-mysql -v /my/own/datadir:/var/lib/mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag</p><p>命令的<code>-v /my/own/datadir:/var/lib/mysql</code>部分,从宿主下挂在<code>/my/own/datadir</code>作为容器里的<code>/var/lib/mysql</code>,那里是MySQL默认的数据文件的地方</p></li></ol><h3 id="直到MySQL初始化完成之前没有连接"><a href="#直到MySQL初始化完成之前没有连接" class="headerlink" title="直到MySQL初始化完成之前没有连接"></a>直到MySQL初始化完成之前没有连接</h3><p>当容器启动时如果没有数据库初始化,将会创建一个默认的数据库.这个是预期的行为,这意味着它将不会接受传入的连接直到初始化完成.这个会在使用自动化工具时有问题,比如<code>docker-compose</code>,这个会同时启动几个容器</p><p>如果你尝试连接MySQL的应用没有处理MySQL 停机或者等待MySQL完美启动,在服务启动前放一个连接重试是必须的.在官方镜像里这样的实现,<a href="https://github.com/docker-library/wordpress/blob/1b48b4bccd7adb0f7ea1431c7b470a40e186f3da/docker-entrypoint.sh#L195-L235" target="_blank" rel="noopener">WordPress</a>或者<a href="https://github.com/docker-library/docs/blob/9660a0cccb87d8db842f33bc0578d769caaf3ba9/bonita/stack.yml#L28-L44" target="_blank" rel="noopener">Bonita</a></p><h3 id="对于已存在的数据库的用法"><a href="#对于已存在的数据库的用法" class="headerlink" title="对于已存在的数据库的用法"></a>对于已存在的数据库的用法</h3><p>如果你启动一个有数据目录的<code>mysql</code>容器,而且存在了一个数据库(specifically,一个<code>mysql</code>子目录),应该从命令行里提交一个<code>$MYSQL_ROOT_PASSWORD</code>变量;他无论如何都会被忽略,并且已存在的数据库不会改变</p><h3 id="以任意的用户运行"><a href="#以任意的用户运行" class="headerlink" title="以任意的用户运行"></a>以任意的用户运行</h3><p>如果你知道你目录的权限已经设置合理(比如对于一个已存在的数据库运行,像上面说的)或者你需要指定mysqld运行的UID/GID.可以用<code>--user</code>来调用镜像设置任何值来完成想要的权限/配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir data</span><br><span class="line">ls -lnd data</span><br><span class="line"> docker run -v &quot;$PWD/data&quot;:/var/lib/mysql --user 1000:1000 --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -d mysql:tag</span><br></pre></td></tr></table></figure><h3 id="创建数据库导出"><a href="#创建数据库导出" class="headerlink" title="创建数据库导出"></a>创建数据库导出</h3><p>大多数正常的工具都能用,尽管他们的用法有一点令人费解 在一些场合来确保他们有权限访问<code>mysqld</code>服务.一个简单的方法来确保就是使用<code>docker exec</code>和从相同的容器里运行工具,像下面这样:</p><p><code>docker exec some-mysql sh -c &#39;exec mysqldump --all-databases -uroot -p&quot;$MYSQL_ROOT_PASSWORD&quot;&#39; &gt; /some/path/on/your/host/all-databases.sql</code></p><h3 id="从导出文件里恢复数据"><a href="#从导出文件里恢复数据" class="headerlink" title="从导出文件里恢复数据"></a>从导出文件里恢复数据</h3><p>对于恢复数据,你可以用<code>-i</code>选项的<code>docker exec</code>命令,像下面这样:</p><p><code>$ docker exec -i some-mysql sh -c &#39;exec mysql -uroot -p&quot;$MYSQL_ROOT_PASSWORD&quot;&#39; &lt; /some/path/on/your/host/all-databases.sql</code></p>]]></content>
      
      
      <categories>
          
          <category> Docker </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hexo手册</title>
      <link href="/2019/09/21/Hexo%E6%89%8B%E5%86%8C/"/>
      <url>/2019/09/21/Hexo%E6%89%8B%E5%86%8C/</url>
      
        <content type="html"><![CDATA[<h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><h4 id="创建文章"><a href="#创建文章" class="headerlink" title="创建文章"></a>创建文章</h4><p><code>hexo new [layout] title</code><br>layout 可以选</p><ul><li>draft<br>草稿</li><li>post<br>post</li><li>page<br>页面</li></ul><h4 id="草稿转为文章"><a href="#草稿转为文章" class="headerlink" title="草稿转为文章"></a>草稿转为文章</h4><p><code>hexo publish [layout] title</code></p><h3 id="文章在首页显示加载更多"><a href="#文章在首页显示加载更多" class="headerlink" title="文章在首页显示加载更多"></a>文章在首页显示加载更多</h3><p>在想要的地方加入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- more --&gt;</span><br></pre></td></tr></table></figure><a id="more"></a>]]></content>
      
      
      <categories>
          
          <category> 手册 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 手册 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HowTo</title>
      <link href="/2019/09/07/HowTo/"/>
      <url>/2019/09/07/HowTo/</url>
      
        <content type="html"><![CDATA[<h2 id="How-to"><a href="#How-to" class="headerlink" title="How to"></a>How to</h2><h3 id="1-怎么用英语询问时间"><a href="#1-怎么用英语询问时间" class="headerlink" title="1. 怎么用英语询问时间"></a>1. 怎么用英语询问时间</h3><ul><li><p>同事或者朋友</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">what time is it?</span><br></pre></td></tr></table></figure></li><li><p>陌生人</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Have you got the time ?</span><br><span class="line">更加礼貌的方式</span><br><span class="line">Execuse me, have you got the time?</span><br><span class="line">Execuse me,have you got the time please?</span><br><span class="line">Sorry,hava you got the time?</span><br><span class="line">Sorry,hava you got the time please?</span><br></pre></td></tr></table></figure></li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 英语 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>英语四级满分作文模板</title>
      <link href="/2019/08/28/%E8%8B%B1%E8%AF%AD%E5%9B%9B%E7%BA%A7%E6%BB%A1%E5%88%86%E4%BD%9C%E6%96%87%E6%A8%A1%E6%9D%BF/"/>
      <url>/2019/08/28/%E8%8B%B1%E8%AF%AD%E5%9B%9B%E7%BA%A7%E6%BB%A1%E5%88%86%E4%BD%9C%E6%96%87%E6%A8%A1%E6%9D%BF/</url>
      
        <content type="html"><![CDATA[<blockquote><p>来自<a href="http://www.eudic.cn/ting/" target="_blank" rel="noopener">每日英语听力</a>上的专辑</p></blockquote><p><img src="/images/%E5%98%A4glish.gif" alt="Let&#39;s speak 嘤glish"></p><h3 id="08-30-慰问生病的学生"><a href="#08-30-慰问生病的学生" class="headerlink" title="08-30 慰问生病的学生"></a>08-30 慰问生病的学生</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Dear Alice,I am deeply concerned that I did not see you</span><br><span class="line">in  class today.</span><br></pre></td></tr></table></figure><blockquote><p>亲爱的爱丽丝,今天上课没有见到你,我很担心.</p></blockquote><a id="more"></a><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Later I learned that you stayed at home because of  a  </span><br><span class="line">heavy cold.The weather is quite fickle these days ,and </span><br><span class="line">you should make sure that you  err on the  side of caution </span><br><span class="line">by wearing a bit more .</span><br></pre></td></tr></table></figure><blockquote><p>后来我得知你是因为得了重感冒在家休息。最近天气变化无常,你应该适时增减衣物。</p></blockquote><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">I think the bed rest is the best medicine for a cold and hope that</span><br><span class="line">you will recover in no time at all.Looking forward to seeing you </span><br><span class="line">agan in class tomorrow.</span><br></pre></td></tr></table></figure><blockquote><p>我想卧床休息是治疗感冒的良药吧,希望你能尽快康复。希望明天上课能见到你。</p></blockquote><ul><li><p>err on the side of caution</p><p>宁可多表现(某种行为)</p><p>再…也不为过</p></li><li><p>make sure that + 从句</p><p>一定要做某事</p></li><li><p>in no time</p><p>立即;很快</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> 英语 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>第一篇博客</title>
      <link href="/2019/08/27/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/"/>
      <url>/2019/08/27/%E7%AC%AC%E4%B8%80%E7%AF%87%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<p><img src="1566915832561.png" alt="1566915832561"></p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><blockquote><p>滑而不稽则罔，稽而不滑则殆 .</p><p>​                                                           –孔子</p></blockquote><a id="more"></a><h3 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h3><blockquote><p>使用<a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">Hexo</a> 搭建的</p><p>互联网上的个人小窝搭建好啦</p><p>本来是想练习Flask+VueJs自己写个小博客网站来练习的(工程已凉),过程中发现了Hexo这个东西,决定用他来做个自己的博客 网站</p></blockquote><p>搭建过程不用敲代码,现在使用的是Hexo 的 NexT主题,还是默认样式.</p><p>在各种姻缘巧合之下，这个暂时不知道写什么文章的博客就暂时搭建好了</p><p>顺便写一篇’入伙’文章….</p><p><img src="1566915861558.png" alt="1566915861558"></p><h3 id="写文章"><a href="#写文章" class="headerlink" title="写文章"></a>写文章</h3><p>现在如果要添加一篇文章的话,要 </p><ul><li>本地用Hexo新建一篇,会生成.md文件</li><li>编辑生成的.md文件,编写</li><li>用Hexo 根据.md文件生成静态网页 </li><li>上传到FTP空间(可以直接使用命令,但是配置ftp时连接不上,现在是手动上传)</li></ul><p>现在还是有些麻烦 ,和些小问题. 今天有看到一个UI的工具,还没试,用那个应该会简单许多.</p><p><img src="1566915925884.png" alt="1566915925884"></p><h2 id="搭建环境"><a href="#搭建环境" class="headerlink" title="搭建环境"></a>搭建环境</h2><h3 id="1-博客框架"><a href="#1-博客框架" class="headerlink" title="1. 博客框架"></a>1. 博客框架</h3><p>使用<a href="https://hexo.io/zh-cn/" target="_blank" rel="noopener">Hexo</a>来搭建的</p><p><img src="1566911122321.png" alt="1566911122321"></p><h3 id="2-部署主机"><a href="#2-部署主机" class="headerlink" title="2.部署主机"></a>2.部署主机</h3><p>最后将生成的网页部署到 FTP主机 (我的这个就是),或者Github Page,Heroku上</p><p>放到FTP主机上前提得有主机,我用的是之前买的(一直荒废)香港3v主机</p><p>如果是部署到Github那些地方应该就是免费的(还没试过)</p><p><img src="1566911060119.png" alt="1566911060119"></p><h3 id="3-域名"><a href="#3-域名" class="headerlink" title="3.域名"></a>3.域名</h3><p>域名是之前在<a href="https://www.cndns.com" target="_blank" rel="noopener">美橙互联</a>上购买的<code>formatfa.top</code></p><p><img src="1566911264156.png" alt="1566911264156"></p><p><img src="1566911310625.png" alt="1566911310625"></p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
    </entry>
    
    
    
    <entry>
      <title>图片文章测试</title>
      <link href="/2019/08/26/%E5%9B%BE%E7%89%87%E6%96%87%E7%AB%A0%E6%B5%8B%E8%AF%95/"/>
      <url>/2019/08/26/%E5%9B%BE%E7%89%87%E6%96%87%E7%AB%A0%E6%B5%8B%E8%AF%95/</url>
      
        <content type="html"><![CDATA[<p><img src="1566826762609.png" alt="1566826762609"></p><p><img src="1566826987497.png" alt="1566826987497"></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
